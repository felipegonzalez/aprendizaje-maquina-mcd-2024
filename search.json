[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje Máquina",
    "section": "",
    "text": "Temario y referencias\nTodas las notas y material del curso estarán en este repositorio.\n\nIntroducción al aprendizaje máquina\nMétodos locales y estructurados. Ingeniería de variables de entrada.\nPrincipios de Regularización\nProblemas de clasificación, métricas y evaluación\nMétodos de remuestreo y validación cruzada\nRedes neuronales\nÁrboles, bosques aleatorios y boosting\nDiagnóstico y mejora en problemas de aprendizaje supervisado\nReducción de dimensionalidad: Embeddings, descomposición en valores singulares, componentes principales\nAnálisis de conglomerados\n\n\nEvaluación\n\nTareas semanales (20%) para discutir en clase, compartidas en el repositorio y en nuestro espacio de trabajo de Posit Cloud.\nExamen parcial (40% práctico), con una componente oral.\nUn examen final (40% práctico), con una componente oral.\n\n\n\nMaterial\nCada semestre las notas cambian, en algunas partes considerablemente. Las de este semestre están en este repositorio, incluyendo ejemplos, ejercicios y tareas.\n\n\nReferencias principales\n\nAn Introduction to Statistical Learning, James et al. (2014)\nDeep Learning, Goodfellow, Bengio, y Courville (2016)\nTidy Modeling with R, Kuhn y Silge (2022)\n\n\n\nOtras referencias\n\nPattern Recognition and Machine Learning, Bishop (2006)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\nPredicción conforme\n\n\n\nSoftware\nPara hacer las tareas y exámenes pueden usar cualquier lenguaje o flujo de trabajo que les convenga (R o Python, por ejemplo) - el único requisito esté basado en código y no point-and-click. En lo posible utilizamos librerías especializadas que se pueden utilizar desde varias plataformas (keras, por ejemplo).\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc.\n\n\nGoodfellow, Ian, Yoshua Bengio, y Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.\n\n\nKuhn, M., y J. Silge. 2022. Tidy Modeling with R. O’Reilly Media. https://books.google.com.mx/books?id=9cJ6EAAAQBAJ.",
    "crumbs": [
      "Temario y referencias"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 ¿Qué es aprendizaje de máquina (machine learning)?\nMétodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. Nos interesan principalmente aquellas tareas o decisiones que se toman de manera repetida y donde el costo de cada error es relativamente bajo.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe también aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisión afecta directa e inmediatamente al entorno.\nLas tareas más apropiadas para este enfoque, en general, son aquellas en donde:\nAunque es posible extender lo que veremos a contextos más generales, cuando uno de los tres anteriores requisitos no se cumple es mejor usar o complementar con otros enfoques.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#qué-es-aprendizaje-de-máquina-machine-learning",
    "href": "01-introduccion.html#qué-es-aprendizaje-de-máquina-machine-learning",
    "title": "1  Introducción",
    "section": "",
    "text": "Existe una cantidad considerable de datos relevantes para aprender a ejecutar la tarea.\nEl costo por errores al ejecutar la tarea es relativamente bajo (al menos comparado con alternativas).\nLa tarea se repite de manera más o menos homogénea una cantidad grande de veces.\n\n\n\nEjemplos de tareas de aprendizaje:\n\nPredecir si un cliente de tarjeta de crédito va a caer en impago en los próximos tres meses.\nReconocer palabras escritas a mano (OCR).\nDetectar llamados de ballenas en grabaciones de boyas.\nEstimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica.\nDividir a los clientes de Netflix según sus gustos.\nRecomendar artículos a clientes de un programa de lealtad o servicio online.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Igual oír cada segundo de grabación de las boyas para saber si hay ballenas o no. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender más del problema que nos interesa: estas soluciones forman parte de un ciclo de análisis de datos donde podemos aprender de una forma más concentrada cuáles son características y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc). Las razones para no tomar un enfoque de reglas construidas “a mano”:\n\nCuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¿qué búsquedas www se enfocan en dar direcciones como resultados? ¿cómo filtrar comentarios no aceptables en foros?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "href": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "title": "1  Introducción",
    "section": "1.2 Ejemplo: reglas y aprendizaje",
    "text": "1.2 Ejemplo: reglas y aprendizaje\nLectura de un medidor mediante imágenes. Supongamos que en una infraestructura donde hay medidores análogos (de agua, electricidad, gas, etc.) que no se comunican. ¿Podríamos pensar en utilizar fotos tomadas automáticamente para medir el consumo?\nPor ejemplo, consideramos el siguiente problema (tomado de aquí, ver código y datos):\n\nlibrary(imager)\nlibrary(tidyverse)\nlibrary(gt)\nset.seed(437)\npath_img &lt;- \"../datos/medidor/\"\npath_full_imgs &lt;- list.files(path = path_img, full.names = TRUE)\nmedidor &lt;- load.image(sample(path_full_imgs, 1))\npar(mar = c(1, 1, 1, 1))\nplot(medidor, axes = FALSE)\n\n\n\n\n\n\n\n\nNótese que las imágenes y videos son matrices o arreglos de valores de pixeles, por ejemplo estas son las dimensiones para una imagen:\n\ndim(medidor)\n\n[1] 142 142   1   3\n\n\nEn este caso, la imagen es de 193 x 193 pixeles y tiene tres canales, o tres matrices de 193 x 193 donde la entrada de cada matriz es la intensidad del canal correspondiente. Buscámos hacer cálculos con estas matrices para extraer la información que queremos. En este caso, construiremos estos cálculos a mano.\nPrimero filtramos (extraemos canal rojo y azul, restamos, difuminamos y aplicamos un umbral):\n\nmedidor_rojo &lt;- medidor |&gt;  R() \nmedidor_azul &lt;- medidor |&gt; B()\nmedidor_1 &lt;- (medidor_rojo - medidor_azul) |&gt; isoblur(5)\naguja &lt;-  medidor_1 |&gt;  imager::threshold(\"90%\", approx = FALSE)\n\n\n\n\n\n\n\n\n\n\nLogramos extraer la aguja, aunque hay algo de ruido adicional. Una estrategia es extraer la componente conexa más grande (que debería corresponder a la aguja), y luego calcular su orientación. Una manera fácil es encontrar una recta que vaya del centro de la imagen hasta el punto más alejado del centro (aunque quizá puedes pensar maneras más robustas de hacer esto):\n\ncalcular_punta &lt;- function(pixset){\n  centro &lt;- floor(dim(pixset)[1:2] / 2)\n  # segmentar en componentes conexas\n  componentes &lt;- split_connected(pixset)\n  # calcular la más grande\n  num_pixeles &lt;- map_dbl(componentes, sum)\n  ind_maxima &lt;- which.max(num_pixeles)\n  pixset_tbl &lt;- as_tibble(componentes[[ind_maxima]]) |&gt; \n    mutate(dist = (x - centro[1])^2 + (y - centro[2])^2) |&gt; \n    top_n(1, dist)  |&gt; \n    mutate(x_1 = x - centro[1], y_1 = y - centro[2])\n  pixset_tbl[1, ] \n}\n\nY ahora podemos aplicar el proceso de arriba a todas la imágenes:\n\npath_imgs &lt;- list.files(path = path_img)\n\npath_full_imgs &lt;- list.files(path = path_img, full.names = TRUE)\n# en este caso los datos están etiquetados\ny_imagenes &lt;- path_imgs |&gt; str_sub(1, 3) |&gt; as.numeric()\n# procesar algunas imagenes\nset.seed(82)\nindice_imgs &lt;- sample(1:length(path_full_imgs), 500)\nangulos &lt;- path_full_imgs[indice_imgs] |&gt; \n    map( ~ load.image(.x)) |&gt;  \n    map(~ R(.x) - B(.x)) |&gt; \n    map( ~ isoblur(.x, 5)) |&gt; \n    map( ~ imager::threshold(.x, \"90%\")) |&gt; \n    map( ~ calcular_punta(.x)) |&gt; \n  bind_rows()\n\n\nangulos_tbl &lt;- angulos |&gt; \n  mutate(y_medidor = y_imagenes[indice_imgs])\nggplot(angulos_tbl, \n    aes(x = 180 * atan2(y_1, x_1) / pi + 90, y = y_medidor)) +\n  geom_point() + xlab(\"Ángulo\")\n\n\n\n\n\n\n\n\nEl desempeño no es muy malo pero tiene algunas fallas grandes. Quizá refinando nuestro pipeline de procesamiento podemos mejorarlo.\n\nPor el contrario, en el enfoque de aprendizaje, comenzamos con un conjunto de datos etiquetado (por una persona, por un método costoso, etc.), y utilizamos alguna estructura general para aprender a producir la respuesta a partir de las imágenes. Por ejemplo, en este caso podríamos una red convolucional sobre los valores de los pixeles de la imagen:\n\nlibrary(keras3)\n# usamos los tres canales de la imagen\nimagenes &lt;- map(path_full_imgs, ~ image_load(.x, target_size = c(64L, 64L)))\nimgs_array &lt;-  imagenes |&gt; map(~ image_to_array(.x)) \nimgs_array &lt;- map(imgs_array, ~ array_reshape(.x, c(1, 64, 64, 3)))\nx &lt;- abind::abind(imgs_array, along = 1)\nset.seed(25311)\nindices_entrena &lt;- sample(1:dim(x)[1], size = 4200)\nx_entrena &lt;- x[indices_entrena,,,]\ny_entrena &lt;- y_imagenes[indices_entrena] / 10\nx_valida &lt;- x[-indices_entrena,,,]\ny_valida &lt;- y_imagenes[-indices_entrena] / 10\n\n\nmodelo_aguja &lt;- keras_model_sequential() |&gt;\n  layer_rescaling(1./255) |&gt;\n  layer_random_rotation(0.05, fill_mode = \"nearest\") |&gt;\n  layer_random_zoom(0.05, fill_mode = \"nearest\") |&gt;\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5)) |&gt; \n  layer_max_pooling_2d(pool_size = c(2, 2)) |&gt;\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5)) |&gt; \n  layer_max_pooling_2d(pool_size = c(2, 2)) |&gt; \n  layer_conv_2d(filters = 16, kernel_size = c(3, 3)) |&gt; \n  layer_max_pooling_2d(pool_size = c(2, 2)) |&gt; \n  layer_flatten() |&gt; \n  layer_dropout(0.2) |&gt; \n  layer_dense(units = 100, activation = \"sigmoid\") |&gt;\n  layer_dropout(0.2) |&gt; \n  layer_dense(units = 100, activation = \"sigmoid\") |&gt;\n  layer_dropout(0.2) |&gt; \n  layer_dense(units = 1, activation = 'linear')\n\nAjustamos el modelo:\n\nmodelo_aguja |&gt; compile(\n  loss = \"mse\",\n  optimizer = optimizer_adam(learning_rate = 0.0005),\n  metrics = list(metric_mean_absolute_error())\n)                                                                                                        \n# Entrenar\nmodelo_aguja |&gt; fit(\n  x_entrena, y_entrena,\n  epochs = 200,\n  verbose = TRUE, \n  batch_size = 64,\n  validation_data = list(x_valida, y_valida)\n)\nsave_model(modelo_aguja, \"cache/modelo-aguja.keras\")\n\n\nmodelo &lt;- load_model(\"cache/modelo-aguja.keras\")\nmodelo\n\nModel: \"sequential_9\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ rescaling_7 (Rescaling)           │ (None, 64, 64, 3)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ random_rotation_7                 │ (None, 64, 64, 3)        │             0 │\n│ (RandomRotation)                  │                          │               │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ random_zoom_7 (RandomZoom)        │ (None, 64, 64, 3)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_26 (Conv2D)                │ (None, 60, 60, 32)       │         2,432 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_26 (MaxPooling2D)   │ (None, 30, 30, 32)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_27 (Conv2D)                │ (None, 26, 26, 32)       │        25,632 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_27 (MaxPooling2D)   │ (None, 13, 13, 32)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_28 (Conv2D)                │ (None, 11, 11, 16)       │         4,624 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_28 (MaxPooling2D)   │ (None, 5, 5, 16)         │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ flatten_9 (Flatten)               │ (None, 400)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_25 (Dropout)              │ (None, 400)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_25 (Dense)                  │ (None, 100)              │        40,100 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_26 (Dropout)              │ (None, 100)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_26 (Dense)                  │ (None, 100)              │        10,100 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_27 (Dropout)              │ (None, 100)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_27 (Dense)                  │ (None, 1)                │           101 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 248,969 (972.54 KB)\n Trainable params: 82,989 (324.18 KB)\n Non-trainable params: 0 (0.00 B)\n Optimizer params: 165,980 (648.36 KB)\n\n\nY observamos que obtenemos predicciones prometedoras:\n\npreds &lt;- predict(modelo, x[-indices_entrena,,,])\n\n23/23 - 1s - 24ms/step\n\npreds_tbl &lt;- tibble(y = y_imagenes[-c(indices_entrena)] / 10, preds = preds)\nggplot(preds_tbl, aes(x = preds, y = y)) +\n  geom_jitter(alpha = 0.5) +\n  geom_abline(colour = 'red')\n\n\n\n\n\n\n\n\nDe forma que podemos resolver este problema con algoritmos generales, sin tener que aplicar métodos sofisticados de procesamiento de imágenes. El enfoque de aprendizaje es particularmente efectivo cuando hay cantidades grandes de datos poco ruidosos, y aunque en este ejemplo los dos enfoques dan resultados razonables, en procesamiento de imágenes es cada vez más común usar redes neuronales grandes para resolver este tipo de problemas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#sec-medicioncostosa",
    "href": "01-introduccion.html#sec-medicioncostosa",
    "title": "1  Introducción",
    "section": "1.3 Ejemplo: mediciones costosas",
    "text": "1.3 Ejemplo: mediciones costosas\nEn algunos casos, el estándar de la medición que nos interesa es uno que es costoso de cumplir: a veces se dice que etiquetar los datos es costoso. Un ejemplo es producir las estimaciones de ingreso trimestral de un hogar que se recolecta en la ENIGH (ver aquí). En este caso particular, se utiliza esta encuesta como datos etiquetados para poder estimar el ingreso de otros hogares que no están en la muestra del ENIGH, pero para los que se conocen características de las vivienda, características de los integrantes, y otras medidas que son más fácilmente recolectadas en encuestas de opinión.\nVeremos otro ejemplo: estimar el valor de mercado de las casas en venta de una región. Es posible que tengamos un inventario de casas con varias de sus características registradas, pero producir estimaciones correctas de su valor de mercado puede requerir de inspecciones costosas de expertos, o tomar aproximaciones imprecisas de esta cantidad (por ejemplo, cuál es el precio ofertado).\nUtilizaremos datos de casas que se vendieron en Ames, Iowa en cierto periodo. En este caso, conocemos el valor a la que se vendió una casa. Buscamos producir una estimación para otras casas para las cuales conocemos características como su localización, superficie en metros cuadrados, año de construcción, espacio de estacionamiento, y así sucesivamente. Estas medidas son más fáciles de recolectar, y quisiéramos producir una estimación de su precio de venta en términos de estas medidas.\nEn este ejemplo intentaremos una forma simple de predecir.\n\nlibrary(tidymodels)\nlibrary(patchwork)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(68821)\n# dividir muestra\ncasas_split &lt;- initial_split(casas, prop = 0.75)\n# obtener muestra de entrenamiento\ncasas_entrena &lt;- training(casas_split)\n# graficar\ng_1 &lt;- ggplot(casas_entrena, aes(x = precio_miles)) +\n  geom_histogram()\ng_2 &lt;- ggplot(casas_entrena, aes(x = area_hab_m2, \n                          y = precio_miles, \n                          colour = condicion_venta)) +\n  geom_point() \ng_1 + g_2\n\n\n\n\n\n\n\n\nLa variable de condición de venta no podemos utilizarla para predecir, pues sólo la conocemos una vez que la venta se hace. Sin embargo, nos interesa principalemente entender qué sucede cuando las casas se venden en condiciones normales. Consideramos además del área habitable, por ejemplo, la calidad general de terminados:\n\nggplot(casas_entrena |&gt;  \n       filter(condicion_venta == \"Normal\") |&gt;  \n       mutate(calidad_grupo = \n        cut(calidad_gral, breaks = c(0, 5, 7, 8, 10))), \n  aes(x = area_hab_m2, \n      y = precio_miles,\n      colour = calidad_grupo)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\")\n\n\n\n\nPrecio vs área y calidad\n\n\n\n\nVemos que estas dos variables que hemos usado explican buena parte de la variación de los precios de las casas. Podemos examinar otras variables como la existencia y tamaño del garage:\n\nggplot(casas_entrena |&gt;  filter(condicion_venta == \"Normal\"),\n       aes(x = area_hab_m2, y = precio_miles, colour = area_garage_m2)) +\n  geom_point(alpha = 0.5) + facet_wrap(~ (area_garage_m2 &gt; 0))\n\n\n\n\n\n\n\n\nY quizá podríamos proponer una fórmula simple de la forma:\n\\[Precio = a[\\textrm{Calidad}] + b[\\textrm{Calidad}]\\, Area + c\\, \\textrm{AreaGarage} + d\\,\\textrm{TieneGarage}\\]\ndonde los valores de \\(a[\\textrm{Calidad}] , b[\\textrm{Calidad}], c, d\\) podríamos estimarlos de los datos. La pendiente de Area dependende de la calificación de la calidad de los terminados.\nNuestro proceso comenzaría entonces construir los datos para usar en el modelo:\n\nreceta_casas &lt;- \n  recipe(precio_miles ~ area_hab_m2 + calidad_gral + \n           area_garage_m2, \n         data = casas_entrena) |&gt;  \n  step_cut(calidad_gral, breaks = c(3, 5, 6, 7, 8)) |&gt;  \n  step_normalize(starts_with(\"area\")) |&gt; \n  step_mutate(tiene_garage = ifelse(area_garage_m2 &gt; 0, 1, 0)) |&gt; \n  step_dummy(calidad_gral) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) \n\nDefinimos el tipo de modelo que queremos ajustar, creamos un flujo y ajustamos\n\n# modelo\ncasas_modelo &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n# flujo\nflujo_casas &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(casas_modelo)\n# ajustar flujo\najuste &lt;- fit(flujo_casas, casas_entrena)\najuste\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_cut()\n• step_normalize()\n• step_mutate()\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                       (Intercept)                         area_hab_m2  \n                           111.124                              22.505  \n                    area_garage_m2                        tiene_garage  \n                            12.014                               3.230  \n               calidad_gral_X.3.5.                 calidad_gral_X.5.6.  \n                            30.104                              54.623  \n               calidad_gral_X.6.7.                 calidad_gral_X.7.8.  \n                            79.565                             119.639  \n              calidad_gral_X.8.10.   area_hab_m2_x_calidad_gral_X.3.5.  \n                           217.099                              -7.942  \n area_hab_m2_x_calidad_gral_X.5.6.   area_hab_m2_x_calidad_gral_X.6.7.  \n                             2.839                              14.141  \n area_hab_m2_x_calidad_gral_X.7.8.  area_hab_m2_x_calidad_gral_X.8.10.  \n                            14.221                              -1.421  \n\n\nY ahora podemos hacer predicciones para nuevos datos no observados en el entrenamiento:\n\nset.seed(8)\ncasas_prueba &lt;- testing(casas_split) \nejemplos &lt;- casas_prueba|&gt; sample_n(5)\npredict(ajuste, ejemplos) |&gt; \n  bind_cols(ejemplos |&gt; select(precio_miles, area_hab_m2)) |&gt; \n  arrange(desc(precio_miles)) |&gt; gt() |&gt; \n  fmt_number(columns = everything(), decimals = 1)\n\n\n\n\n\n\n\n.pred\nprecio_miles\narea_hab_m2\n\n\n\n\n242.3\n275.0\n152.9\n\n\n177.3\n181.0\n155.6\n\n\n169.3\n175.5\n132.1\n\n\n123.1\n133.0\n117.8\n\n\n115.6\n128.5\n90.2\n\n\n\n\n\n\n\nY finalmente podemos evaluar nuestro modelo. En este casos mostramos diversas métricas como ejemplo:\n\nmetricas &lt;- metric_set(mape, mae, rmse)\nmetricas(casas_prueba |&gt; bind_cols(predict(ajuste, casas_prueba)), \n     truth = precio_miles, estimate = .pred) |&gt; gt() |&gt; \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nmape\nstandard\n14.1\n\n\nmae\nstandard\n23.4\n\n\nrmse\nstandard\n33.3\n\n\n\n\n\n\n\n\ncasas_prueba_f &lt;- filter(casas_prueba,\n  condicion_venta %in% c(\"Normal\", \"Partial\", \"Abnorml\"))\nggplot(casas_prueba_f |&gt;\n       bind_cols(predict(ajuste, casas_prueba_f)),\n       aes(x = .pred, y = precio_miles)) +\n  geom_point() +\n  geom_abline(colour = \"red\") + facet_wrap(~ condicion_venta) +\n  xlab(\"Predicción (miles)\") + ylab(\"Precio (miles)\")\n\n\n\n\n\n\n\n\nEste modelo tiene algunos defectos y todavía tiene error considerablemente grande. La mejora sin embargo podemos cuantificarla con un modelo base o benchmark. En este caso utilizamos el siguiente modelo simple, cuya predicción es el promedio de entrenamiento:\n\n# nearest neighbors es grande, así que la predicción\n# es el promedio de precio en entrenamiento\ncasas_promedio &lt;- nearest_neighbor(\n    neighbors = 1000, weight_func = \"rectangular\") |&gt;\n  set_mode(\"regression\") |&gt; \n  set_engine(\"kknn\")\nworkflow_base &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(casas_promedio)\najuste_base &lt;- fit(workflow_base, casas_entrena)\nmetricas(casas_prueba |&gt; bind_cols(predict(ajuste_base, casas_prueba)), \n     truth = precio_miles, estimate = .pred)|&gt; gt() |&gt; \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nmape\nstandard\n33.4\n\n\nmae\nstandard\n54.8\n\n\nrmse\nstandard\n77.2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "1  Introducción",
    "section": "1.4 Aprendizaje supervisado y no supervisado",
    "text": "1.4 Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión\nProblemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html",
    "href": "02-principios-supervisado.html",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "",
    "text": "2.1 Población y pérdida\nSupongamos que tenemos una población grande de observaciones potenciales de la forma\n\\[(x_1, x_2, \\ldots, x_p, y) \\]\nY para esa población nos interesa predecir una variable respuesta \\(y\\) numérica en términos de variables de entrada disponibles \\(x = (x_1,x_2,\\ldots, x_p)\\):\n\\[(x_1, x_2, \\ldots, x_p) \\to y\\]\nEl proceso que produce la salida \\(y\\) a partir de las entradas es típicamente muy complejo y dificíl de describir de forma mecanística (por ejemplo, el ingreso dadas características de los hogares).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#población-y-pérdida",
    "href": "02-principios-supervisado.html#población-y-pérdida",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "",
    "text": "Ejemplo\nPara ilustrar esta discusión teórica, consideraremos datos simulados. La población está dada por el siguiente proceso generador de datos:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(gt)\ngenera_datos &lt;- function(n = 500, tipo = NULL){\n  dat_tbl &lt;- tibble(nse = runif(n, 0, 100)) |&gt;\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |&gt;\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |&gt; \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |&gt; \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |&gt; \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl &lt;- dat_tbl |&gt; \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |&gt; select(id, tipo, x = estudio_años, y = ingreso)\n}\n\nTenemos una sola entrada y una respuesta numérica, y una muestra se ve como sigue:\n\nset.seed(1234)\ndatos_tbl &lt;- genera_datos(n = 500, tipo = \"entrena\")\nggplot(datos_tbl, aes(x = x, y = y)) + geom_jitter(width = 0.3) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")\n\n\n\n\n\n\n\n\n\nBuscamos construir una función \\(f\\) tal que si observamos cualquier \\(x = (x_1, x_2, \\ldots, x_p) \\to y\\), entonces nuestra predicción es\n\\[\\hat{y} = f(x_1, x_2, \\ldots, x_p) = f(x).\\] Con esta regla o algoritmo \\(f\\) queremos predecir con buena precisión el valor de \\(y\\). Esta \\(f\\), como explicamos antes, puede ser producida de muy distintas maneras (experiencia, reglas a mano, datos, etc.)\nNuestra primera tarea es definir qué quiere decir predecir con buena precisión.\nPara hacer esto tenemos que introducir una medida del error, que llamamos en general función de pérdida.\n\n\n\n\n\n\nFunción de pérdida y error de predicción\n\n\n\nSi el verdadero valor observado es \\(y\\) y nuestra predicción es \\(f(x)\\), denotamos la pérdida asociada a esta observación como \\[L(y, f(x))\\] Para medir el desempeño general de la regla \\(f\\), consideramos su valor esperado, el error de predicción, que es el promedio sobre toda la población:\n\\[Err(f) = E[L(y, f(x))]\\]\nEste es el error que obtendríamos promediando las pérdidas sobre toda la población de interés.\n\n\nObservación: Para fijar ideas, podríamos usar por ejemplo la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) o la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\).\nObservación: en aprendizaje automático, evaluación del error casi siempre es un error promedio. Esto es aceptable cuando no hay riesgos de errores graves, y cuando haremos un número considerable de predicciones en el futuro.\nAl menos en teoría, podemos encontrar una \\(f\\) que minimiza esta pérdida:\n\n\n\n\n\n\nPredictor óptimo\n\n\n\nPara una población dada, el predictor óptimo (teórico) es\n\\[f^* = \\underset{f}{\\mathrm{argmin}} E[L(y, f(x))].\\]\nEs decir: el mínimo error posible que podemos obtener es \\(Err(f^*)\\). Para cualquier otro predictor \\(f\\) tenemos que \\(Err(f) \\geq Err(f^*).\\)\n\n\nPor ejemplo si usamos la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\), entonces puede mostrarse que\n\\[f^*(x) = E(y | x)\\] de forma que \\(f^*\\) es la media condicional de la \\(y\\) dado que sabemos que las entradas son \\(x\\). Si usáramos la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) entonces \\[f^*(x) = \\textrm{mediana}(y|x).\\] Distintas funciones de pérdida dan distintas soluciones teóricas. Por ejemplo, si existen valores atípicos en \\(y\\) producidos por errores de registro o medición, usar la pérdida absoluta puede dar mejores resultados que la cuadrática, que tiende a dar mayor peso a errores grandes.\nObservaciones:\n\nPodemos ver nuestra tarea entonces como una de ajuste de curvas: queremos aproximar tan bien como sea posible la función \\(f^*(x)\\).\nNo es simple decidir qué función de pérdida debería utilizarse para un problema dado de predicción.\nGeneralmente es una combinación de costos/beneficios del problema que tratamos, conveniencia computacional, y cómo se comportan los errores de nuestros predictores bajo distintas pérdidas. Sin embargo, al principio del proceso de construcción de modelos es mejor escoger una métrica simple que capture a grandes rasgos el comportamiento que esperamos (pérdida cuadrática, absoluta o logarítmica por ejemplo).\nMuchas veces es mejor considerar el problema de selección de la pérdida desde dos ángulos: el primero es computacional y de propiedades de la predicción, y el segundo tiene que ver con costos y beneficios asociados al problema que queremos resolver. Para el primero, alguna de las pérdidas estándar (como las que vimos arriba, cuadrática y absoluta, o logarítmica) son usualmente suficiente. En el segundo enfoque, el análisis es generalmente involucra más aspectos particulares del problema y generalmente tiene que hacerse de manera ad-hoc.\n\n\n\nEjemplo\nSupongamos que nos interesa minimizar la pérdida cuadrática. Si tomamos una muestra muy grande (para este problema), podemos aproximar la predicción óptima directamente. Abajo graficamos nuestra muestra chica de datos junto con una buena aproximación del predictor óptimo:\n\npoblacion_tbl &lt;- genera_datos(n = 50000, tipo = \"poblacion\")\n# calcular óptimo\npreds_graf_tbl &lt;- poblacion_tbl |&gt; \n  group_by(x) |&gt; # condicionar a x\n  summarise(.pred = mean(y)) |&gt; # media en cada grupo\n  mutate(predictor = \"_óptimo\")\n# graficar con una muestra grande\nggplot(datos_tbl, aes(x = x)) +\n  geom_jitter(aes(y = y), colour = \"red\") + \n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), \n    linewidth = 1.1) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "href": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.2 Estimando el desempeño y datos de prueba",
    "text": "2.2 Estimando el desempeño y datos de prueba\nPara obtener una estimación de la pérdida para una función \\(f\\) que usamos para hacer predicciones, podemos tomar una muestra de datos del proceso generador:\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\nCompararíamos entonces las respuestas observadas \\(\\mathbf{y^{(i)}}\\) con las predicciones \\(f(\\mathbf{x^{(i)}})\\). Ahora resumimos evaluando el error promedio sobre los datos de prueba. El error de prueba de \\(f\\) es\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , f(\\mathbf{x}^{(i)}))\\] Por ejemplo, si usamos la pérdida cuadrática,\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m (\\mathbf{y}^{(i)} - f(\\mathbf{x}^{(i)}))^2\\] Si \\(m\\) es grande, entonces tenemos por la ley de los grandes números que \\[Err(f) \\approx \\widehat{Err} (f)\\] Podemos también estimar el error de estimación de \\(\\widehat{Err}(f)\\) con técnicas estándar, por ejemplo bootstrap o aproximación normal.\nObervación: nótese que en estos cálculos no es necesario hacer ningún supuesto acerca de \\(f\\), que en este argumento está fija y no utiliza la muestra de prueba.\n\nEjemplo: óptimo\nSupongamos que \\(f\\) es el predictor óptimo que obtuvimos arriba (pero esto aplica para cualquier otra función \\(f\\) que usemos para hacer predicciones). Tomamos una muestra de prueba, y evaluamos usando la raíz de la pérdida cuadrática media:\n\nprueba_tbl &lt;- genera_datos(n = 2000, tipo = \"prueba\")\neval_tbl &lt;- prueba_tbl |&gt;  \n  left_join(preds_graf_tbl, by = \"x\") \nresumen_tbl &lt;- eval_tbl |&gt;  \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen &lt;- function(resumen_tbl){\n  resumen_tbl |&gt; \n    select(-.estimator) |&gt; \n    pivot_wider(names_from = tipo, values_from = .estimate) |&gt; \n    gt() |&gt; \n    fmt_number(where(is_double), decimals = 0)\n}\nfmt_resumen(resumen_tbl)\n\n\n\n\n\n\n\npredictor\n.metric\nprueba\n\n\n\n\n_óptimo\nrmse\n49\n\n\n\n\n\n\n\nEste es nuestro error de prueba. Como la muestra de prueba no es muy grande, podríamos usar un método estándar para estimar su precisión, por ejemplo con bootstrap.\n\n\nEjemplo: regla\nAhora probemos con otro predictor, por ejemplo, supongamos que estamos usando la regla de “cada año de escolaridad aumenta ingresos potenciales en 20 unidades”, un predictor construido con reglas manuales que es\n\nf_regla &lt;- function(x){\n  20 * x\n}\n\nAbajo lo graficamos en comparación con el modelo óptimo:\n\naños_x &lt;- tibble(x = seq(0, 17, by = 0.5))\npreds_regla_tbl &lt;- años_x |&gt; \n  mutate(.pred = f_regla(x), predictor = \"regla\")\npreds_graf_tbl &lt;- bind_rows(preds_regla_tbl, preds_graf_tbl)\nggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.2) +\n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), size = 1.1) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\neval_tbl &lt;- prueba_tbl |&gt;  \n  left_join(preds_graf_tbl, by = \"x\", relationship = \"many-to-many\") \nresumen_tbl &lt;- eval_tbl |&gt;  \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_tbl)\n\n\n\n\n\n\n\npredictor\n.metric\nprueba\n\n\n\n\n_óptimo\nrmse\n49\n\n\nregla\nrmse\n91\n\n\n\n\n\n\n\nObserva que el error es considerablemente mayor que el error que obtuvimos con el predictor óptimo del ejemplo anterior. Quisiéramos buscar algoritmos que tengan mejor desempeño aprendiendo de datos anteriores.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#aprendizaje",
    "href": "02-principios-supervisado.html#aprendizaje",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.3 Aprendizaje supervisado",
    "text": "2.3 Aprendizaje supervisado\nEn aprendizaje supervisado, buscamos construir la función \\(f\\) de manera automática usando datos. Supongamos entonces que tenemos un conjunto de datos etiquetados (sabemos la \\(y\\) correspondiente a cada \\(x\\)):\n\\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\]\nque llamamos conjunto de entrenamiento.\nUn algoritmo de aprendizaje (aprender de los datos automáticamente) es una regla que asigna a cada conjunto de entrenamiento \\({\\mathcal L}\\) una función \\(\\hat{f}\\):\n\\[{\\mathcal L} \\to \\hat{f} = f_{\\mathcal L} \\]\nUna vez que construimos la función \\(\\hat{f}\\), podemos hacer predicciones.\nEl desempeño del predictor particular \\(\\hat{f}\\) se mide igual que antes: observamos otra muestra \\({\\mathcal T}\\), que llamamos muestra de prueba,\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\ny calculamos el error de prueba. Si suponemos que \\(m\\) es suficientemente grande:\n\\[ \\widehat{Err}(\\hat{f}) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , \\hat{f}(\\mathbf{x}^{(i)})) \\]\nes una buena aproximación del error de predicción \\(Err(\\hat{f})\\).\nAdicionalmente, definimos otra cantidad de menor interés, el error de entrenamiento, como\n\\[\\overline{err} = \\frac{1}{N}\\sum_{i=1}^N L(y^{(i)} , \\hat{f}(x^{(i)})).\\] que es una medida de qué tan bien se ajusta a \\(\\hat{f}\\) a los datos con los que se entrenó \\(\\hat{f}\\). Usualmente esta cantidad no es apropiada para medir el desempeño de un predictor, pues el algoritmo \\(\\hat{f}\\) incluye las “respuestas” \\(y_i\\) en su construcción, de forma que tiende a ser una estimación optimista del error de predicción.\n\nEjemplo: vecinos más cercanos\nConsideremos usar un método de \\(k\\)-vecinos más cercanos para resolver este problema. Este método es simple: si queremos hacer una predicción en las entradas \\(x\\), buscamos los puntos de entrenamiento con entradas \\(x^{(i)}\\) más cercanas a \\(x\\), que denotamos como \\(N_k(x)\\). Tomamos las \\(y\\) correspondientes a estas \\(x\\) y las usamos para hacer nuestra predicción:\n\\[f_2(x) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x)} y^{(i)}\\]\nPrimero obtendremos una muestra de entrenamiento:\n\nset.seed(12)\nentrena_tbl &lt;- genera_datos(n = 20, tipo = \"entrena\")\n\nEn nuestro ejemplo, en lugar de usar un número fijo de vecinos, utilizaremos 10% de los datos más cercanos al punto donde queremos predecir:\n\n# modelo\nmodelo_kvecinos &lt;- nearest_neighbor(\n    neighbors = nrow(entrena_tbl) * 0.1, \n    weight_func = \"gaussian\") |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"kknn\")\n# preprocesamiento\nreceta &lt;- recipe(y ~ x, data = entrena_tbl |&gt; select(x, y))\n# flujo\nflujo &lt;- workflow() |&gt; \n  add_recipe(receta) |&gt; \n  add_model(modelo_kvecinos)\n# Ajustamos flujo\nflujo_ajustado_vecinos &lt;- fit(flujo, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl &lt;- bind_rows(prueba_tbl, entrena_tbl) \nresumen_vmc_tbl &lt;- \n  predict(flujo_ajustado_vecinos, eval_tbl) |&gt; \n  mutate(predictor = \"vecinos\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_vmc_tbl)\n\n\n\n\n\n\n\npredictor\n.metric\nentrena\nprueba\n\n\n\n\nvecinos\nrmse\n36\n65\n\n\n\n\n\n\n\nEl error de prueba, que es el que nos interesa hacer chico, es considerablemente grande. Si graficamos podemos ver el problema:\n\npreds_vmc &lt;- predict(flujo_ajustado_vecinos, años_x) |&gt; \n  bind_cols(años_x) |&gt; mutate(predictor = \"vecinos\")\npreds_graf_tbl &lt;- bind_rows(preds_vmc, preds_graf_tbl |&gt; \n  filter(predictor == \"_óptimo\"))\ng_1 &lt;- ggplot(entrena_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |&gt; filter(predictor != \"regla\"), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  geom_point(aes(y = y), colour = \"red\") +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\n\n\n\n\nDonde vemos que este método intenta interpolar los datos, capturando ruido y produciendo variaciones que lo alejan del modelo óptimo. Esto lo notamos en lo siguiente:\n\nHay una brecha grande entre el error de entrenamiento y el error predictivo.\nEsta estimación de vecinos más cercanos es muy dependiente de la muestra de entrenamiento que obtengamos, pues intenta casi interpolar los datos. Esto sugiere alta variabilidad de las predicciones dependiendo de la muestra particular de entrenamiento que utilizamos.\nDecimos que este predictor está sobreajustado.\n\n\n\nEjemplo: regresión lineal\nAhora intentaremos con un modelo lineal. En este caso, utilizamos un predictor de la forma\n\\[f(x) = \\beta_0 + \\beta_1x\\] Usamos la muestra de entrenamiento para encontrar la \\(\\beta_0\\) y \\(\\beta_1\\) que minimizar el error sobre los datos disponibles de entrenamiento, lo cual es un problema de optimización relativamente fácil. Usamos entonces \\[\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\] para hacer nuestras predicciones.\n\nmodelo_lineal &lt;- linear_reg() |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"lm\")\nflujo_lineal &lt;- workflow() |&gt; \n  add_recipe(receta) |&gt; \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal &lt;- fit(flujo_lineal, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl &lt;- bind_rows(prueba_tbl, entrena_tbl) \nresumen_lineal_tbl &lt;- \n  predict(flujo_ajustado_lineal, eval_tbl) |&gt; \n  mutate(predictor = \"lineal\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(bind_rows(resumen_vmc_tbl, resumen_lineal_tbl))\n\n\n\n\n\n\n\npredictor\n.metric\nentrena\nprueba\n\n\n\n\nvecinos\nrmse\n36\n65\n\n\nlineal\nrmse\n49\n56\n\n\n\n\n\n\n\nY el desempeño de este método es mejor que vecinos más cercanos (ver columna de prueba).\n\npreds_1 &lt;- predict(flujo_ajustado_lineal, tibble(x = 0:17)) |&gt; \n  bind_cols(tibble(x = 0:17, predictor = \"lineal\"))\npreds_graf_tbl &lt;- bind_rows(preds_1, preds_graf_tbl)\ng_1 &lt;- ggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.1) +\n  geom_line(data = preds_graf_tbl |&gt; filter(predictor %in% c(\"_óptimo\", \"lineal\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\n\n\n\n\nEn este caso:\n\nNo hay brecha tan grande entre el error de entrenamiento y el error predictivo\nObservamos patrones claros de desajuste: el predictor lineal no captura el patrón curvo que presentan los datos: en la parte media de las \\(x\\) tiende a producir predicciones demasiado altas y lo contario ocurre en los extremos\nDecimos que esté modelo presenta subajuste.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "href": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.4 Entendiendo el error de predicción",
    "text": "2.4 Entendiendo el error de predicción\nEstos dos ejemplos de predictores tienen mal desempeño (comparado con el óptimo por distintas razones. Para entender qué pasa, consideramos los residuales de cada ajuste, para un caso de prueba:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})\\] Esta cantidad puede tener un valor positivo o negativo grande, lo que indica errores grandes. Sea \\(f^*\\) el predictor óptimo que explicamos arriba. Entonces, en primer lugar:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{(f^* (\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{reducible} + \\underbrace{(\\mathbf{y}- f^*(\\mathbf{x}))}_\\text{irreducible}.\\] donde vemos que si las dos cantidades de la derecha están cercanas a cero, entonces el residual es cercano a cero (la predicción es precisa):\n\nError irreducible: no depende de nuestro algoritmo, sino de la información que tenemos en \\(x\\) para predecir \\(y\\). Si queremos hacer esté error más chico, necesitamos incluir otras variables \\(x\\) relevantes para predecir \\(y\\).\nError reducible: qué tan lejos nuestro método está del óptimo. Podemos mejorar este error seleccionando nuestra muestra de entrenamiento y método de predicción \\(\\hat{f}\\) de manera adecuada.\n\nEn nuestros dos ejemplos anteriores, el error reducible era considerablemente grande (como podemos verificar comparando con el predictor óptimo, que sólo sufre de error irreducible). Pero la razón por la que ese error reducible es grande es diferente en cada caso.\nPara explicar la diferencia, podemos considerar \\(f_{lim},\\) el predictor que obtendríamos con nuestro método si ajustáramos nuestro método con la población completa, de manera que \\(\\hat{f_{\\mathcal{L}}}\\to f_{\\lim}\\) cuando el tamaño de la muestra de entrenamiento \\({\\mathcal{L}}\\) se hace muy grande.\nPodemos refinar nuestra descomposición y escribir:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{f^* (\\mathbf{x}) - f_{\\lim}(\\mathbf{x})}_\\text{sesgo-especificacion} +\n  \\underbrace{f_{\\lim}(\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{error-estimacion} +\n  \\underbrace{y - f^*(\\mathbf{x})}_\\text{irreducible}.\\]\nEl error reducible ahora se descompone en dos partes:\n\nEl sesgo de especificacion: que se debe a la incapacidad de nuestro modelo de capturar la forma del predictor óptimo, incluso conociendo toda la población. Este término no depende de la muestra de entrenamiento: depende de la capacidad de nuestro método para aprender en condiciones ideales.\nEl error de estimación: este error resulta de que tenemos información limitada de la población, y nuestro ajuste se aleja de lo que obtendríamos con información completa. Esta parte del error varía dependiendo de la muestra particular de entrenamiento que utilizamos.\n\nEn nuestros dos ejemplos, intuímos que vecinos más cercanos sufre más de error de estimación y regresión lineal de sesgo de especificación, lo cual verificamos más adelante.\nPodemos refinar aún más nuestra descomposición considerando qué pasa con distintas muestras del mismo tamaño para entender mejor el error de estimación. Si consideramos el valor esperado de nuestra predicción a lo largo de las posibles muestras \\(\\mathcal L\\) que podemos extraer, descomponemos el segundo término como:\n\\[\\hat{f_{\\mathcal{L}}}(\\mathbf{x}) - f_{\\lim}(\\mathbf{x})  = \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) -  E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) + E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - f_{\\lim}(\\mathbf{x}) \\] donde el valor esperado es sobre todas las muestras de entrenamiento de un tamaño fijo \\(n\\) que podríamos obtener. El primer término puede llamarse variabilidad, mientras que el segundo es el sesgo que obtenemos al usar una muestra \\(n\\) finita (para algunos métodos, el segundo término puede ser igual a cero). Desde este punto de vista, podemos hacer también la descomposición:\n\\[\n\\begin{align}\n\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = &  \\underbrace{f^* (\\mathbf{x}) - f_{lim}(\\mathbf{x})}_\\text{sesgo-especificacion}  +  \\\\ &\\underbrace{f_{lim}(\\mathbf{x}) - E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{sesgo-estimacion} +  \\underbrace{E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{variabilidad} + \\\\  &\\underbrace{ \\mathbf{y} - f^*(\\mathbf{x})}_\\text{irreducible}.\n\\end{align}\n\\]\nTenemos entonces cuatro términos:\n\nEl sesgo de especificación mide la capacidad del modelo de utilizar datos de muestras cada vez más grandes. No depende de una muestra particular ni su tamaño.\nEl sesgo de estimación mide en promedio qué tan lejos está la estimación del ideal con datos completos, y depende de la naturaleza de la muestra de entrenamiento, incluyendo su tamaño.\nLa variabilidad es el único término que depende de la muestra particular que usamos. También depende del tamaño de muestra que utilizamos.\n\nLos dos primeros términos usualmente se agrupan en un sólo término de sesgo, y obtenemos la siguiente definición usual:\n\n\n\n\n\n\nDescomposición sesgo-varianza\n\n\n\nEl error total (la diferencia entre observado y nuestra predicción) se descompone como:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{f^* (\\mathbf{x}) - E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{sesgo} +   \\underbrace{E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{variabilidad} + \\underbrace{y - f^*(\\mathbf{x})}_\\text{irreducible}.\\]\n\n\nQue explica qué sucede con distintas posibles muestras de entrenamiento de tamaño fijo. El sesgo en este caso significa en promedio qué tan lejos nuestro predictor está del óptimo, y la variabilidad qué tanto puede variar nuestra predicción con respecto al promedio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "href": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.5 Ejemplo: fuentes de error",
    "text": "2.5 Ejemplo: fuentes de error\nVamos a ver qué sucede con nuestros dos métodos si utilizamos una muestra grande:\n\nmuestra_grande_tbl &lt;- sample_n(poblacion_tbl, 10000) |&gt; \n  mutate(tipo = \"entrena\")\nmodelo_kvecinos &lt;- nearest_neighbor(\n    neighbors = nrow(muestra_grande_tbl) * 0.10, \n    weight_func = \"gaussian\") |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"kknn\")\n# Ajustamos (no es necesario usar la población completa para este ejemplo)\nflujo_vecinos &lt;- workflow() |&gt; \n  add_recipe(receta) |&gt; \n  add_model(modelo_kvecinos)\nflujo_ajustado_vecinos_limite &lt;- fit(flujo_vecinos, muestra_grande_tbl)\nflujo_ajustado_lineal_limite &lt;- fit(flujo_lineal, muestra_grande_tbl)\n\neval_tbl &lt;- bind_rows(prueba_tbl, muestra_grande_tbl)\nresumen_vecinos_lim_tbl &lt;- \n  predict(flujo_ajustado_vecinos_limite, eval_tbl) |&gt; \n  mutate(predictor = \"vecinos_limite\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nresumen_lineal_lim_tbl &lt;- \n  predict(flujo_ajustado_lineal_limite, eval_tbl) |&gt; \n  mutate(predictor = \"lineal_limite\") |&gt; \n  bind_cols(eval_tbl) |&gt; \n  group_by(predictor, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(bind_rows(resumen_vmc_tbl, \n  resumen_lineal_tbl, \n  resumen_vecinos_lim_tbl, \n  resumen_lineal_lim_tbl) |&gt; arrange(predictor))\n\n\n\n\n\n\n\npredictor\n.metric\nentrena\nprueba\n\n\n\n\nlineal\nrmse\n49\n56\n\n\nlineal_limite\nrmse\n54\n54\n\n\nvecinos\nrmse\n36\n65\n\n\nvecinos_limite\nrmse\n49\n49\n\n\n\n\n\n\n\n¿Qué patrones ves en esta tabla? Podemos también graficar para entender mejor qué está pasando:\n\npreds_1 &lt;- predict(flujo_ajustado_vecinos_limite, tibble(x = 0:17)) |&gt; \n  bind_cols(tibble(x = 0:17, predictor = \"vecinos_limite\"))\npreds_2 &lt;- predict(flujo_ajustado_lineal_limite, tibble(x = 0:17)) |&gt; \n  bind_cols(tibble(x = 0:17, predictor = \"lineal_limite\"))\npreds_graf_tbl &lt;- bind_rows(preds_1, preds_2, preds_graf_tbl) |&gt; \n  mutate(predictor = factor(predictor))\ng_1 &lt;- ggplot(datos_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |&gt; \n            filter(str_detect(predictor, \"vecinos|_óptimo\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Vecinos\") \ng_2 &lt;- ggplot(datos_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |&gt; \n            filter(str_detect(predictor, \"lineal|_óptimo\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Lineal\") \ng_1 + g_2 + plot_layout(guides = 'collect')\n\n\n\n\n\n\n\n\nEsto patrón sugiere que:\n\nNuestro método de vecinos más cercanos tiene errores bajos por sesgo, pero tiene error considerable por sobreajuste o variabilidad.\nNuestro método lineal no tiene mucha variabilidad (el estimado con una muestra grande es casi igual al de la muestra de entrenamiento), sino más bien por sesgo.\nEl error por sesgo se reduce usando métodos más flexibles o menos restringidos que puedan capturar patrones claros en los datos.\nPara reducir la variabilidad podemos usar métodos más simples o restringidos que no capturen tanto ruido.\nEl balance de complejidad correcto depende del tamaño de muestra de entrenamiento.\nEl error irreducible se puede reducir incorporando información adicional relevante a las entradas.\n\n\n\n\n\n\n\nComplejidad y error de predicción\n\n\n\nPara un tamaño de muestra de entrenamiento fijo,\n\nMétodos de predicción más flexibles o complejos tienden a sufrir más de error por variabilidad, pues dependen fuertemente de la muestra utilizada.\nMétodos de predicción más rígidos o simples tienden a sufrir más error por sesgo, pues dependen menos de la muestra utilizada.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "href": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.6 Agregando más información y error irreducible",
    "text": "2.6 Agregando más información y error irreducible\nPodemos ver qué sucede cuando tenemos disponibles más variables relevantes. En este caso, probaremos con dos entradas:\n\ngenera_datos_2 &lt;- function(n = 500, tipo = NULL){\n  dat_tbl &lt;- tibble(nse = runif(n, 0, 100)) |&gt;\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |&gt;\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |&gt; \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |&gt; \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |&gt; \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl &lt;- dat_tbl |&gt; \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |&gt; select(id, tipo, x_1 = estudio_años, x_2 = nse,  y = ingreso)\n}\n\n\nentrena_tbl &lt;- genera_datos_2(20, tipo = \"entrena\")\nprueba_tbl &lt;- genera_datos_2(500, tipo = \"prueba\")\nreceta_2 &lt;- recipe(y ~ x_1 + x_2, data = entrena_tbl)\nflujo_lineal &lt;- workflow() |&gt; \n  add_recipe(receta_2) |&gt; \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal &lt;- fit(flujo_lineal, entrena_tbl)\npredict(flujo_ajustado_lineal, bind_rows(entrena_tbl, prueba_tbl)) |&gt; \n  bind_cols(bind_rows(entrena_tbl, prueba_tbl)) |&gt;\n  group_by(tipo) |&gt; \n  rmse(truth = y, estimate = .pred) |&gt; gt() |&gt; \n  fmt_number(.estimate, decimals = 1)\n\n\n\n\n\n\n\ntipo\n.metric\n.estimator\n.estimate\n\n\n\n\nentrena\nrmse\nstandard\n27.1\n\n\nprueba\nrmse\nstandard\n31.5\n\n\n\n\n\n\n\nY vemos cómo inmediatamente redujimos el error de predicción: en este caso, aunque la variabilidad aumentó un poco (tenemos más parámetros que estimar vs el modelo con una sola variable), la reducción en el sesgo y en el error irreducible es tan grande que el desempeño es muy superior. Examina el caso de vecinos más cercanos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "href": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.7 Acerca de la estimación del error de predicción",
    "text": "2.7 Acerca de la estimación del error de predicción\nCuando usamos una muestra de prueba limitada, podemos evaluar la precisión de nuestra estimación del error de predicción usando por ejemplo el bootstrap. En nuestro ejemplo anterior podríamos hacer los siguiente:\n\nlibrary(infer)\npreds &lt;- predict(flujo_ajustado_lineal, bind_rows(prueba_tbl)) |&gt; \n  bind_cols(prueba_tbl) \npreds |&gt; \n  generate(reps = 1000, type = \"bootstrap\") |&gt; \n  group_by(replicate, tipo) |&gt; \n  rmse(truth = y, estimate = .pred) |&gt; \n  select(replicate, tipo, stat = .estimate) |&gt;\n  get_ci(level = 0.90) |&gt; \n  gt() |&gt; fmt_number(where(is_double), decimals = 1)\n\n\n\n\n\n\n\nlower_ci\nupper_ci\n\n\n\n\n29.9\n33.0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#resumen",
    "href": "02-principios-supervisado.html#resumen",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.8 Resumen",
    "text": "2.8 Resumen\n\n\n\n\n\n\nTarea fundamental del análisis supervisado\n\n\n\n\nUsando datos de entrenamiento \\({\\mathcal L}\\), construimos una funcion \\(\\hat{f}\\) para predecir. Estas funciones se ajustan usualmente intentando estimar directamente el predictor óptimo \\(f^*(x)\\) (si lo conocemos teóricamente), o indirectamente intentando minimizar la pérdida sobre el conjunto de entrenamiento.\nSi observamos nuevos valores \\(\\mathbf{x}\\), nuestra predicción es \\(\\hat{y} = \\hat{f}(\\mathbf{x})\\).\nBuscamos que cuando observemos nuevos casos para predecir, nuestro error de predicción sea bajo en promedio (\\(Err\\) sea bajo).\nUsualmente estimamos \\(Err\\) mediante una muestra de prueba o validación \\({\\mathcal T}\\).\nNos interesan métodos de construir \\(\\hat{f}\\) que produzcan errores de predicción bajos.\n\n\n\n\nNótese que el error de entrenamiento se calcula sobre la muestra \\({\\mathcal L}\\) que se usó para construir \\(\\hat{f}\\), mientras que el error de predicción se estima usando una muestra independiente \\({\\mathcal T}\\).\n\\(\\hat{Err}\\) es una estimación razonable de el error de predicción \\(Err\\) (por ejemplo, \\(\\hat{Err} \\to Err\\) cuando el tamaño de la muestra de prueba crece), pero \\(\\overline{err}\\) típicamente es una estimación mala del error de predicción.\nNótese también que aunque generalmente podemos ajustar reduciendo el error de entrenamiento, lo que queremos es reducir el error de prueba: es decir, el error fuera de la muestra de entrenamiento.\n\n\n\n\n\n\n\nReduciendo el error de predicción\n\n\n\nPara reducir el error de predicción, podemos:\n\nIncluir variables relevantes que reduzcan el error irreducible\nReducir variabilidad usando métodos más estables o menos complejos\nReducir sesgo usando métodos más flexibles\nUsar métodos con la estructura adecuada para el problema\n\nGeneralmente 2 y 3 están en contraposición, a lo que muchas veces se le llama equilibrio de varianza y sesgo. Los puntos 1 y 4 generalmente mejoran los resultados reduciendo tanto sesgo como variabilidad.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios-supervisado.html#resolver-problemas-con-aprendizaje-automático",
    "href": "02-principios-supervisado.html#resolver-problemas-con-aprendizaje-automático",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.9 Resolver problemas con aprendizaje automático",
    "text": "2.9 Resolver problemas con aprendizaje automático\nEn este curso nos concentraremos en la construcción, evaluación y mejora de modelos predictivos. Para que estas ideas funcionen en problemas reales, hay más aspectos a considerar que no discutiremos con tanto detalle, pues en general están muy ligados al problema particular de predicción que nos interesa (y muchas veces son considerablemente más difíciles de la teoría y los algoritmos):\n\nPara entender exactamente cuál es el problema que queremos resolver se requiere trabajo analítico considerable, y también trabajo en entender aspectos del área o negocio donde nos interesa usar aprendizaje máquina. Muchas veces es fácil resolver un problema muy preciso, que tenemos a la mano, pero que más adelante nos damos cuenta de que no es útil.\nEl punto anterior indentificar las métricas que queremos monitorear y mejorar, lo cual no siempre es claro. Hablaremos de este aspecto sólo desde un punto vista técnico. Optimizar métricas incorrectas es poco útil en el mejor de los casos, y en los peores pueden causar daños. Evitar esto requiere monitoreo constante de varios aspectos del funcionamiento de nuestros modelos y sus consecuencias.\n¿Cómo poner en producción modelos y mantenerlos? Un flujo apropiado de trabajo, que comienza con pipelines de preproceso y heurísticas simples, para después utilizar modelos de aprendizaje automático, seguido de monitoreo y entrenamiento continuo son cruciales para tener éxito con este enfoque.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "03-metodos-locales.html",
    "href": "03-metodos-locales.html",
    "title": "3  Métodos locales no estructurados",
    "section": "",
    "text": "3.1 Controlando complejidad\nPrimero examinamos cómo controlamos el nivel de complejidad para un método local como \\(k\\) vecinos más cercanos. La idea es que:\nComenzamos con un ejemplo simple en dimensión baja:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos locales no estructurados</span>"
    ]
  },
  {
    "objectID": "03-metodos-locales.html#controlando-complejidad",
    "href": "03-metodos-locales.html#controlando-complejidad",
    "title": "3  Métodos locales no estructurados",
    "section": "",
    "text": "Más complejidad: Si tomamos \\(k\\) demasiado chica, cada estimación usa pocos datos y puede ser ruidosa (incurrimos en variabilidad). Sin embargo, el predictor resultante puede ajustarse a patrones locales y globales.\nMenos complejidad: Si tomamos \\(k\\) demasiado grande, cada estimación usa potencialmente datos no relevantes muy lejanos a donde queremos predecir (incurrimos en sesgo), sin embargo cada estimación es más estable pues utiliza más datos.\n\n\n\nEjemplo\n\nlibrary(tidyverse)\nlibrary(gt)\nauto &lt;- read_csv(\"../datos/auto.csv\")\n# seleccionar variables y poner en sistema métrico\ndatos &lt;- auto |&gt; \n  select(name, weight, year, mpg, displacement) |&gt; \n  mutate(\n    peso_kg = weight * 0.45359237,\n    rendimiento_kpl = mpg * (1.609344 / 3.78541178), \n    año = year\n  )\n\nVamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (75% para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba):\n\nlibrary(tidymodels)\nset.seed(121)\ndatos_split &lt;- initial_split(datos, prop = 0.75)\ndatos_entrena &lt;- training(datos_split)\ndatos_prueba &lt;- testing(datos_split)\nnrow(datos_entrena)\n\n[1] 294\n\nnrow(datos_prueba)\n\n[1] 98\n\n\nVamos a usar año y peso de los coches para predecir su rendimiento:\n\nggplot(datos_entrena, \n  aes(x = peso_kg, y = rendimiento_kpl, colour = año)) +\n  geom_point()\n\n\n\n\n\n\n\n\nProbaremos con varios valores para \\(k\\), el número de vecinos más cercanos. La función de predicción ajustada es entonces:\n\n# nótese que normalizamos entradas - esto también es importante\n# hacer cuando hacemos vecinos más cercanos, pues en otro caso\n# las variables con escalas más grandes dominan el cálculo\nvmc_1 &lt;- nearest_neighbor(neighbors = tune(), weight_func = \"gaussian\") |&gt;  \n  set_engine(\"kknn\") |&gt;  \n  set_mode(\"regression\")\nreceta_vmc &lt;- recipe(\n  rendimiento_kpl ~ peso_kg + año, datos_entrena) |&gt; \n  step_normalize(all_predictors()) \nflujo_vecinos &lt;- workflow() |&gt;  \n  add_recipe(receta_vmc) |&gt; \n  add_model(vmc_1)\n# definir parámetros que nos interesa explorar\nvecinos_params &lt;- parameters(neighbors(range = c(1, 100)))\n# definir cuáles valores de los parámetros exploramos\nvecinos_grid &lt;- grid_regular(vecinos_params, levels = 100)\nmis_metricas &lt;- metric_set(rmse)\n\nEn la siguiente gráfica mostramos cómo cambia el error de los las predicciones sobre la muestra de prueba separada de la de entrenamiento. En este caso le llamaremos muestra de validación porque más adelante veremos que puede ser conveniente dividir en entrenamiento-validación-prueba en lugar de usar sólo 2 particiones:\n\nr_split &lt;- manual_rset(list(datos_split), \"validación\")\nvecinos_eval_tbl &lt;- tune_grid(flujo_vecinos,\n                            resamples = r_split,\n                            grid = vecinos_grid,\n                            metrics = mis_metricas) \nvecinos_ajustes_tbl &lt;- vecinos_eval_tbl |&gt;\n  unnest(cols = c(.metrics)) |&gt; \n  select(id, neighbors, .metric, .estimate)\nggplot(vecinos_ajustes_tbl, aes(x = neighbors, y = .estimate)) +\n  geom_line() + geom_point() +\n  ylab(\"Error de validación\") + xlab(\"Vecinos\")\n\n\n\n\n\n\n\n\nDonde obtenemos más o menos lo que esperaríamos: modelos con muy pocos vecinos o demasiados vecinos se desempeñan relativamente mal.\nSeleccionaremos el mejor modelo según el error estimado de predicción y visualizamos primero nuestras predicciones y los datos de entrenamiento de la siguiente forma:\n\nmejor_rmse &lt;- select_best(vecinos_eval_tbl, metric = \"rmse\")\najuste_1 &lt;- finalize_workflow(flujo_vecinos, mejor_rmse) |&gt; \n  fit(datos_entrena)\ndat_graf &lt;- tibble(peso_kg = seq(900, 2200, by = 10)) |&gt; \n  crossing(tibble(año = c(70, 75, 80)))\ndat_graf &lt;- dat_graf |&gt; \n  mutate(pred_1 = predict(ajuste_1, dat_graf) |&gt; pull(.pred))\nggplot(datos_entrena, aes(x = peso_kg, group = año, colour = año)) +\n  geom_point(aes(y = rendimiento_kpl), alpha = 0.6) + \n  geom_line(data = dat_graf, aes(y = pred_1),  linewidth = 1.2)\n\n\n\n\n\n\n\n\nEl método parece funcionar razonablemente bien para este problema simple. Sin embargo, si el espacio de entradas no es de dimensión baja, entonces podemos encontrarnos con dificultades.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos locales no estructurados</span>"
    ]
  },
  {
    "objectID": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "href": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "title": "3  Métodos locales no estructurados",
    "section": "3.2 La maldición de la dimensionalidad",
    "text": "3.2 La maldición de la dimensionalidad\nEl método de k-vecinos más cercanos funciona mejor cuando\n\nNo es necesario hacer \\(k\\) demasiado grande, de forma que terminemos tomando valores lejanos que inducen sesgo.\nNo es necesario hacer \\(k\\) demasiado chica, de forma que nuestras predicciones sean inestables.\n\n\n\n\n\n\n\nMaldición de la dimensionalidad\n\n\n\nEn dimensión alta, para la mayoría de las \\(\\mathbf{x}\\) donde queremos hacer predicciones típicamente no existen vecinos cercanos, aún para conjuntos de entrenamiento muy grandes.\n\n\nEsto implica que para tamaños típicos \\(n\\) de muestra de entrenamiento:\n\nSi tomamos \\(k\\) chica, el sesgo por especificación es chico (muestras muy grandes), pero el sesgo de estimación puede ser grande pues estamos de todas formas obligados a buscar vecinos lejos de donde queremos predecir. La variabilidad también es alta pues usamos pocos datos para cada predicción.\nSi tomamos \\(k\\) más grande, el sesgo por especificación tiende ser más grande (pues promediamos sobre regiones relativamente grandes). Perdemos la supuesta ventaja del método local, aún cuando quizá reduzcamos el sesgo de estimación.\nPara que una \\(k\\) chica tenga sesgo de estimación bajo, el tamaño \\(n\\) de la muestra de entrenamiento tiene que ser gigantesca.\n\n\nEjemplo\nConsideremos que la salida Y es determinística \\(Y = e^{-8\\sum_{j=1}^p x_j^2}\\). Vamos a usar 1-vecino más cercano para hacer predicciones, con una muestra de entrenamiento de 1000 casos. Generamos $x^{i}’s uniformes en \\([ 1,1]\\), para \\(p = 2\\), y calculamos la respuesta \\(Y\\) para cada caso:\n\nfun_exp &lt;- function(x) exp(-8 * sum(x ^ 2))\nx &lt;- map(1:1000, ~ runif(2, -1, 1))\ndat &lt;- tibble(x = x) |&gt; \n        mutate(y = map_dbl(x, fun_exp))\nggplot(dat |&gt; mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), \n       aes(x = x_1, y = x_2, colour = y)) + geom_point()\n\n\n\n\n\n\n\n\nLa mejor predicción en \\(x_0 = (0,0)\\) es \\(f((0,0)) = 1\\). El vecino más cercano al origen es\n\ndat &lt;- dat |&gt; mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |&gt; \n  arrange(dist_origen)\nmas_cercano &lt;- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  &lt;list&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 &lt;dbl [2]&gt; 0.995      0.0261\n\nmas_cercano$x[[1]]\n\n[1] -0.025090354  0.007277334\n\n\nNuestra predicción es entonces \\(\\hat{f}(0)=\\) 0.994555, que es bastante cercano al valor verdadero (1).\nAhora intentamos hacer lo mismo para dimensión \\(p=8\\).\n\nx &lt;- map(1:1000, ~ runif(8, -1, 1))\ndat &lt;- tibble(x = x) |&gt; \n       mutate(y = map_dbl(x, fun_exp))\ndat &lt;- dat |&gt; mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |&gt; \n  arrange(dist_origen)\nmas_cercano &lt;- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  &lt;list&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 &lt;dbl [8]&gt; 0.104       0.532\n\nmas_cercano$x[[1]]\n\n[1]  0.30027994  0.36774993 -0.06613864 -0.03673154  0.12260975  0.16718980\n[7] -0.01866598 -0.09308947\n\n\nY el resultado es un desastre. Nuestra predicción es\n\nmas_cercano$y\n\n[1] 0.1038249\n\n\nNecesitariamos una muestra de alrededor de un millón de casos para obtener resultados no tan malos (haz pruebas).\n¿Qué es lo que está pasando? La razón es que en dimensiones altas, los puntos de la muestra de entrenamiento están muy lejos unos de otros, y están cerca de la frontera, incluso para tamaños de muestra relativamente grandes como n = 1000. Cuando la dimensión crece, la situación empeora exponencialmente.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos locales no estructurados</span>"
    ]
  },
  {
    "objectID": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "href": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "title": "3  Métodos locales no estructurados",
    "section": "3.3 Regresión lineal en dimensión alta",
    "text": "3.3 Regresión lineal en dimensión alta\nAhora intentamos algo similar con una función que es razonable aproximar con una función lineal:\n\nfun_cuad &lt;- function(x)  0.5 * (1 + x[1])^2\n\nY queremos predecir para \\(x=(0,0,\\ldots,0)\\), cuyo valor exacto es\n\nfun_cuad(0)\n\n[1] 0.5\n\n\nLos datos se generan de la siguiente forma:\n\nsimular_datos &lt;- function(p = 40){\n    x &lt;- map(1:1000,  ~ runif(p, -1, 1))\n    dat &lt;- tibble(x = x) |&gt; mutate(y = map_dbl(x, fun_cuad)) \n    dat\n}\n\nPor ejemplo para dimensión baja \\(p=1\\) (nótese que una aproximación lineal es razonable):\n\nejemplo &lt;- simular_datos(p = 1) |&gt; mutate(x = unlist(x))\nggplot(ejemplo, aes(x = x, y = y)) + geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAhora repetimos el proceso en dimensión \\(p=40\\): simulamos las entradas, y aplicamos un vecino más cercano\n\nvmc_1 &lt;- function(dat){\n    dat &lt;- dat |&gt; \n        mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |&gt; \n        arrange(dist_origen)\n        mas_cercano &lt;- dat[1, ]\n        mas_cercano$y\n}\nset.seed(834)\ndat &lt;- simular_datos(p = 40)\nvmc_1(dat)\n\n[1] 1.206478\n\n\nEste no es un resultado muy bueno. Sin embargo, regresión se desempeña considerablemente mejor:\n\nregresion_pred &lt;- function(dat){\n    p &lt;- length(dat$x[[1]])\n    dat_reg &lt;- cbind(\n        y = dat$y, \n        x = matrix(unlist(dat$x), ncol = p, byrow=T)) |&gt; \n        as.data.frame()\n    mod_lineal &lt;- lm(y ~ ., dat = dat_reg)\n    origen &lt;- data.frame(matrix(rep(0, p), 1, p))\n    names(origen) &lt;- names(dat_reg)[2:(p+1)]\n    predict(mod_lineal, newdata = origen)\n}\nregresion_pred(dat)\n\n        1 \n0.6677861 \n\n\nLa razón de este mejor desempeño de regresión es que en este caso, el modelo lineal explota la estructura aproximadamente lineal del problema (¿cuál estructura lineal? haz algunas gráficas). Nota: corre este ejemplo varias veces con semilla diferente.\nSolución: vamos a hacer varias simulaciones, para ver qué modelo se desempeña mejor.\n\nsims &lt;- map(1:200, function(i){\n    dat &lt;- simular_datos(p = 40)\n    vmc_y &lt;- vmc_1(dat)\n    reg_y &lt;- regresion_pred(dat)\n    tibble(rep = i, \n           error = c(abs(vmc_y - 0.5), abs(reg_y - 0.5)), \n            tipo = c(\"vmc\", \"regresion\"))\n}) |&gt; bind_rows()\nggplot(sims, aes(x = tipo, y = error)) + geom_boxplot() \n\n\n\n\n\n\n\n\nAsí que típicamente el error de vecinos más cercanos es más alto que el de regresión. El error esperado es para vmc es más de doble que el de regresión:\n\nsims |&gt; group_by(tipo) |&gt; \n  summarise(media_error = mean(error)) |&gt; \n  gt()\n\n\n\n\n\n\n\ntipo\nmedia_error\n\n\n\n\nregresion\n0.1662124\n\n\nvmc\n0.3542532\n\n\n\n\n\n\n\nLo que sucede más específicamente es que en regresión lineal utilizamos todos los datos para hacer nuestra estimación en cada predicción. Si la estructura del problema es aproximadamente lineal, entonces regresión lineal explota la estructura para hacer pooling de toda la información para construir predicción con sesgo y varianza bajas. En contraste, vecinos más cercanos sufre de varianza alta.\n\n\n\n\n\n\nMétodos locales sin estructura\n\n\n\nLos métodos locales muchas veces no funcionan bien en dimensión alta. La razón es que:\n\nEl sesgo es alto, pues promediamos puntos muy lejanos al lugar donde queremos predecir (aunque tomemos pocos vecinos cercanos).\nEn el caso de que encontremos unos pocos puntos cercanos, la varianza también puede ser alta porque promediamos relativamente pocos vecinos.\n\nMétodos con más estructura global, apropiada para el problema, logran explotar información de puntos que no están tan cerca del lugar donde queremos predecir.\n\n\nMuchas veces el éxito en la predicción depende de establecer esas estructuras apropiadas ya sea mediante:\n\nEstructura en nuestros modelos (por ejemplo, efectos lineales cuando variables tienen efectos aproximadamente lineales, árboles cuando hay algunas interacciones, redes convolucionales para procesamiento de imágenes y señales, dependencia del contexto en modelos de lenguaje, etc.)\nReducción de dimensionalidad apropiada (por ejemplo, embeddings basados en otros modelos, o técnicas como componentes principales/descompocisión en valores singulares, etc.).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Métodos locales no estructurados</span>"
    ]
  },
  {
    "objectID": "04-lineales-ingenieria.html",
    "href": "04-lineales-ingenieria.html",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "",
    "text": "Ejemplos\nQueremos predecir las ventas futuras anuales \\(y\\) de una tienda que se va a construir en un lugar dado. Las variables que describen el lugar son \\(x_1 = trafico\\_peatones\\), \\(x_2=trafico\\_coches\\). En una aproximación simple, podemos suponer que la tienda va a capturar una fracción de esos tráficos que se van a convertir en ventas. Quisieramos predecir con una función de la forma \\[f_\\beta (peatones, coches) = \\beta_0 + \\beta_1\\, peatones + \\beta_2\\, coches.\\] Por ejemplo, después de un análisis estimamos que\nEntonces haríamos predicciones con \\[\\hat{f}(peatones, coches) = 1000000 +  4\\,peatones + 3\\, coches.\\] El modelo lineal es más flexible de lo que parece en una primera aproximación, porque tenemos libertad para construir las variables de entrada a partir de nuestros datos. Por ejemplo, si tenemos una tercera variable \\(estacionamiento\\) que vale 1 si hay un estacionamiento cerca o 0 si no lo hay, podríamos definir las variables\nDonde la idea de agregar \\(x_4\\) es que si hay estacionamiento entonces vamos a capturar una fracción adicional del trafico de coches, y la idea de \\(x_3\\) es que la tienda atraerá más nuevas visitas si hay un estacionamiento cerca. Buscamos ahora modelos de la forma \\[f_\\beta(x_1,x_2,x_3,x_4) = \\beta_0 + \\beta_1x_1 + \\beta_2 x_2 + \\beta_3 x_3 +\\beta_4 x_4\\] y podríamos obtener después de nuestra análisis las estimaciones - \\(\\hat{\\beta}_0 = 800000\\) (ventas base) - \\(\\hat{\\beta}_1 = 4\\) - \\(\\hat{\\beta}_2 = (300)*0.005 = 1.5\\) - \\(\\hat{\\beta}_3 = 400000\\) (ingreso adicional si hay estacionamiento por nuevo tráfico) - \\(\\hat{\\beta}_4 = (300)*0.02 = 6\\) (ingreso adicional por tráfico de coches si hay estacionamiento)\ny entonces haríamos predicciones con el modelo \\[\\hat{f} (x_1,x_2,x_3,x_4) = 800000 + 4\\, x_1 + 1.5 \\,x_2 + 400000\\, x_3 +6\\, x_4\\]\nEn este caso particular, teníamos peatones, coches y estacionamiento, pero construimos también el producto de tráfico de coches y estacionamiento, pues la relación de ventas con coches es diferente dependiendo de sí la tienda tiene estacionamiento o no.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Métodos lineales e ingenería de entradas</span>"
    ]
  },
  {
    "objectID": "04-lineales-ingenieria.html#aprendizaje-de-coeficientes-ajuste",
    "href": "04-lineales-ingenieria.html#aprendizaje-de-coeficientes-ajuste",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.1 Aprendizaje de coeficientes (ajuste)",
    "text": "4.1 Aprendizaje de coeficientes (ajuste)\nEn el ejemplo anterior, los coeficientes fueron calculados (o estimados) usando experiencia, reglas, argumentos teóricos, o quizá otras fuentes de datos (como estudios o encuestas, conteos, etc.)\nAhora quisiéramos construir un algoritmo para aprender estos coeficientes del modelo \\[f_\\beta (x) = \\beta_0 + \\beta_1 x_1 + \\cdots \\beta_p x_p\\] a partir de una muestra de entrenamiento de datos históricos de tiendas que hemos abierto antes: \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] El criterio de ajuste (algoritmo de aprendizaje) más usual para regresión lineal es el de mínimos cuadrados.\nConstruimos las predicciones (ajustados) para la muestra de entrenamiento: \\[ f_\\beta (x^{(i)}) = \\beta_0 + \\beta_1 x_1^{(i)}+ \\cdots + \\beta_p x_p^{(i)}\\]\nY consideramos las diferencias de los ajustados con los valores observados:\n\\[e^{(i)} = y^{(i)} - f_\\beta (x^{(i)})\\]\nLa idea entonces es minimizar la suma de los residuales al cuadrado, para intentar que la función ajustada pase lo más cercana a los puntos de entrenamiento que sea posible. La función de pérdida que utilizamos más frecuentemente es la pérdida cuadrática, dada por:\n\\[L(\\beta) = \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\]\n\n\n\n\n\n\nMínimos cuadrados para regresión lineal\n\n\n\nBuscamos encontrar: \\[\\hat{\\beta} = \\mathrm{arg\\,min}_{\\beta} L(\\beta) = \\mathrm{arg\\,min}_{\\beta}\\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] donde \\[f_\\beta (x^{(i)}) = \\beta_0 + \\beta_1 x_1^{(i)}+ \\cdots + \\beta_p x_p^{(i)}\\]\n\n\nHay varias maneras de resolver este problema: puede hacerse analíticamente con álgebra lineal, o con algún método numérico como descenso máximo (que puede escalarse fácilmente). Típicamente la función objetivo es convexa, y la solución es única, excepto en casos degenerados que podremos evitar más adelante usando regularización.\nObservación: Como discutimos al final de la sección anterior, minimizar directamente el error de entrenamiento para encontrar los coeficientes puede resultar en en un modelo sobreajustado/con varianza alta/ruidoso. Hay cuatro grandes estrategias para mitigar este problema: restringir o estructurar la familia de funciones, penalizar la función objetivo, perturbar la muestra de entrenamiento, o cambiar el proceso de minimización perturbando la función objetivo en cada paso o deteniendo el proceso antes de llegar a un mínimo sobreajustado. El método mas común es cambiar la función objetivo, que discutiremos más adelante en la sección de regularización.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Métodos lineales e ingenería de entradas</span>"
    ]
  },
  {
    "objectID": "04-lineales-ingenieria.html#ingeniería-de-entradas",
    "href": "04-lineales-ingenieria.html#ingeniería-de-entradas",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.2 Ingeniería de entradas",
    "text": "4.2 Ingeniería de entradas\nAlgunas veces, encontrar la estructura apropiada puede requerir más trabajo que simplemente escoger una familia de modelos. Por ejemplo, en el caso de precios de casa, vimos que podríamos mejorar el ajuste haciendo que el coeficiente de área habitable dependiera de la calidad de los terminados (Sección 1.3).\nUsualmente tendremos que hacer varias transformaciones para obtener buen desempeño de un modelo lineal. En la siguientes secciones mostramos algunas de las más usuales.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Métodos lineales e ingenería de entradas</span>"
    ]
  },
  {
    "objectID": "04-lineales-ingenieria.html#variables-categóricas",
    "href": "04-lineales-ingenieria.html#variables-categóricas",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.3 Variables categóricas",
    "text": "4.3 Variables categóricas\nEn primer lugar, podemos incluir variables categóricas creando variables numéricas 0-1 para cada categoría. Por ejemplo para la variable calidad sótano:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(83)\ncasas_split &lt;- initial_split(casas, prop = 0.75)\ncasas_entrena &lt;- training(casas_split)\ncasas_entrena |&gt; count(calidad_sotano)\n\n# A tibble: 5 × 2\n  calidad_sotano     n\n  &lt;chr&gt;          &lt;int&gt;\n1 Ex                89\n2 Fa                23\n3 Gd               457\n4 TA               500\n5 &lt;NA&gt;              26\n\n\nEl mejor nivel es Ex (excelente), luego sigue Gd (bueno), luego Fa (razonable) y finalmente TA (típico)). Hay otro nivel Po (Malo) que no aparece en estos datos.\nEn primer lugar, podemos codificar los valores faltantes, que en este caso indican casas sin sótano:\n\nreceta_na &lt;- recipe(~ calidad_sotano, casas_entrena) |&gt; \n  step_unknown(calidad_sotano, new_level = \"no_sótano\") |&gt; \n  step_relevel(calidad_sotano, ref_level = \"TA\")\ncasas_preproc &lt;- prep(receta_na) |&gt; juice()\ncasas_preproc |&gt; count(calidad_sotano)\n\n# A tibble: 5 × 2\n  calidad_sotano     n\n  &lt;fct&gt;          &lt;int&gt;\n1 TA               500\n2 Ex                89\n3 Fa                23\n4 Gd               457\n5 no_sótano         26\n\n\nAhora convertimos a codificación dummy:\n\nset.seed(7)\nreceta_dummy &lt;- \n  recipe( ~ calidad_sotano, casas_entrena) |&gt; \n  step_unknown(calidad_sotano, new_level = \"no_sótano\") |&gt; \n  step_relevel(calidad_sotano, ref_level = \"TA\") |&gt; \n  step_dummy(calidad_sotano, keep_original_cols = TRUE)\n# preparar receta\nreceta_dummy_prep &lt;- prep(receta_dummy) \n# extrae los datos de entrenamiento preprocesados\nreceta_dummy_prep |&gt; juice() |&gt; \n  sample_n(10) |&gt; gt() |&gt; \n  tab_options(table.font.size = 10)\n\n\n\n\n\n\n\ncalidad_sotano\ncalidad_sotano_Ex\ncalidad_sotano_Fa\ncalidad_sotano_Gd\ncalidad_sotano_no_sótano\n\n\n\n\nTA\n0\n0\n0\n0\n\n\nGd\n0\n0\n1\n0\n\n\nEx\n1\n0\n0\n0\n\n\nEx\n1\n0\n0\n0\n\n\nTA\n0\n0\n0\n0\n\n\nTA\n0\n0\n0\n0\n\n\nTA\n0\n0\n0\n0\n\n\nTA\n0\n0\n0\n0\n\n\nTA\n0\n0\n0\n0\n\n\nTA\n0\n0\n0\n0\n\n\n\n\n\n\n\nNótese que no hay columna para el nivel TA, que tomamos como referencia. Incluir esta columna sería redundante, pues tenemos una constante en el predictor. En general, cuando una variable categórica tiene \\(k\\) niveles, esta codificación produce \\(k-1\\) columnas binarias.\nVeamos qué pasa cuando preprocesamos datos de prueba (para después poder hacer predicciones):\n\nprueba_casas &lt;- testing(casas_split)\n# supongamos que un nuevo nivel aparece\nprueba_casas$calidad_sotano[1] &lt;- \"no visto antes\"\ndatos &lt;- bake(receta_dummy_prep, prueba_casas)\n\nWarning: ! There are new levels in `calidad_sotano`: \"no visto antes\".\nℹ Consider using step_novel() (`?recipes::step_novel()`) \\ before\n  `step_unknown()` to handle unseen values.\n• New levels will be coerced to `NA` by `step_unknown()`.\n\n\nWarning: ! There are new levels in `calidad_sotano`: NA.\nℹ Consider using step_unknown() (`?recipes::step_unknown()`) before\n  `step_dummy()` to handle missing values.\n\n\nEn este caso, podemos hacer nuestro flujo más robusto incluyendo un nuevo nivel en los factores donde pondremos casos no vistos. Modificamos nuestra receta:\n\nreceta_dummy &lt;- \n  recipe( ~ calidad_sotano, casas_entrena) |&gt; \n  step_novel(calidad_sotano, new_level = \"nuevo\") |&gt; \n  step_unknown(calidad_sotano, new_level = \"no_sótano\") |&gt; \n  step_relevel(calidad_sotano, ref_level = \"TA\") |&gt; \n  step_dummy(calidad_sotano, keep_original_cols = TRUE)\n# preparar receta\nreceta_dummy_prep &lt;- prep(receta_dummy) \n\n\nprueba_casas &lt;- testing(casas_split)\n# supongamos que un nuevo nivel aparece\nprueba_casas$calidad_sotano[1] &lt;- \"no visto antes\"\ndatos &lt;- bake(receta_dummy_prep, prueba_casas)\ndatos |&gt; head() |&gt; gt() |&gt; tab_options(table.font.size = 10)\n\n\n\n\n\n\n\ncalidad_sotano\ncalidad_sotano_Ex\ncalidad_sotano_Fa\ncalidad_sotano_Gd\ncalidad_sotano_nuevo\ncalidad_sotano_no_sótano\n\n\n\n\nnuevo\n0\n0\n0\n1\n0\n\n\nEx\n1\n0\n0\n0\n0\n\n\nTA\n0\n0\n0\n0\n0\n\n\nGd\n0\n0\n1\n0\n0\n\n\nTA\n0\n0\n0\n0\n0\n\n\nTA\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nY podemos ignorar el nuevo nivel al hacer predicciones (que equivale a ponerlo en la categoría de referencia, que en este caso es TA), o podemos lidiar de manera ad-hoc con este nivel.\nOtro problema con el que podemos encontrarnos es variables categóricas que son muy ralas. Por ejemplo, una variable que tiene muchas categorías y algunas de ellas tienen muy pocos datos, además de que es probable que observemos nuevas categorías en el futuro. Por ejemplo, para la variable de zona:\n\ncasas_entrena |&gt; count(nombre_zona) |&gt; \n  arrange(desc(n)) |&gt; gt() |&gt; tab_options(table.font.size = 10)\n\n\n\n\n\n\n\nnombre_zona\nn\n\n\n\n\nNAmes\n163\n\n\nCollgCr\n113\n\n\nOldTown\n80\n\n\nEdwards\n74\n\n\nSomerst\n65\n\n\nGilbert\n61\n\n\nSawyer\n59\n\n\nNridgHt\n58\n\n\nNWAmes\n57\n\n\nBrkSide\n45\n\n\nSawyerW\n42\n\n\nCrawfor\n39\n\n\nMitchel\n36\n\n\nIDOTRR\n31\n\n\nNoRidge\n30\n\n\nTimber\n26\n\n\nClearCr\n22\n\n\nStoneBr\n20\n\n\nSWISU\n18\n\n\nBlmngtn\n14\n\n\nBrDale\n14\n\n\nMeadowV\n12\n\n\nNPkVill\n8\n\n\nVeenker\n7\n\n\nBlueste\n1\n\n\n\n\n\n\n\nEn este caso, tenemos muchas categorías, algunas con muy pocos datos, y es posible que observemos nuevos datos. Una técnica es agrupar los datos de baja cardinalidad en un nuevo nivel (incluyendo categorías no observadas en entrenamiento):\n\nreceta_vecindario_1 &lt;- \n  recipe( ~ nombre_zona, casas_entrena) |&gt;\n  step_other(nombre_zona, threshold = 0.01, other = \"otras\") \nreceta_vecindario &lt;- receta_vecindario_1 |&gt; \n  step_dummy(nombre_zona)\n# preparar receta\nreceta_vecindario_prep &lt;- prep(receta_vecindario_1)\nset.seed(8231)\nreceta_vecindario_prep |&gt; juice() |&gt; \n  count(nombre_zona) |&gt; arrange(desc(n)) |&gt; gt()\n\n\n\n\n\n\n\nnombre_zona\nn\n\n\n\n\nNAmes\n163\n\n\nCollgCr\n113\n\n\nOldTown\n80\n\n\nEdwards\n74\n\n\nSomerst\n65\n\n\nGilbert\n61\n\n\nSawyer\n59\n\n\nNridgHt\n58\n\n\nNWAmes\n57\n\n\nBrkSide\n45\n\n\nSawyerW\n42\n\n\nCrawfor\n39\n\n\nMitchel\n36\n\n\nIDOTRR\n31\n\n\nNoRidge\n30\n\n\nTimber\n26\n\n\nClearCr\n22\n\n\nStoneBr\n20\n\n\nSWISU\n18\n\n\notras\n16\n\n\nBlmngtn\n14\n\n\nBrDale\n14\n\n\nMeadowV\n12\n\n\n\n\n\n\n\nEn este caso, las zonas de baja frecuencia fueron agrupadas en la categoría “otras”. Si observamos un nuevo nivel al momento de predicción:\n\nprueba_casas &lt;- testing(casas_split)\n# supongamos que un nuevo nivel aparece\nprueba_casas$nombre_zona[1] &lt;- \"Xochimilco\"\ndatos &lt;- bake(prep(receta_vecindario), prueba_casas)\ndatos |&gt; head() |&gt;  gt() |&gt; tab_options(table.font.size = 10)\n\n\n\n\n\n\n\nnombre_zona_BrDale\nnombre_zona_BrkSide\nnombre_zona_ClearCr\nnombre_zona_CollgCr\nnombre_zona_Crawfor\nnombre_zona_Edwards\nnombre_zona_Gilbert\nnombre_zona_IDOTRR\nnombre_zona_MeadowV\nnombre_zona_Mitchel\nnombre_zona_NAmes\nnombre_zona_NoRidge\nnombre_zona_NridgHt\nnombre_zona_NWAmes\nnombre_zona_OldTown\nnombre_zona_Sawyer\nnombre_zona_SawyerW\nnombre_zona_Somerst\nnombre_zona_StoneBr\nnombre_zona_SWISU\nnombre_zona_Timber\nnombre_zona_otras\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nEl proceso general es (ver por ejemplo esta lista):\n\n\n\n\n\n\nVariables categóricas\n\n\n\n\nEstablecemos los niveles que puede tener cada variable, incluyendo la posibilidad de categorías nuevas al momento de predecir, y categorías para valores no disponibles (NAs) (es posible también imputar con algún método en caso necesario).\nReorganizamos factores dependiendo del problema. Por ejemplo, incluir categorías de baja frecuencia en una categoría separada, o manipulaciones ad-hoc dependiendo del problema.\nSustituimos variables categóricas con \\(K\\) niveles en \\(K-1\\) columnas indicadoras de los niveles (estableciendo) alguna categoría como referencia. Esto no es estrictamente necesario en otros métodos, o si utilizamos regularización (ver sección siguiente).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Métodos lineales e ingenería de entradas</span>"
    ]
  },
  {
    "objectID": "04-lineales-ingenieria.html#interacciones",
    "href": "04-lineales-ingenieria.html#interacciones",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.4 Interacciones",
    "text": "4.4 Interacciones\nOtra manera de expandir nuestro modelo es la utilización de interacciones, que muchas veces son clave para tener éxito con modelos lineales. Vimos ejemplos de interacciones en el ejemplo de las casas (Sección 1.3) y en el primer ejemplo de ventas de tiendas que dependían del tráfico.\n\nEjemplo\nSi \\(x_1\\) es el área en metros cuadrados de una casa, y \\(x_2\\) una calificación numérica de su calidad, podemos considerar el modelo sin interacciones:\n\\[\\beta_0 + \\beta_1x_1 + \\beta_2 x_2\\]\nPero no tiene mucho sentido que el efecto marginal de \\(x_1\\) sea constante para cualquier nivel de calidad, y tampoco que la calidad de terminados agregue una cantidad fija al precio de la casa sin tomar en cuenta su tamaño. Podemos remediar esto creando una nueva variable que es le producto de \\(x_1\\) y \\(x_2\\):\n\\[x_3 = x_1 x_2\\]\ny agregando, nuestro predictor para precio es\n\\[ \\beta_0 +  \\beta_1x_1 + \\beta_2 x_2 + \\beta_3 x_1x_2\\]\nAhora notemos que para \\(x_2\\) fija, el modelo es\n\\[(\\beta_0 + \\beta_2x_2) + (\\beta_1 + \\beta_3x_2)x_1 = \\gamma_0 + \\gamma_1x_1\\] De modo que es lineal en \\(x_1\\). La diferencia es que cuando cambia \\(x_2\\), la recta que ajustamos es diferente.\n\nbeta &lt;- c(0, 50, 100, 20)\ncombs_tbl &lt;- crossing(x_1 = seq(2, 20, by = 1), x_2 = seq(0, 10, by = 2)) |&gt; \n  mutate(x_3 = x_1 * x_2) |&gt; \n  mutate(pred = beta[1] + beta[2]*x_1 + beta[3]*x_2 + beta[4]*x_3)\nggplot(combs_tbl, aes(x = x_1, y = pred, group = x_2, colour = x_2)) +\n  geom_line()\n\n\n\n\n\n\n\n\nY vemos que cuando la calidad es baja, el precio por metro cuadrado es más bajo que cuando la calidad es alta. Otra manera de pensar esto es que la inclusión de la interacción produce curvas marginales que rotan dependiendo del valor de otras variables.\nPregunta. ¿puedes pensar en otros casos donde las interacciones deben jugar un papel importante?\n\n\n\n\n\n\nInteracciones\n\n\n\n\nTransformamos las variables categóricas a dummies. Transformamos las variables numéricas si es necesario (normalizar, aplicar transformación no lineal, etc.)\nIncluimos interacciones de la siguiente forma:\n\n\nPara la interaccion de dos variables numéricas \\(x_1\\) y \\(x_2\\) agregamos el producto \\(x_3 = x_1x_2\\).\nPara interacción de una variable categórica \\(g\\) con una numérica \\(x\\) podemos hacer el mismo procedimiento multiplicando la variable categórica por cada una de las variable dummy que creamos a partir de \\(g\\). Esto en efecto produce una pendiente para \\(x\\) dependiendo del valor que toma \\(g\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Métodos lineales e ingenería de entradas</span>"
    ]
  },
  {
    "objectID": "04-lineales-ingenieria.html#ejemplo-precios-de-casas",
    "href": "04-lineales-ingenieria.html#ejemplo-precios-de-casas",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.5 Ejemplo: precios de casas",
    "text": "4.5 Ejemplo: precios de casas\nEn el ejemplo de precios de casas, por ejemplo, es claro que el efecto en ventas del tamaño de las áreas (habitable, garage, etc.) depende de la calidad de los terminados, como vimos en la introducción. En la siguiente receta de preprocesamiento:\n\nCortamos calidad general en 5 grupos: este paso no es necesario y puede dañar el desempeño, pero es consistente con el análisis que hicimos anteriormente.\nLidiamos con niveles nuevos y los ponemos en una categoría “nuevo” (para que nuestro modelo no falle al momento de predicción)\nPonemos los faltantes de calidad sotano y garage en una categoría nueva (no tienen sótano y/o garage)\nAgrupamos las zonas con pocas observaciones en una categoría de “Otros”\nQuitamos los NA’s de área garage y área sotano, que deben ser igual a 0 cuando no existen estas características.\nCreamos variables dummy de todas las variables categóricas\nIncluimos interacciones de distintas áreas con las dummy correspondientes, incluyendo zona con área habitable\nFinalmente, eliminamos para el ajuste aquellas variables que tengan varianza cercana a cero (500 /1 quiere decir que elimina cualquier variable cuyo conteo del valor más común entre el conteo de la siguiente es mayor a 500).\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(83)\ncasas_split &lt;- initial_split(casas, prop = 0.75)\ncasas_entrena &lt;- training(casas_split)\nreceta_casas &lt;- recipe(precio_miles ~ \n           nombre_zona + \n           area_hab_m2 + area_garage_m2 + area_sotano_m2 + \n           area_lote_m2 + \n           año_construccion + \n           calidad_gral + calidad_garage + calidad_sotano + \n           num_coches  + \n           aire_acondicionado + condicion_venta, \n           data = casas_entrena) |&gt; \n  step_filter(condicion_venta == \"Normal\") |&gt; \n  step_select(-condicion_venta, skip = TRUE) |&gt; \n  step_cut(calidad_gral, breaks = c(3, 5, 7, 8), \n           include_outside_range = TRUE) |&gt;\n  step_novel(nombre_zona, calidad_sotano, calidad_garage) |&gt; \n  step_unknown(calidad_sotano, calidad_garage) |&gt; \n  step_other(nombre_zona, threshold = 0.02, other = \"otras\") |&gt; \n  step_mutate(area_sotano_m2 = ifelse(is.na(area_sotano_m2), 0, area_sotano_m2)) |&gt; \n  step_mutate(area_garage_m2 = ifelse(is.na(area_garage_m2), 0, area_garage_m2)) |&gt; \n  step_dummy(nombre_zona, calidad_gral, calidad_garage, calidad_sotano, aire_acondicionado) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"nombre_zona\")) |&gt; \n  step_interact(terms = ~ area_garage_m2:starts_with(\"calidad_garage\")) |&gt; \n  step_interact(terms = ~ area_sotano_m2: starts_with(\"calidad_sotano\")) |&gt; \n  step_nzv(all_predictors(), freq_cut = 500 / 1, unique_cut = 1)\n\nEntrenamos la receta y vemos cuántos casos y columnas tenemos:\n\nreceta_casas_prep &lt;- prep(receta_casas, verbose = TRUE)\n\noper 1 step filter [training] \noper 2 step select [training] \noper 3 step cut [training] \noper 4 step novel [training] \noper 5 step unknown [training] \noper 6 step other [training] \noper 7 step mutate [training] \noper 8 step mutate [training] \noper 9 step dummy [training] \noper 10 step interact [training] \noper 11 step interact [training] \noper 12 step interact [training] \noper 13 step interact [training] \noper 14 step nzv [training] \nThe retained training set is ~ 0.44 Mb  in memory.\n\ndatos_tbl &lt;- juice(receta_casas_prep)\ndim(datos_tbl)\n\n[1] 907  62\n\n\n\ndatos_tbl |&gt;\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n  head() |&gt; \n  gt()\n\n\n\n\n\n\n\narea_hab_m2\narea_garage_m2\narea_sotano_m2\narea_lote_m2\naño_construccion\nnum_coches\nprecio_miles\nnombre_zona_CollgCr\nnombre_zona_Crawfor\nnombre_zona_Edwards\nnombre_zona_Gilbert\nnombre_zona_IDOTRR\nnombre_zona_Mitchel\nnombre_zona_NAmes\nnombre_zona_NoRidge\nnombre_zona_NridgHt\nnombre_zona_NWAmes\nnombre_zona_OldTown\nnombre_zona_Sawyer\nnombre_zona_SawyerW\nnombre_zona_Somerst\nnombre_zona_Timber\nnombre_zona_otras\ncalidad_gral_X.3.5.\ncalidad_gral_X.5.7.\ncalidad_gral_X.7.8.\ncalidad_gral_X.8.max.\ncalidad_garage_Fa\ncalidad_garage_Gd\ncalidad_garage_TA\ncalidad_garage_unknown\ncalidad_sotano_Fa\ncalidad_sotano_Gd\ncalidad_sotano_TA\ncalidad_sotano_unknown\naire_acondicionado_Y\narea_hab_m2_x_calidad_gral_X.3.5.\narea_hab_m2_x_calidad_gral_X.5.7.\narea_hab_m2_x_calidad_gral_X.7.8.\narea_hab_m2_x_calidad_gral_X.8.max.\narea_hab_m2_x_nombre_zona_CollgCr\narea_hab_m2_x_nombre_zona_Crawfor\narea_hab_m2_x_nombre_zona_Edwards\narea_hab_m2_x_nombre_zona_Gilbert\narea_hab_m2_x_nombre_zona_IDOTRR\narea_hab_m2_x_nombre_zona_Mitchel\narea_hab_m2_x_nombre_zona_NAmes\narea_hab_m2_x_nombre_zona_NoRidge\narea_hab_m2_x_nombre_zona_NridgHt\narea_hab_m2_x_nombre_zona_NWAmes\narea_hab_m2_x_nombre_zona_OldTown\narea_hab_m2_x_nombre_zona_Sawyer\narea_hab_m2_x_nombre_zona_SawyerW\narea_hab_m2_x_nombre_zona_Somerst\narea_hab_m2_x_nombre_zona_Timber\narea_hab_m2_x_nombre_zona_otras\narea_garage_m2_x_calidad_garage_Fa\narea_garage_m2_x_calidad_garage_Gd\narea_garage_m2_x_calidad_garage_TA\narea_sotano_m2_x_calidad_sotano_Fa\narea_sotano_m2_x_calidad_sotano_Gd\narea_sotano_m2_x_calidad_sotano_TA\n\n\n\n\n137.22\n20.07\n79.99\n584.55\n1928\n1\n145.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0.00\n137.22\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n137.22\n0\n0\n20.07\n0\n0.00\n79.99\n\n\n179.86\n46.82\n130.62\n1039.96\n2000\n2\n230.5\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0.00\n0.00\n179.86\n0\n0.00\n0\n0\n179.86\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0.00\n0\n0\n46.82\n0\n130.62\n0.00\n\n\n81.94\n27.31\n0.00\n774.72\n1959\n1\n106.5\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n81.94\n0.00\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n81.94\n0\n0\n0\n0\n0\n0\n0\n0\n0.00\n0\n0\n27.31\n0\n0.00\n0.00\n\n\n153.01\n20.07\n74.88\n637.13\n1915\n1\n128.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0.00\n153.01\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n153.01\n0\n0\n20.07\n0\n74.88\n0.00\n\n\n101.45\n26.57\n50.73\n205.97\n1970\n1\n88.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n101.45\n0.00\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n101.45\n0\n0\n26.57\n0\n0.00\n50.73\n\n\n71.35\n36.79\n71.35\n668.90\n1972\n1\n133.9\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n71.35\n0.00\n0.00\n0\n71.35\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0.00\n0\n0\n36.79\n0\n71.35\n0.00\n\n\n\n\n\n\n\nFinalmente, usamos un modelo lineal con las 62 entradas que acabamos de crear:\n\nflujo_casas &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(linear_reg() |&gt; set_engine(\"lm\"))\najuste &lt;- fit(flujo_casas, casas_entrena)\n\nAunque no es de interés particular para nosotros por el momento, examinamos los coeficientes (que no son tan simples de interpretar como discutiremos más adelante):\n\najuste |&gt; broom::tidy() |&gt; \n  mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt; \n  select(term, estimate) |&gt; \n  gt()\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n-858.68\n\n\narea_hab_m2\n0.80\n\n\narea_garage_m2\n1.02\n\n\narea_sotano_m2\n0.88\n\n\narea_lote_m2\n0.01\n\n\naño_construccion\n0.39\n\n\nnum_coches\n2.11\n\n\nnombre_zona_CollgCr\n16.70\n\n\nnombre_zona_Crawfor\n37.11\n\n\nnombre_zona_Edwards\n28.82\n\n\nnombre_zona_Gilbert\n31.31\n\n\nnombre_zona_IDOTRR\n11.81\n\n\nnombre_zona_Mitchel\n32.01\n\n\nnombre_zona_NAmes\n28.74\n\n\nnombre_zona_NoRidge\n22.00\n\n\nnombre_zona_NridgHt\n-6.51\n\n\nnombre_zona_NWAmes\n17.67\n\n\nnombre_zona_OldTown\n25.28\n\n\nnombre_zona_Sawyer\n37.91\n\n\nnombre_zona_SawyerW\n-11.95\n\n\nnombre_zona_Somerst\n-8.99\n\n\nnombre_zona_Timber\n3.23\n\n\nnombre_zona_otras\n-14.27\n\n\ncalidad_gral_X.3.5.\n35.54\n\n\ncalidad_gral_X.5.7.\n13.76\n\n\ncalidad_gral_X.7.8.\n16.99\n\n\ncalidad_gral_X.8.max.\n-67.26\n\n\ncalidad_garage_Fa\n15.54\n\n\ncalidad_garage_Gd\n39.04\n\n\ncalidad_garage_TA\n18.71\n\n\ncalidad_garage_unknown\n20.61\n\n\ncalidad_sotano_Fa\n72.92\n\n\ncalidad_sotano_Gd\n54.95\n\n\ncalidad_sotano_TA\n60.56\n\n\ncalidad_sotano_unknown\n59.88\n\n\naire_acondicionado_Y\n15.17\n\n\narea_hab_m2_x_calidad_gral_X.3.5.\n-0.25\n\n\narea_hab_m2_x_calidad_gral_X.5.7.\n0.05\n\n\narea_hab_m2_x_calidad_gral_X.7.8.\n0.19\n\n\narea_hab_m2_x_calidad_gral_X.8.max.\n0.81\n\n\narea_hab_m2_x_nombre_zona_CollgCr\n-0.25\n\n\narea_hab_m2_x_nombre_zona_Crawfor\n-0.14\n\n\narea_hab_m2_x_nombre_zona_Edwards\n-0.39\n\n\narea_hab_m2_x_nombre_zona_Gilbert\n-0.33\n\n\narea_hab_m2_x_nombre_zona_IDOTRR\n-0.21\n\n\narea_hab_m2_x_nombre_zona_Mitchel\n-0.42\n\n\narea_hab_m2_x_nombre_zona_NAmes\n-0.31\n\n\narea_hab_m2_x_nombre_zona_NoRidge\n-0.16\n\n\narea_hab_m2_x_nombre_zona_NridgHt\n-0.03\n\n\narea_hab_m2_x_nombre_zona_NWAmes\n-0.26\n\n\narea_hab_m2_x_nombre_zona_OldTown\n-0.32\n\n\narea_hab_m2_x_nombre_zona_Sawyer\n-0.43\n\n\narea_hab_m2_x_nombre_zona_SawyerW\n-0.07\n\n\narea_hab_m2_x_nombre_zona_Somerst\n0.00\n\n\narea_hab_m2_x_nombre_zona_Timber\n-0.14\n\n\narea_hab_m2_x_nombre_zona_otras\n0.00\n\n\narea_garage_m2_x_calidad_garage_Fa\n-0.59\n\n\narea_garage_m2_x_calidad_garage_Gd\n-0.97\n\n\narea_garage_m2_x_calidad_garage_TA\n-0.78\n\n\narea_sotano_m2_x_calidad_sotano_Fa\n-0.86\n\n\narea_sotano_m2_x_calidad_sotano_Gd\n-0.53\n\n\narea_sotano_m2_x_calidad_sotano_TA\n-0.67\n\n\n\n\n\n\n\nNótese que:\n\nEn esta tabla están los coeficientes \\(\\beta_i\\) en las covariables que creamos a partir de las variables de entrada.\nEl modelo lineal no tiene que ser lineal en las variables que recibimos originalmente en la tabla de datos.\nEn este ejemplo, convertimos algunas variables a dummy, y multiplicamos algunas variables de área por esas variables dummy.\n\nFinalmente, evaluamos el desempeño sobre las ventas normales:\n\nmetricas &lt;- metric_set(mape, mae, rmse, rsq)\ncasas_prueba_normal &lt;- testing(casas_split) |&gt; \n  filter(condicion_venta == \"Normal\")\nmetricas(casas_prueba_normal |&gt; bind_cols(predict(ajuste, casas_prueba_normal)), \n     truth = precio_miles, estimate = .pred) |&gt; \n  gt() |&gt; fmt_number(.estimate, decimals = 2)\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nmape\nstandard\n10.34\n\n\nmae\nstandard\n17.12\n\n\nrmse\nstandard\n23.49\n\n\nrsq\nstandard\n0.89",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Métodos lineales e ingenería de entradas</span>"
    ]
  },
  {
    "objectID": "04-lineales-ingenieria.html#no-linealidad-y-atípicos",
    "href": "04-lineales-ingenieria.html#no-linealidad-y-atípicos",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.6 No linealidad y atípicos",
    "text": "4.6 No linealidad y atípicos\nEn un primer ejemplo consideremos la variable de área de lote:\n\nlibrary(patchwork)\ng_1 &lt;- ggplot(casas_entrena, aes(x = area_lote_m2, y = precio_miles)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  ggtitle(\"Sin fitrar\") \ng_2 &lt;- ggplot(casas_entrena |&gt; filter(area_lote_m2 &lt; 5000), aes(x = area_lote_m2, y = precio_miles)) +\n  geom_point(data = casas_entrena, aes(colour = area_lote_m2 &gt; 5000)) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  ggtitle(\"Filtrando valores grandes\") \ng_1 + g_2\n\n\n\n\n\n\n\n\nY notamos que hay algunos valores grandes que pueden perturbar el ajuste lineal. Esto puede producir varianza alta en las predicciones, pues el ajuste depende mucho de unos cuantos valores de entrenamiento (intenta explicar de dónde sale esa varianza). Una solución puede ser transformar la entrada por ejempo usando el logaritmo, que comprime la cola derecha de la distribución de la variable que tiene mucho sesgo:\n\nggplot(casas_entrena, aes(x = area_lote_m2, y = precio_miles)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", span = 0.5, se = FALSE) +\n  scale_x_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNótese que:\n\nProbablemente tendremos que agregar más flexibilidad en nuestro predictor para capturar apropiadamente la información en esta variable, como mostramos en la gráfica anterior. Esto muestra que con flexibilidad apropiada, la existencia de atípicos no necesariamente afecta mucho la estimación donde existe la mayoría de los valores.\nEn algunos casos, si después de investigar estas observaciones vemos que son datos erróneos (y conocemos la razón del error o medición defectuosa), podemos corregirlos o eliminarlos. Esto es parte del preprocesamiento y es necesario también probar el impacto en desempeño de estas modificaciones.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Métodos lineales e ingenería de entradas</span>"
    ]
  },
  {
    "objectID": "04-lineales-ingenieria.html#no-linealidad-y-splines",
    "href": "04-lineales-ingenieria.html#no-linealidad-y-splines",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.7 No linealidad y splines",
    "text": "4.7 No linealidad y splines\nEn algunos casos, la relación de una variable de entrada con la predicción es no lineal. Podemos entonces incluír entradas derivadas de la original usando transformaciones no lineales: por ejemplo, transformar entradas usando el logaritmo, o agregar el cuadrado o la raíz de las variables de entrada.\nUna de las maneras más simples y menos problemáticas de hacer esto es usando splines naturales para modelar, que son funciones cúbicas por tramos dos veces diferenciables. Los tramos están definidos por nudos que podemos definir por ejemplo igualmente espaciados en los datos.\n\nvalores_x &lt;- seq(-10, 110, 1)\nbase_splines &lt;- splines::ns(x = valores_x, \n  knots = c(33, 66), Boundary.knots = c(0, 100))\nspline_1 &lt;- base_splines %*% c(1, 1, 1)\nspline_2 &lt;- base_splines %*% c(-0.1, 2, 1)\ntibble(x = valores_x, y = spline_1, spline = 1) |&gt; \nbind_rows(tibble(x = valores_x, y = spline_2, spline = 2)) |&gt; \n  ggplot(aes(x = x, y = y, \n    group = spline, colour = factor(spline))) + \n  geom_point() + geom_line() +\n  geom_vline(xintercept = c(0, 33, 66, 100), \n    colour = \"red\")\n\n\n\n\n\n\n\n\nEstos dos son ejemplos de funciones cúbicas por tramos y dos veces diferenciables, con nudos en 0, 33, 66 y 100. Su forma particular depende de tres coeficientes, que pueden pensarse también como definidos por dónde tienen que pasar la curva en \\(y\\) para los valores \\(x = 33, 66\\) y \\(100\\). Extrapolan linealmente fuera del rango de los datos.\nPara entender mejor esta construcción, primero consideramos el uso de polinomios para hacer expansión de entradas. Por ejemplo, si escogemos grado 3, la idea es convertir una variable \\(x\\) en varias variables \\(x, x^2, x^3, x^4\\), de modo que nuestro modelo original\n\\[f(x) = a + bx + \\textrm{otros términos}\\] Se convierte en\n\\[f(x) = a + b_1x + b_2x^2 + b_3x^3 + b_4x^4 + \\textrm{otros términos}\\]\nPara hacer esto, sólo tenemos que precalcular \\(x, x^2, x^3, x^4\\) y añadir al modelo lineal.\nEn el caso de splines, con 4 grados de libertad por ejemplo, el modelo se convierte en:\n\\[f(x) = a + b_1s_1(x) + b_2s_2(x) + b_3s_3(x) + b_4s_4(x) + \\textrm{otros términos}\\] donde cada \\(s_i(x)\\) es una función cúbica por tramos que vale 0 fuera de un intervalo. Igual que con polinomios, estas variables \\(s_i(x)\\) pueden precalcularse a partir de la \\(x\\) original y los nudos que escogimos. Puedes ver los detalles en nuestra referencia Hastie, Tibshirani, y Friedman (2017).\nLa ventaja de utilizar estos splines es que\n\nSon más estables en el cálculo, pues a lo más utilizan potencias cúbicas.\nLa complejidad puede aumentarse incrementando el número de nodos, sin incrementar el grado.\nEvitan el problema de la naturaleza global de los polinomios, en los que cambios en una parte de los datos puede afectar severamente la forma del polinomio en otros lugares lejanos. Esto limita los efectos negativos de la varianza adicional en las colas de los datos (por ejemplo atípicos).\n\n\nEjemplo\nRevisamos nuestro ejemplo de rendimiento de coches:\n\nlibrary(tidyverse)\nlibrary(gt)\nauto &lt;- read_csv(\"../datos/auto.csv\")\ndatos &lt;- auto[, c('name', 'weight','year', 'mpg', 'displacement')]\ndatos &lt;- datos |&gt; mutate(\n  peso_kg = weight * 0.45359237,\n  rendimiento_kpl = mpg * (1.609344 / 3.78541178), \n  año = year)\n\nVamos a separar en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (75% para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba):\n\nlibrary(tidymodels)\nset.seed(121)\ndatos_split &lt;- initial_split(datos, prop = 0.75)\ndatos_entrena &lt;- training(datos_split)\ndatos_prueba &lt;- testing(datos_split)\n\nVamos a usar año y peso de los coches para predecir su rendimiento:\n\nggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl, colour = año)) +\n  geom_point()\n\n\n\n\n\n\n\n\nNuestra receta incluye la transformación no lineal de splines:\n\nreceta_lineal &lt;- recipe(rendimiento_kpl ~ peso_kg + año, datos_entrena) |&gt; \n  step_ns(peso_kg, deg_free = 3) |&gt; \n  step_ns(año, deg_free = 2)\nmod_lineal &lt;- linear_reg() |&gt;  \n  set_engine(\"lm\")  \nflujo &lt;- workflow() |&gt;  \n  add_recipe(receta_lineal) |&gt; \n  add_model(mod_lineal)\n\nLos datos de entrada son los siguientes:\n\njuice(prep(receta_lineal)) |&gt; head() |&gt; gt()\n\n\n\n\n\n\n\nrendimiento_kpl\npeso_kg_ns_1\npeso_kg_ns_2\npeso_kg_ns_3\naño_ns_1\naño_ns_2\n\n\n\n\n12.754311\n-0.1384501\n0.3919536\n-0.2338715\n0.1263158\n-0.08343893\n\n\n13.136941\n-0.1508427\n0.4978146\n-0.2970367\n0.5652102\n0.00590925\n\n\n12.754311\n0.3794499\n0.4646633\n-0.2232209\n0.4765544\n0.35513660\n\n\n7.695101\n0.4471996\n0.4245669\n-0.1625354\n0.5652102\n0.00590925\n\n\n8.757960\n0.4368600\n0.4313780\n-0.1743974\n0.5652102\n0.00590925\n\n\n14.242314\n-0.1126420\n0.2985773\n-0.1781555\n0.5794245\n-0.12316573\n\n\n\n\n\n\n\nNótese que tenemos 4 entradas en lugar de las 2 originales, pues creamos dos transformaciones no lineales de peso_kg. El modelo es lineal en estas 4 variables, pero no en las 2 originales. Ajustamos:\n\nflujo_ajustado &lt;- fit(flujo, datos_entrena)\n\nY ahora podemos graficar los resultados y vemos cómo pudimos capturar la relación no lineal entre peso y rendimiento:\n\ndat_graf &lt;- tibble(peso_kg = seq(900, 2200, by = 10)) |&gt;   \n  crossing(tibble(año = c(70, 75, 80)))\ndat_graf &lt;- dat_graf |&gt; \n  mutate(pred_1 = predict(flujo_ajustado, dat_graf) |&gt; pull(.pred))\nggplot(datos_entrena, aes(x = peso_kg, group = año, colour = año)) +\n  geom_point(aes(y = rendimiento_kpl), alpha = 0.6) + \n  geom_line(data = dat_graf, aes(y = pred_1),  size = 1.2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nLos grados de libertad también pueden afinarse utilizando un conjunto de validación como hicimos antes en vecinos más cercanos.\n\n\nEjemplo: casas\nPor ejemplo, podríamos incluir un efecto no lineal de area_lote, calidad y condición general y año de construcción:\n\nreceta_casas &lt;- recipe(precio_miles ~ \n           nombre_zona + \n           area_hab_m2 + area_garage_m2 + area_sotano_m2 + \n           area_lote_m2 + \n           año_construccion + \n           calidad_gral + calidad_garage + calidad_sotano + \n           condicion_gral + \n           num_coches  + \n           aire_acondicionado + condicion_venta, \n           data = casas_entrena) |&gt; \n  step_filter(condicion_venta == \"Normal\") |&gt; \n  step_select(-condicion_venta, skip = TRUE) |&gt; \n  \n  step_novel(nombre_zona, calidad_sotano, calidad_garage) |&gt; \n  step_unknown(calidad_sotano, calidad_garage) |&gt; \n  step_other(nombre_zona, threshold = 0.02, other = \"otras\") |&gt; \n  step_mutate(area_sotano_m2 = \n    ifelse(is.na(area_sotano_m2), 0, area_sotano_m2)) |&gt; \n  step_mutate(area_garage_m2 = \n    ifelse(is.na(area_garage_m2), 0, area_garage_m2)) |&gt;\n # step_log(area_lote_m2) |&gt; \n  step_ns(año_construccion, deg_free = 2) |&gt; \n  step_ns(calidad_gral, deg_free = 2) |&gt; \n  step_ns(condicion_gral, deg_free = 2) |&gt; \n  step_ns(area_lote_m2, deg_free = 3) |&gt; \n  step_dummy(nombre_zona,  calidad_garage, calidad_sotano, aire_acondicionado) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) |&gt; \n  step_interact(terms = ~ area_hab_m2:starts_with(\"nombre_zona\")) |&gt; \n  step_interact(terms = ~ area_garage_m2:starts_with(\"calidad_garage\")) |&gt; \n  step_interact(terms = ~ area_sotano_m2: starts_with(\"calidad_sotano\")) |&gt; \n  step_nzv(all_predictors(), freq_cut = 500 / 1, unique_cut = 1)\n\n\nflujo_casas &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(linear_reg() |&gt; set_engine(\"lm\"))\najuste &lt;- fit(flujo_casas, casas_entrena)\n\nFinalmente, evaluamos el desempeño sobre las ventas normales, y obtenemos una mejoría con respecto a nuestro modelo anterior:\n\nmetricas &lt;- metric_set(mape, mae, rmse, rsq)\ncasas_prueba_normal &lt;- testing(casas_split) |&gt; \n  filter(condicion_venta == \"Normal\")\nmetricas(casas_prueba_normal |&gt; \n  bind_cols(predict(ajuste, casas_prueba_normal)), \n     truth = precio_miles, estimate = .pred) |&gt; \n  gt() |&gt; fmt_number(.estimate, decimals = 2)\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nmape\nstandard\n8.47\n\n\nmae\nstandard\n14.45\n\n\nrmse\nstandard\n19.77\n\n\nrsq\nstandard\n0.92\n\n\n\n\n\n\n\nFinalmente, examinamos la respuesta contra la predicción:\n\ncasas_prueba_normal |&gt; \n  bind_cols(predict(ajuste, casas_prueba_normal)) |&gt; \n  ggplot(aes(x = .pred, y = precio_miles)) + geom_abline() + \n    geom_point(colour = \"red\") + coord_obs_pred()\n\n\n\n\n\n\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Métodos lineales e ingenería de entradas</span>"
    ]
  },
  {
    "objectID": "05-regularizacion-1.html",
    "href": "05-regularizacion-1.html",
    "title": "5  Regularización y variabilidad",
    "section": "",
    "text": "5.1 Ejemplo: datos simulados y varianza\nConsideremos un problema donde tenemos unas 100 entradas con 120 casos. Supondremos que la función verdadera es\n\\[f(x) = \\sum_{j=1}^{100} \\beta_j x_j\\] con ciertos pesos o coeficientes \\(\\beta\\) que fijaremos.\nlibrary(tidyverse)\nlibrary(gt)\nset.seed(28015)\nbeta_vec &lt;- rnorm(100, 0, 0.2)\np &lt;- length(beta_vec)\nbeta &lt;- tibble(term = str_c('V', 1:p), valor = beta_vec)\nhead(beta)\n\n# A tibble: 6 × 2\n  term     valor\n  &lt;chr&gt;    &lt;dbl&gt;\n1 V1    -0.121  \n2 V2     0.0374 \n3 V3    -0.129  \n4 V4     0.240  \n5 V5    -0.00962\n6 V6    -0.0443\nSimulamos datos:\nsim_datos &lt;- function(n, beta){\n  p &lt;- nrow(beta)\n  mat_x &lt;- matrix(rnorm(n * p, 0, 1), n, p) + rnorm(n, 0, 5) \n  colnames(mat_x) &lt;- beta |&gt; pull(term)\n  beta_vec &lt;- beta |&gt; pull(valor)\n  f_x &lt;- mat_x %*% beta_vec \n  y &lt;- as.numeric(f_x) + rnorm(n, 0, 1)\n  datos &lt;- as_tibble(mat_x) \n  datos |&gt; mutate(y = y)\n}\ndatos &lt;- sim_datos(n = 4000, beta = beta)\nSeparamos datos de entrenamiento y prueba y definimos y ajustamos un predictor lineal:\nlibrary(tidymodels)\nset.seed(994)\nn_entrena &lt;- nrow(datos) * 0.03\nseparacion &lt;- initial_split(datos, 0.03)\ndat_ent &lt;- training(separacion)\nmodelo &lt;-  linear_reg() |&gt; set_engine(\"lm\")\nreceta &lt;- recipe(y ~ ., dat_ent)\nflujo &lt;- workflow() |&gt; \n  add_model(modelo) |&gt; \n  add_recipe(receta)\nflujo_ajustado &lt;- fit(flujo, dat_ent)\nmod_1  &lt;- flujo_ajustado |&gt; extract_fit_engine()\nExtraemos los coeficientes y graficamos ajustados contra verdaderos:\ncoefs_1 &lt;- tidy(mod_1) |&gt; \n  left_join(beta, by = \"term\")\nggplot(coefs_1 |&gt; filter(term != \"(Intercept)\"), \n       aes(x = valor, y = estimate)) +\n  geom_point() +\n  xlab('Coeficientes verdaderos') + \n  ylab('Coeficientes estimados') +\n  geom_abline()\nY notamos que las estimaciones no son buenas. Podemos hacer otra simulación para confirmar que el problema es que las estimaciones son muy variables.\nCon otra muestra de entrenamiento, vemos que las estimaciones tienen varianza alta.\ndatos_ent_2 &lt;- sim_datos(n = 120, beta = beta)\nmod_2 &lt;- fit(flujo, datos_ent_2) |&gt; extract_fit_engine()\ncoefs_2 &lt;- tidy(mod_2)\nqplot(coefs_1$estimate, coefs_2$estimate) + xlab('Coeficientes mod 1') + \n  ylab('Coeficientes mod 2') +\n  geom_abline(intercept=0, slope =1)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\nEn la práctica, nosotros tenemos una sola muestra de entrenamiento. Así que, con una muestra de tamaño\n\\(n=120\\) como en este ejemplo, obtendremos típicamente resultados no muy buenos. Estos coeficientes ruidosos afectan nuestras predicciones de manera negativa, aún cuando el modelo ajustado parece reproducir razonablemente bien la variable respuesta:\nlibrary(patchwork)\ndat_pr &lt;- testing(separacion)\npreds_entrena &lt;- predict(flujo_ajustado, dat_ent) |&gt; \n  bind_cols(dat_ent |&gt; select(y))\npreds_prueba &lt;- predict(flujo_ajustado, dat_pr) |&gt; \n  bind_cols(dat_pr |&gt; select(y))\ng_1 &lt;- ggplot(preds_entrena, aes(x = .pred, y = y)) +\n  geom_abline(colour = \"red\") +\n  geom_point() + \n  xlab(\"Predicción\") + ylab(\"y\") +\n  labs(subtitle = \"Muestra de entrenamiento\")\ng_2 &lt;- ggplot(preds_prueba, aes(x = .pred, y = y)) + \n  geom_abline(colour = \"red\") +\n  geom_point() + \n  xlab(\"Predicción\") + ylab(\"y\") +\n  labs(subtitle = \"Muestra de prueba\")\ng_1 + g_2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularización y variabilidad</span>"
    ]
  },
  {
    "objectID": "05-regularizacion-1.html#ejemplo-controlando-la-varianza",
    "href": "05-regularizacion-1.html#ejemplo-controlando-la-varianza",
    "title": "5  Regularización y variabilidad",
    "section": "5.2 Ejemplo: controlando la varianza",
    "text": "5.2 Ejemplo: controlando la varianza\nComo el problema es la variabilidad de los coeficientes (en este ejemplo sabemos que no hay sesgo pues conocemos el modelo verdadero), podemos atacar este problema poniendo restricciones a los coeficientes, de manera que caigan en rangos más aceptables.\nUna manera de hacer esto es restringir el rango de los coeficientes cambiando la función que minimizamos para ajustar el modelo lineal. Recordamos que la cantidad que queremos minimizar es\n\\[D(\\beta) = D(a_0, \\beta_1, \\ldots, \\beta_p) = \\sum_{i=1}^N (y^{(i)} - f_\\beta (x^{(i)}))^2 = \\sum_{i=1}^N (y^{(i)} - \\beta_0 - \\beta_1 x_1^{(i)}-\\beta_2x_2^{(i)} - \\cdots - \\beta_px_p^{(i)})^2\\]\ndonde la suma es sobre los datos de entrenamiento. Queremos encontrar \\(a =(\\beta_0, \\beta_1, \\ldots, \\beta_p)\\) para resolver\n\\[\\min_\\beta D(\\beta)\\]\nEn el ejemplo que estamos considerando, vemos que existe mucha variación en los coeficientes obtenidos de muestra de entrenamiento a muestra de entrenamiento, y que algunos de ellos toman valores muy grandes positivos o negativos. Podemos entonces intentar resolver mejor el problema penalizado\n\\[\\min_\\beta \\left\\{ D(\\beta) + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right\\}\\]\nSi escogemos un valor relativamente grande de \\(\\lambda &gt; 0\\), entonces terminaremos con una solución donde los coeficientes\nno pueden alejarse mucho de 0, y esto previene parte del sobreajuste que observamos en nuestro primer ajuste. Otra manera de decir esto es: intentamos minimizar cuadrados, pero no permitimos que los coeficientes se alejen demasiado de cero, o ponemos un costo a soluciones que intentan “mover” mucho los coeficientes para ajustar mejor al conjunto de entrenamiento.\n\n(Quitar unidades) Normalmente normalizamos las variables de entrada \\(x\\) para que tenga sentido penalizar todos los coeficientes con una misma \\(\\lambda\\).\n(Representación alternativa) También es posible poner restricciones sobre el tamaño de \\(\\sum_j \\beta_j^2\\), lo cual es equivalente al problema de penalización.\n(Ordenada al origen) Usualmente no penalizamos la constante \\(\\beta_0\\), de forma que si \\(\\lambda\\) es muy grande, nuestro modelo ajustado predice simplemente la media de los datos de entrenamiento. En otro caso, nuestra predicción limite sería 0, lo cual rara vez tiene sentido.\nEste tipo de penalización se llama muchas veces \\(L_2\\), o penalización ridge.\n\nEn este caso obtenemos:\n\nmodelo_reg &lt;-  linear_reg(mixture = 0, penalty = 0.1) |&gt;\n  set_engine(\"glmnet\", lambda.min.ratio = 0)\nflujo_reg &lt;- workflow() |&gt; \n  add_model(modelo_reg) |&gt; \n  add_recipe(receta)\nflujo_reg &lt;- fit(flujo_reg, dat_ent)\nmod_reg  &lt;- flujo_reg |&gt; extract_fit_parsnip()\n\nLos coeficientes del modelo penalizado son:\n\ncoefs_penalizado &lt;- tidy(mod_reg) \n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\ncoefs_penalizado\n\n# A tibble: 101 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  -0.236      0.1\n 2 V1           -0.0774     0.1\n 3 V2            0.0340     0.1\n 4 V3           -0.0405     0.1\n 5 V4            0.151      0.1\n 6 V5            0.150      0.1\n 7 V6            0.114      0.1\n 8 V7           -0.207      0.1\n 9 V8            0.0148     0.1\n10 V9            0.191      0.1\n# ℹ 91 more rows\n\n\nNótese que efectivamente la suma de cuadrados de los coeficientes penalizados es considerablemente más chica que las del modelo no penalizado:\n\nsum(coefs_penalizado$estimate[-1]^2)\n\n[1] 1.957447\n\n\n\nsum(coefs_1$estimate[-1]^2)\n\n[1] 12.59754\n\n\nLos nuevos coeficientes estimados tienen menor variación, y están más cercanos a los valores reales:\n\nqplot(coefs_1$valor[-1], coefs_penalizado$estimate[-1]) + \n  xlab('Coeficientes') + \n  ylab('Coeficientes estimados') +\n  geom_abline()\n\n\n\n\n\n\n\n\n\npreds_prueba_2 &lt;- predict(mod_reg, dat_pr) |&gt; \n  bind_cols(dat_pr |&gt; select(y))\npreds_prueba_ambas &lt;- bind_rows(\n          preds_prueba |&gt; mutate(tipo = \"sin penalizar (prueba)\"),\n          preds_prueba_2 |&gt; mutate(tipo = \"penalizado (prueba)\"))\nggplot(preds_prueba_ambas, aes(x = y, y = .pred)) +\n  geom_abline(colour = \"red\") +\n  geom_point(alpha = 0.3) + \n  xlab(\"Predicción\") + ylab(\"y\") +\n  facet_wrap(~ tipo, nrow = 1) + \n  labs(subtitle = \"Muestra de prueba\") \n\n\n\n\n\n\n\n\n\nmetricas &lt;- metric_set(mae, rmse)\nres_1 &lt;- metricas(preds_prueba, truth = y, estimate = .pred) |&gt; \n  mutate(tipo = \"no penalizado\")\nres_2 &lt;- metricas(preds_prueba_2, truth = y, estimate = .pred) |&gt; \n  mutate(tipo = \"penalizado\")\nbind_rows(res_1, res_2) |&gt;\n  arrange(.metric) |&gt; \n  gt() |&gt; fmt_number(.estimate, decimals = 2)\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\ntipo\n\n\n\n\nmae\nstandard\n2.34\nno penalizado\n\n\nmae\nstandard\n1.27\npenalizado\n\n\nrmse\nstandard\n2.95\nno penalizado\n\n\nrmse\nstandard\n1.60\npenalizado\n\n\n\n\n\n\n\nY vemos que los errores de predicción se reducen considerablemente.\n\nObsérvese que esta mejora en varianza tiene un costo: un aumento en el sesgo (observa en los extremos de las predicciones regularizadas).\nSin embargo, lo que nos importa principalmente es reducir el error de predicción, y eso lo logramos escogiendo un balance sesgo-varianza apropiado para los datos y el problema.\n\n\n\n\n\n\n\nRegularización L2\n\n\n\nCuando agregamos el término de penalización tipo ridge al error de entrenamiento como objetivo a minimizar en el ajuste, los coeficientes de la solución penalizada están encogidos con respecto a los no penalizados.\nRegularizar reduce la varianza de los coeficientes a lo largo de distintas muestras de entrenamiiento, lo que reduce la posibilidad de sobreajuste.\nUtilizamos regularización para reducir el error de predicción cuando el problema es variabilidad grande de los coeficientes (coeficientes ruidosos) en modelos relativamente grandes o con pocos datos de entrenamiento.\n\n\nEn general, a métodos donde restringimos el espacio de modelos o penalizamos ajustes complejos en la función de pérdida que nos interesa se llaman métodos con regularización. Un ejemplos es todos los modelos donde en lugar de considerar la función de perdida \\(L\\) solamente, consideramos minimizar\n\\[L(f) + \\Omega(f),\\] donde \\(f\\) es una medida de la complejidad, como puede ser: que la función \\(f\\) tiene oscilaciones grandes o pendientes grandes, tiene un número grande de discontinuidades, etc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularización y variabilidad</span>"
    ]
  },
  {
    "objectID": "05-regularizacion-1.html#ejemplo-2-penalización-y-estimaciones-ruidosas",
    "href": "05-regularizacion-1.html#ejemplo-2-penalización-y-estimaciones-ruidosas",
    "title": "5  Regularización y variabilidad",
    "section": "5.3 Ejemplo 2: penalización y estimaciones ruidosas",
    "text": "5.3 Ejemplo 2: penalización y estimaciones ruidosas\nConsideremos los siguientes datos clásicos de Radiación Solar, Temperatura, Velocidad del Viento y Ozono para distintos días en Nueva York (Chambers et al. (1983)):\n\nair_data &lt;- airquality |&gt; \n    mutate(Wind_cat = cut(Wind, quantile(Wind, c(0, 0.33, 0.66, 1)), \n                          include.lowest = T)) |&gt; \n    filter(!is.na(Ozone) & !is.na(Solar.R))\nair &lt;- air_data\nggplot(air, aes(x = Solar.R, y = Ozone,  colour = Temp)) + \n  geom_point() +\n  facet_wrap(~Wind_cat, ncol = 3) + \n  scale_colour_gradientn(colours = rainbow(2, rev = TRUE))\n\n\n\n\n\n\n\n\nLa gráfica muestra algunas interacciones y relaciones no lineales. Formulamos un modelo lineal como sigue:\n\nreceta_ozono &lt;- recipe(Ozone ~ Temp + Wind + Solar.R,\n                       data = air) |&gt; \n  step_ns(Temp, Wind, Solar.R, deg_free = 2) |&gt; \n  step_interact(terms = ~ starts_with(\"Temp_ns\"):starts_with(\"Wind_ns\")) |&gt; \n  step_interact(terms = ~ starts_with(\"Temp_ns\"):starts_with(\"Solar.R_ns\"))\najuste_ozono &lt;- workflow() |&gt; \n  add_recipe(receta_ozono) |&gt; \n  add_model(linear_reg() |&gt; set_engine(\"lm\")) |&gt; \n  fit(air)\n\nY el ajuste se ve como sigue:\n\npred_grid &lt;- expand_grid(Wind = c(5,10,15), \n                         Temp = seq(60, 90, 10), \n                         Solar.R = seq(20, 300, by = 10)) |&gt; \n    mutate(Wind_cat = cut(Wind, \n           quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), \n           include.lowest = T))\npred_grid &lt;- pred_grid |&gt; \n  bind_cols(predict(ajuste_ozono, pred_grid))\ng_lineal &lt;- ggplot(air, aes(x = Solar.R, colour = Temp)) + \n    geom_point(aes(y = Ozone)) +\n    facet_wrap( ~ Wind_cat) + \n    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +\n    geom_line(data = pred_grid, \n      aes(y = .pred, group = interaction(Temp, Wind_cat)), size = 1) +\n    labs(subtitle = \"Curvas de modelo lineal, para viento = 5, 10, 15\") \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ng_lineal\n\n\n\n\n\n\n\n\nNótese que algunos aspectos de este modelo parecen ser muy ruidosos: por ejemplo, el comportamiento de las curvas para el primer pánel (donde hay pocos datos de temperatura baja), el hecho de que en algunos casos parece haber curvaturas decrecientes e incluso predicciones negativas. No deberíamos dar mucho crédito a las predicciones de este modelo, y tiene peligro de producir predicciones desastrosas.\nSin embargo, si usamos algo de regularización:\n\najuste_ozono &lt;- workflow() |&gt; \n  add_recipe(receta_ozono) |&gt; \n  add_model(linear_reg(mixture = 0, penalty = 3.0) |&gt; \n              set_engine(\"glmnet\", lambda.min.ratio = 0)) |&gt; \n  fit(air)\n# nota: normalmente no es necesario usar lambda.min.ratio\n\nY el ajuste se ve como sigue:\n\npred_grid &lt;- expand_grid(Wind = c(5,10,15), \n                         Temp = seq(60, 90, 10), \n                         Solar.R = seq(10, 320, by = 10)) |&gt; \n    mutate(Wind_cat = \n           cut(Wind, quantile(airquality$Wind, c(0, 0.33, 0.66, 1)), \n               include.lowest = T))\npred_grid &lt;- pred_grid |&gt; \n  bind_cols(predict(ajuste_ozono, pred_grid))\ng_lineal &lt;- ggplot(air, aes(x = Solar.R, colour = Temp)) + \n    geom_point(aes(y = Ozone)) +\n    facet_wrap( ~ Wind_cat) + \n    scale_colour_gradientn(colours = rainbow(2, rev = TRUE)) +\n    geom_line(data = pred_grid, aes(y = .pred, group = interaction(Temp, Wind_cat)), size = 1) +\n    labs(subtitle = \"Curvas de modelo lineal, para viento = 5, 10, 15\") \ng_lineal\n\n\n\n\n\n\n\n\nEste ajuste se ve mucho más razonable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularización y variabilidad</span>"
    ]
  },
  {
    "objectID": "05-regularizacion-1.html#regresión-ridge-escogiendo-el-parámetro-de-complejidad",
    "href": "05-regularizacion-1.html#regresión-ridge-escogiendo-el-parámetro-de-complejidad",
    "title": "5  Regularización y variabilidad",
    "section": "5.4 Regresión ridge: escogiendo el parámetro de complejidad",
    "text": "5.4 Regresión ridge: escogiendo el parámetro de complejidad\nComo vimos antes, no es posible seleccionar el parámetro \\(\\lambda\\) usando la muestra de entrenamiento (¿con qué \\(\\lambda\\) cómo se obtiene el menor error cuadrático medio sobre la muestra de entrenamiento). Usaremos un conjunto de validación relativamente grande\n\nset.seed(191)\nsource(\"../R/casas_traducir_geo.R\")\n\nRows: 1460 Columns: 81\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (43): MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConf...\ndbl (38): Id, MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, Ye...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 27 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Neighborhood\ndbl (2): lat, long\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# esta proporción es para ejemplificar\ncasas_split &lt;- initial_split(casas, prop = 0.25) \ncasas_entrena &lt;- training(casas_split)\nreceta_casas &lt;- \n  recipe(precio_miles ~ calidad_gral +\n           area_hab_m2 + area_garage_m2 + area_sotano_m2 + \n           area_2o_piso_m2 + \n           año_construccion + año_venta + condicion_venta +\n           nombre_zona + \n           condicion_gral + \n           condicion_exteriores + \n           tipo_sotano + calidad_sotano +\n           baños_completos +  num_coches +\n           aire_acondicionado + \n           tipo_edificio + estilo, \n         data = casas_entrena) |&gt; \n  step_filter(condicion_venta == \"Normal\") |&gt; \n  step_select(-condicion_venta, skip = TRUE) |&gt; \n  step_cut(calidad_gral, breaks = c(3, 5, 7, 8), \n           include_outside_range = TRUE) |&gt; \n  step_cut(condicion_gral, breaks = c(3, 5, 7, 8), \n           include_outside_range = TRUE) |&gt; \n  step_mutate(sin_piso_2 = as.numeric(area_2o_piso_m2 == 0)) |&gt;\n  step_novel(tipo_sotano, calidad_sotano) |&gt; \n  step_novel(condicion_exteriores, tipo_edificio, estilo) |&gt; \n  step_unknown(tipo_sotano, calidad_sotano, new_level = \"sin_sotano\") |&gt; \n  step_unknown(condicion_exteriores) |&gt; \n  step_other(nombre_zona, threshold = 0.05) |&gt; \n  step_dummy(calidad_gral, condicion_gral, \n             nombre_zona, aire_acondicionado,\n             calidad_sotano, tipo_sotano, condicion_exteriores, \n             tipo_edificio, estilo) |&gt; \n  step_interact(terms = ~ c(area_hab_m2, area_garage_m2, \n    area_sotano_m2, area_2o_piso_m2):starts_with(\"calidad_gral\")) |&gt; \n  step_interact(terms = ~ area_sotano_m2:starts_with(\"calidad_sotano\")) |&gt; \n  step_interact(terms = ~ c(area_hab_m2, area_garage_m2, \n    area_sotano_m2, area_2o_piso_m2):starts_with(\"condicion_gral\")) |&gt; \n  step_interact(terms = ~ c(area_hab_m2, area_garage_m2, \n    area_sotano_m2, area_2o_piso_m2):starts_with(\"nombre_zona\")) |&gt; \n  step_zv(all_predictors())\n\nPara ver el número de entradas de este modelo:\n\nprep(receta_casas) |&gt; juice() |&gt; dim()\n\n[1] 289 108\n\n\n\nmodelo_penalizado &lt;- linear_reg(mixture = 0.0, penalty = tune()) |&gt; \n  set_engine(\"glmnet\", lambda.min.ratio = 0)\nflujo_casas &lt;- workflow() |&gt; \n  add_recipe(receta_casas) |&gt; \n  add_model(modelo_penalizado)\n\nConstruimos manualmente el conjunto de validación:\n\n# creamos un objeto con datos de entrenamiento y de prueba\nval_split &lt;- manual_rset(casas_split |&gt; list(), \"validación\")\nlambda_params &lt;- parameters(penalty(range = c(-3, 3), \n                                    trans = log10_trans()))\nlambda_grid &lt;- grid_regular(lambda_params, levels = 20)\nlambda_grid\n\n# A tibble: 20 × 1\n      penalty\n        &lt;dbl&gt;\n 1    0.001  \n 2    0.00207\n 3    0.00428\n 4    0.00886\n 5    0.0183 \n 6    0.0379 \n 7    0.0785 \n 8    0.162  \n 9    0.336  \n10    0.695  \n11    1.44   \n12    2.98   \n13    6.16   \n14   12.7    \n15   26.4    \n16   54.6    \n17  113.     \n18  234.     \n19  483.     \n20 1000      \n\n\n\nmis_metricas &lt;- metric_set(rmse)\neval_tbl &lt;- tune_grid(flujo_casas,\n                      resamples = val_split,\n                      grid = lambda_grid,\n                      metrics = mis_metricas) \nridge_ajustes_tbl &lt;- eval_tbl |&gt;\n  unnest(cols = c(.metrics)) |&gt; \n  select(id, penalty, .metric, .estimate)\nridge_ajustes_tbl |&gt; gt()\n\n\n\n\n\n\n\nid\npenalty\n.metric\n.estimate\n\n\n\n\nvalidación\n1.000000e-03\nrmse\n35.36721\n\n\nvalidación\n2.069138e-03\nrmse\n35.36721\n\n\nvalidación\n4.281332e-03\nrmse\n35.36721\n\n\nvalidación\n8.858668e-03\nrmse\n35.36721\n\n\nvalidación\n1.832981e-02\nrmse\n35.36721\n\n\nvalidación\n3.792690e-02\nrmse\n35.36721\n\n\nvalidación\n7.847600e-02\nrmse\n34.88798\n\n\nvalidación\n1.623777e-01\nrmse\n33.64199\n\n\nvalidación\n3.359818e-01\nrmse\n32.41837\n\n\nvalidación\n6.951928e-01\nrmse\n31.22570\n\n\nvalidación\n1.438450e+00\nrmse\n30.26859\n\n\nvalidación\n2.976351e+00\nrmse\n29.62744\n\n\nvalidación\n6.158482e+00\nrmse\n29.45135\n\n\nvalidación\n1.274275e+01\nrmse\n29.65517\n\n\nvalidación\n2.636651e+01\nrmse\n30.23900\n\n\nvalidación\n5.455595e+01\nrmse\n31.52315\n\n\nvalidación\n1.128838e+02\nrmse\n34.17121\n\n\nvalidación\n2.335721e+02\nrmse\n38.82727\n\n\nvalidación\n4.832930e+02\nrmse\n45.67700\n\n\nvalidación\n1.000000e+03\nrmse\n54.01752\n\n\n\n\n\n\n\n\nggplot(ridge_ajustes_tbl, \n    aes(x = penalty, y = .estimate, colour = .metric)) + \n  geom_point() + geom_line() + scale_x_log10() \n\n\n\n\n\n\n\n\nY vemos que con una penalización alrededor de \\(\\lambda = 1\\) podemos obtener mejor desempeño que con el modelo no regularizado.\nPregunta: en qué partes de la gráfica es relativamente grande la varianza? ¿en qué parte es relativamente grande el sesgo?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularización y variabilidad</span>"
    ]
  },
  {
    "objectID": "05-regularizacion-1.html#regresión-lasso",
    "href": "05-regularizacion-1.html#regresión-lasso",
    "title": "5  Regularización y variabilidad",
    "section": "5.5 Regresión lasso",
    "text": "5.5 Regresión lasso\nSe puede controlar la varianza de mínimos cuadrados de otras maneras. Cuando la varianza proviene también de la inclusión de variables que no necesariamente están relacionadas con la respuesta, podemos usar métodos de selección de variables, como en stepwise regression, por ejemplo.\nOtra manera interesante de lograr mejor desempeño predictivo con modelos más parsimoniosos resulta de usar un término de penalización distinto al de ridge. En ridge, el problema que resolvemos es minimizar el objetivo\n\\[D(\\beta) + \\lambda \\sum_{j=1}^p \\beta_j^2\\]\nEn regresión lasso, usamos una penalización de tipo \\(L_1\\):\n\\[D(a) + \\lambda \\sum_{j=1}^p |\\beta_j|\\] En un principio, no parece ser muy diferente a ridge. Veremos sin embargo que usar esta penalización también se puede ver como un proceso de selección de variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularización y variabilidad</span>"
    ]
  },
  {
    "objectID": "05-regularizacion-1.html#lasso-vs-ridge",
    "href": "05-regularizacion-1.html#lasso-vs-ridge",
    "title": "5  Regularización y variabilidad",
    "section": "5.6 Lasso vs Ridge",
    "text": "5.6 Lasso vs Ridge\nConsideramos cómo predecir el porcentaje de grasa corporal a partir de distintas mediciones de dimensiones corporales:\n\ndat_grasa &lt;- read_csv(file = '../datos/bodyfat.csv') \nset.seed(183)\ngrasa_particion &lt;- initial_split(dat_grasa, 0.7)\ngrasa_ent &lt;- training(grasa_particion)\ngrasa_pr &lt;- testing(grasa_particion)\n\n\n# nota: con glmnet no es necesario normalizar, pero aquí lo hacemos\n# para ver los coeficientes en términos de las variables estandarizadas:\ngrasa_receta &lt;- recipe(grasacorp ~ ., grasa_ent) |&gt; \n  update_role(cadera, cuello, muñeca, \n              tobillo, rodilla, new_role = \"ninguno\") |&gt; \n  step_normalize(all_predictors())\nmodelo_2 &lt;- linear_reg(mixture = 0, penalty = 0) |&gt; \n  set_engine(\"glmnet\", lambda.min.ratio = 1e-20) \nflujo_2 &lt;- workflow() |&gt; \n  add_model(modelo_2) |&gt; \n  add_recipe(grasa_receta)\nflujo_2 &lt;- flujo_2 |&gt; fit(grasa_ent) \nmodelo_2 &lt;- extract_fit_parsnip(flujo_2)\ncoefs &lt;- modelo_2 |&gt; pluck(\"fit\") |&gt;tidy() |&gt; \n  filter(term != \"(Intercept)\")\ng_l2 &lt;- ggplot(coefs, aes(x = lambda, y = estimate, colour = term)) +\n  geom_line(size = 1.4) + scale_x_log10() +\n  scale_colour_manual(values = cbb_palette) +\n  labs(subtitle = \"Regularizacion L2\")\ng_l2\n\n\n\n\n\n\n\n\nEstas gráfica se llama traza de los coeficientes, y nos muestra cómo cambian los coefi´cientes conforme cambiamos la regularización. Nótese que cuando la regularización es chica, obtenemos algunos resultados contra-intuitivos como que el coeficiente de peso es negativo para predecir el nivel de grasa corporal. Cuando regularizamos más, este coeficiente es positivo. La razón de esto tiene qué ver con la correlación fuerte de las variables de entrada, por ejemplo:\n\ncor(grasa_ent |&gt; select(peso, abdomen, biceps, muslo)) |&gt; \n  round(2)\n\n        peso abdomen biceps muslo\npeso    1.00    0.91   0.81  0.88\nabdomen 0.91    1.00   0.71  0.81\nbiceps  0.81    0.71   1.00  0.75\nmuslo   0.88    0.81   0.75  1.00\n\n\nAhora probemos con regularización lasso:\n\n## mixture = 1 es regresión lasso\nmodelo_1 &lt;-  linear_reg(mixture = 1, penalty = 0) |&gt; \n  set_engine(\"glmnet\", lambda.min.ratio = 0) \nflujo_1 &lt;- workflow() |&gt; \n  add_model(modelo_1) |&gt; \n  add_recipe(grasa_receta)\n\n\nflujo_1 &lt;- flujo_1 |&gt; fit(grasa_ent) \nmodelo_1 &lt;- extract_fit_parsnip(flujo_1)\ncoefs &lt;- modelo_1 |&gt; pluck(\"fit\") |&gt; tidy() |&gt; \n  filter(term != \"(Intercept)\")\nggplot(coefs, \n    aes(x = lambda, y = estimate, colour = term)) +\n  geom_line(size = 1.4) + scale_x_log10() +\n  scale_colour_manual(values = cbb_palette) +\n  labs(subtitle = \"Regularizacion L1\")\n\n\n\n\n\n\n\n\nY nótese que conforme aumentamos la penalización, algunas variables salen del modelo (sus coeficientes son cero). Por ejemplo, para un valor de \\(lambda\\) intermedio, obtenemos un modelo simple de la forma:\n\ncoefs |&gt; filter(step == 21) |&gt; \n  select(term, estimate, lambda)\n\n# A tibble: 3 × 3\n  term     estimate lambda\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 edad        0.160  0.434\n2 estatura   -0.452  0.434\n3 abdomen     6.67   0.434\n\n\nY nótese que este modelo solo incluye 3 variables.La traza confirma que la regularización lasso, además de encoger coeficientes, saca variables del modelo conforme el valor de regularización aumenta.\nLa razón de esta diferencia cualitativa entre cómo funciona lasso y ridge se puede entender considerando que los problemas de penalización mostrados arriba puede escribirse en forma de problemas de restricción. Por ejemplo, lasso se puede reescribir como el problema de resolver\n\\[\\min_a D(\\beta)\\] sujeto a \\[\\sum_{i=1}^p |\\beta_j| &lt; t\\] En la gráfica siguiente (tomada de Hastie, Tibshirani, y Friedman (2017)), lasso está a la izquierda y ridge está a la derecha, las curvas rojas son curvas de nivel de la suma de cuadrados \\(D(a)\\), y \\(\\hat{\\beta}\\) es el estimador usual de mínimos cuadrados de los coeficientes (sin penalizar). En azul está la restricción:\n\n\n\nRidge vs Lasso (Hastie, Tibshirani y Friedman)\n\n\n\n\n\n\n\n\nRegularización para modelos lineales\n\n\n\n\nEn regresión ridge, los coeficientes se encogen gradualmente desde la solución no restringida hasta el origen. Ridge es un método de encogimiento de coeficientes. Regresión ridge es especialmente útil cuando tenemos varias variables de entrada fuertemente correlacionadas. Regresión ridge intenta encoger juntos coeficientes de variables correlacionadas para reducir varianza en las predicciones.\nEn regresión lasso, los coeficientes se encogen gradualmente, pero también se excluyen variables del modelo. Por eso lasso es un método de encogimiento y selección de variables. Lasso encoge igualmente coeficientes para reducir varianza, pero también comparte similitudes con regresión de mejor subconjunto, en donde para cada número de variables \\(l\\) buscamos escoger las \\(l\\) variables que den el mejor modelo. Sin embargo, el enfoque de lasso es más escalable y puede calcularse de manera más simple.\n\n\n\nNota: es posible también utilizar una penalización que mezcla ridge y lasso:\n\\[\\lambda \\left (\\alpha \\sum_j |a_j| + (1-\\alpha)\\sum_j a_j^2 \\right )\\]\ny \\(\\alpha\\) es un parámetro que podemos afinar:\n\n# elastic net = ridge + lasso\n# mixture es alpha y penalty es lambda\nmodelo_enet &lt;- linear_reg(mixture = 0.5, penalty = 0.05)\n# y si queremos afinar:\nmodelo_enet &lt;- linear_reg(mixture = tune(), penalty = tune())",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularización y variabilidad</span>"
    ]
  },
  {
    "objectID": "05-regularizacion-1.html#regularización-con-descenso-en-gradiente",
    "href": "05-regularizacion-1.html#regularización-con-descenso-en-gradiente",
    "title": "5  Regularización y variabilidad",
    "section": "5.7 Regularización con descenso en gradiente",
    "text": "5.7 Regularización con descenso en gradiente\nOtra forma de hacer regularización que se utiliza comunmente se basa en el método de minimización que usamos para obtener nuestra función \\(\\hat{f}\\) para hacer predicciones. La idea es utilizar un método iterativo que comience con una \\(f_0\\) simple, y luego iterar a una nueva \\(f_1\\) que se adapta mejor a los datos pero no es muy diferente a \\(f_0\\). En lugar de seguir iterando hasta llegar a un mínimo de \\(L(f),\\) evaluamos con una muestra de prueba para encontrar un lugar apropiado para detenernos (early stopping). También podemos modificar \\(L(f)\\) en cada paso para evitar atorarnos en un mínimo sobreajustado.\nUna manera de hacer esto es usando el método de descenso estocástico, (ver apéndices Apéndice A y Apéndice B) que consiste en:\n\nEn cada iteración \\(i\\) construimos una función de pérdida \\(L^{(i)}(f)\\) basada solamente en una parte de los datos (batch).\nEn cada iteración \\(i\\) sólo nos movemos en una dirección de descenso para los parámetros, sin intentar buscar un mínimo local o global. La dirección de descenso está dada por \\(-\\nabla L^{(i)}\\).\n\nAunque este método es más útil en casos como redes neuronales o métodos basados en árboles, podemos comenzar por un ejemplo en regresión lineal para entender su efecto regularizador:\n\nlibrary(keras3)\nx_grasa &lt;- grasa_receta |&gt; prep() |&gt; juice() |&gt; \n  select(abdomen, edad, antebrazo, biceps, estatura, muslo, pecho, peso) |&gt; \n  as.matrix()\nvars_nombres &lt;- colnames(x_grasa)\ny_grasa &lt;- grasa_receta |&gt; prep() |&gt; juice() |&gt; pull(grasacorp)\n## keras tiene distintos algos de optimización\nmodelo_reg &lt;- keras_model_sequential() |&gt; \n  layer_dense(units = 1, \n    kernel_initializer = initializer_constant(0))\nmodelo_reg |&gt; compile(\n  loss = \"mse\",\n  optimizer = optimizer_sgd(learning_rate = 0.01)\n)\n# esto es más eficiente hacerlo con callbacks en general:\npesos_tbl &lt;- map_dfr(1:400, function(epoca){\n    modelo_reg |&gt; fit(\n      x = x_grasa, y = y_grasa,\n      epochs = 1, \n      verbose = FALSE)\n    pesos_tbl &lt;- get_weights(modelo_reg)[[1]] |&gt; t() |&gt; \n      as_tibble() \n    names(pesos_tbl) &lt;- vars_nombres\n    pesos_tbl |&gt; mutate(epoca = epoca)\n  }\n)\n\n\nlibrary(patchwork)\ng_dest &lt;- pesos_tbl |&gt; pivot_longer(cols  = -contains(c(\"grasacorp\", \"epoca\"))) |&gt; \n  ggplot(aes(x = epoca, y = value, colour = name)) + \n  geom_line(linewidth = 1.1) +   scale_colour_manual(values = cbb_palette) +\n  scale_x_continuous(trans  = compose_trans(\"log10\", \"reverse\")) +\n  labs(subtitle = \"Descenso estocástico\")\n\ng_dest + g_l2\n\n\n\n\n\n\n\n\nY vemos que si inicializamos el proceso de minimización con valores chicos, pararnos en una época (iteración completa sobre los datos) nos permite tener un efecto similar al de utilizar regularización tipo L2.\n\n\n\n\n\n\nDescenso estocástico\n\n\n\nEl método de descenso estocástico (usualmente por minilotes) nos permite resolver problemas de optimización, y muchas veces actúa también como regularizador (al cambiar en cada paso la función de pérdida, y utilizando early stopping). Sus ventajas son:\n\nAl cambiar la función de pérdida en cada paso, es posible escapar de puntos estacionarios subóptimos (si el problema tiene varios puntos estacionarios, es decir, donde el gradiente es cero). Evitamos mínimos locales sobreajustados: estos desaparecen cuando consideramos “lotes” o “batches” de datos, en lugar del conjunto completo para cada iteración.\nGeneralmente detenemos las iteraciones cuando el error de validación deja de disminuir, lo cual es una forma de regularización que mantiene los parámetros en valores relativamente chicos.\nEs eficiente en el sentido de que no es necesario utilizar todo los datos para hacer un paso suficientemente bueno, y es escalable a grandes conjuntos de datos.\n\nEs crucial escoger un tamaño de paso adecuado para cada problema. Generalmente se considera un parámetro que debe ser afinado, de manera similar al parámetro de regularización L2 que vimos arriba.\n\n\nFinalmente, notamos que este tipo de regularización resulta en comportamientos a veces inesperados del desempeño predictivo en función del tamaño del modelo utilizado, por ejemplo, puede ocurrir el fenómeno de doble descenso (ver James et al. (2014)):\n\nComenzando en modelos muy simples, al incrementar la complejidad o tamaño del model el error de prueba disminuye.\nA partir de cierto momento, incrementar la complejidad incrementa el error de prueba (como esperaríamos por un aumento en varianza)\nSin embargo, si continuamos incrementando la complejidad, el error de prueba vuelve a descender, y llega a un valor más bajo que todos los observados anteriormente.\n\nEsto se puede deber, por ejemplo, pues cuando existen pocos grados de libertad, la solución es escencialmente única (del paso 1 a 2 de arriba). Sin embargo, con modelos más grandes aparecen muchas otras soluciones posibles: las mejores en entrenamiento son peores en generalización, pero las que son “regularizadas” por descenso estocástico son mejores en desempeño predictivo. Puedes pensar en un polinomio de grado muy alto: la solución de grado alto que encontramos con descenso estocástico puede ser muy suave, en contraste con lo que encontraríamos si llegáramos a un mínimo global o cercano al global.\n\n\n\n\nChambers, J. M., W. S. Cleveland, B. Kleiner, y P. A. Tukey. 1983. Graphical Methods for Data Analysis. Chapman & Hall statistics series. Wadsworth International Group. https://books.google.com.mx/books?id=I-tQAAAAMAAJ.\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularización y variabilidad</span>"
    ]
  },
  {
    "objectID": "06-redes-neuronales-1.html",
    "href": "06-redes-neuronales-1.html",
    "title": "6  Redes neuronales (intro)",
    "section": "",
    "text": "6.1 Introducción a redes neuronales\nEn partes anteriores, vimos cómo hacer más flexibles los métodos de regresión: la idea es construir entradas derivadas a partir de las variables originales, e incluirlas en el modelo de regresión. Este enfoque es bueno cuando tenemos relativamente pocas variables originales de entrada, y tenemos una idea de qué variables derivadas es buena idea incluir (por ejemplo, splines para una variable como edad, interacciones para variables importantes, etc). Sin embargo, si hay una gran cantidad de entradas, esta técnica puede ser prohibitiva en términos de cálculo y trabajo manual.\nPor ejemplo, si tenemos unas 100 entradas numéricas, al crear todas las interacciones \\(x_i x_j\\) y los cuadrados \\(x_i^2\\) terminamos con unas 5150 variables. Para el problema de dígitos (256 entradas o pixeles) terminaríamos con unas 32 mil entradas adicionales. Aún cuando es posible regularizar, en estos casos suena más conveniente construir entradas derivadas a partir de los datos.\nPara hacer esto, consideramos entradas \\(X_1, . . . , X_p\\), y supongamos que tenemos un problema regresión donde queremos predecir \\(Y\\). Aunque hay muchas maneras de construir entradas derivadas, una manera simple sería construir \\(m\\) nuevas entradas mediante:\n\\[a_k = h \\left ( \\theta_{k,0} + \\sum_{j=1}^p \\theta_{k,j}x_j\n\\right)\\]\npara \\(k=1,\\ldots, m\\), donde \\(h\\) es una función no lineal (logística o relu entre otras), y las \\(\\theta\\) son parámetros que seleccionaremos más tarde. La idea es hacer combinaciones lineales de variables transformadas.\nModelamos ahora la respuesta usando las entradas derivadas en lugar de las originales en una regresión lineal:\n\\(a_1, . . . , a_m\\): \\[f(x) =  \\beta_0 + \\sum_{j=1}^m \\beta_ja_j\\]\nPodemos representar este esquema con una red dirigida (\\(m=3\\) variables derivadas):",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neuronales (intro)</span>"
    ]
  },
  {
    "objectID": "06-redes-neuronales-1.html#introducción-a-redes-neuronales",
    "href": "06-redes-neuronales-1.html#introducción-a-redes-neuronales",
    "title": "6  Redes neuronales (intro)",
    "section": "",
    "text": "La función logística\nUna de las transformaciones \\(h\\) más comunes para construir entradas derivadas es la función logística:\n\n\n\n\n\n\nNota\n\n\n\nLa función logística está dada por \\[h(x)=\\frac{e^x}{1+e^x}\\]\n\n\n\nh &lt;- function(x){exp(x)/(1+exp(x)) }\nggplot(tibble(x = seq(-6, 6, 0.01)), aes(x = x)) + stat_function(fun = h)\n\n\n\n\n\n\n\n\nObservaciones:\n\n¿Por qué usar \\(h\\) para las entradas derivadas \\(a_k\\)? En primer lugar, nótese que si no transformamos con alguna función no lineal \\(h\\), el modelo final \\(p_1\\) para la probabilidad condicional es el mismo que el de regresión logística (combinaciones lineales de combinaciones lineales son combinaciones lineales). Sin embargo, al transformar con \\(h\\), las \\(x_j\\) contribuyen de manera no lineal a las entradas derivadas.\nLas variables \\(a_k\\) que se pueden obtener son similares (para una variable de entrada) a los splines que vimos en la parte anterior.\nEs posible demostrar que si se crean suficientes entradas derivadas (\\(m\\) es suficientemente grande), entonces la función \\(f(x)\\) puede aproximar cualquier función continua. La función \\(h\\) (que se llama función de activación no es especial: funciones continuas con forma similar a la sigmoide (logística) pueden usarse también (por ejemplo, arcotangente, o lineal rectificada). La idea es que cualquier función se puede aproximar mediante superposición de funciones tipo sigmoide (ver por ejemplo Cybenko 1989, Approximation by Superpositions of a Sigmoidal Function).\n\n\n\n¿Cómo construyen entradas las redes neuronales?\nComencemos por un ejemplo simple de clasificación binaria con una sola entrada \\(x\\). Supondremos que el modelo verdadero está dado por:\n\nh &lt;- function(x){\n    1/(1 + exp(-x)) # es lo mismo que exp(x)/(1 + exp(x))\n}\nx &lt;- seq(-2, 2, 0.1)\nf &lt;- atan(2 - 2 * x^2)\nset.seed(2805721)\nx_1 &lt;- runif(10, -2, 2)\ny &lt;- rnorm(10, atan(2 - 2 * x_1^2), 0.2)\ndatos &lt;- tibble(x_1, y)\ndat_f &lt;- tibble(x, f)\ng &lt;- ggplot(dat_f) + geom_line(aes(x, f))\ng\n\n\n\n\n\n\n\ng + geom_point(data = datos, aes(x = x_1, y = y), colour = 'red')\n\n\n\n\n\n\n\n\ndonde adicionalmente graficamos 30 datos simulados. Recordamos que queremos ajustar la curva roja, que da la probabilidad condicional de clase. Podríamos ajustar un modelo de regresión logística expandiendo manualmente el espacio de entradas agregando \\(x^2\\), y obtendríamos un ajuste razonable. Pero la idea aquí es que podemos crear entradas derivadas de forma automática.\nSupongamos entonces que pensamos crear dos entradas \\(a_1\\) y \\(a_2\\), funciones de \\(x_1\\), y luego predecir \\(g.1\\), la clase, en función de estas dos entradas. Por ejemplo, podríamos tomar:\n\n\n\n\n\n\n\n\n\ndonde hacemos una regresión para predecir \\(y\\) mediante \\[f(a) = \\beta_0 + \\beta_1a_1+\\beta_2 a_2,\\] \\(a_1\\) y \\(a_2\\) están dadas por \\[a_1(x)=h(\\beta_{1,0} + \\beta_{1,1} x_1),\\] \\[a_2(x)=h(\\beta_{2,0} + \\beta_{2,1} x_1).\\]\nPor ejemplo, podríamos tomar\n\na_1 &lt;- h( 1 + 2 * x)  # 2(x+1/2)\na_2 &lt;- h(-1 + 2 * x)  # 2(x-1/2) # una es una versión desplazada de otra.\n\nLas funciones \\(a_1\\) y \\(a_2\\) dependen de \\(x\\) de la siguiente forma:\n\ndat_a &lt;- tibble(x = x, a_1 = a_1, a_2 = a_2)\ndat_a_2 &lt;- dat_a |&gt; gather(variable, valor, a_1:a_2)\nggplot(dat_a_2, aes(x=x, y=valor, colour=variable, group=variable)) + geom_line()\n\n\n\n\n\n\n\n\nSi las escalamos y sumamos, obtenemos\n\ndat_a &lt;- tibble(x=x, a_1 = a_1, a_2 =  a_2, \n  suma = -1.5 +  6 * a_1 -  6 * a_2)\ndat_a_2 &lt;- dat_a |&gt; \n  pivot_longer(a_1:suma, names_to = \"variable\", values_to = \"valor\")\nggplot(dat_a_2, aes(x = x, y = valor, colour = variable, group = variable)) + geom_line()\n\n\n\n\n\n\n\n\ny finalmente obtenemos:\n\ndat_2 &lt;- tibble(x, f = (-1.5 + 6 * a_1 - 6 * a_2))\nggplot(dat_2, aes(x=x, y = f)) + geom_line()+\ngeom_line(data=dat_f, aes(x=x,y=f), col='red') +\n   geom_point(data = datos, aes(x = x_1, y = y))\n\n\n\n\n\n\n\n\nque da un ajuste razonable. Este es un ejemplo de cómo la mezcla de dos funciones logísticas puede replicar esta función con forma de chipote. Otras funciones más complejas se pueden obtener incluyendo más \\(a_j\\)’s que son versiones escaladas y desplazadas de la función logística. El mecanismo para combinar estas \\(a_j\\)’s es similar al de los splines que vimos en la sección anterior.\n\n\n¿Cómo ajustar los parámetros?\nPara encontrar los mejores parámetros, minimizamos la devianza sobre los parámetros \\(\\beta_0,\\beta_1,\\beta_2\\) y \\(\\beta_{1,0},\\beta_{1,1},\\beta_{2,0},\\beta_{2,1}\\).\nVeremos más adelante que conviene hacer esto usando descenso o en gradiente o descenso en gradiente estocástico, pero por el momento usamos la función optim de R para minimizar la devianza. En primer lugar, creamos una función que para todas las entradas calcula los valores de salida. En esta función hacemos feed-forward de las entradas a través de la red para calcular la salida.\n\n## esta función calcula los valores de cada nodo en toda la red,\n## para cada entrada\nfeed_fow &lt;- function(beta, x){\n  a_1 &lt;- h(beta[1] + beta[2] * x) # calcula variable 1 de capa oculta\n  a_2 &lt;- h(beta[3] + beta[4] * x) # calcula variable 2 de capa oculta\n  f &lt;- (beta[5] + beta[6] * a_1 + beta[7] * a_2) # calcula capa de salida\n  f\n}\n\nNótese que simplemente seguimos el diagrama mostrado arriba para hacer los cálculos, combinando linealmente las entradas en cada capa.\nAhora definimos una función para calcular la devianza. Conviene crear una función que crea funciones, para obtener una función que sólo se evalúa en los parámetros para cada conjunto de datos de entrenamiento fijos:\n\nperdida_cuad_fun &lt;- function(x, y){\n    # esta función es una fábrica de funciones\n   perdida_cuad &lt;- function(beta){\n      f &lt;- feed_fow(beta, x)\n      mean((y - f)^2)\n   }\n  perdida_cuad\n}\n\nPor ejemplo:\n\nperdida_cuad &lt;- perdida_cuad_fun(x_1, y) # crea función\n## ahora dev toma solamente los 7 parámetros beta:\nperdida_cuad(c(0,0,0,0,0,0,0))\n\n[1] 1.034101\n\n\nFinalmente, intentamos resolver el problema de minimización de la pérdida cuadrática de los datos de entrenamiento. Para esto usaremos la función optim de R:\n\nset.seed(5)\nsalida &lt;- optim(rnorm(7), perdida_cuad, method = 'BFGS') # inicializar al azar punto inicial\nbeta &lt;- salida$par\nbeta\n\n[1] -9.097577  5.315080 -6.973249 -6.762489  1.093208 -3.309966 -2.488820\n\n\nY ahora podemos graficar con el vector \\(\\beta\\) encontrado:\n\n## hacer feed forward con beta encontrados\np_2 &lt;- feed_fow(beta, x)\ndat_2 &lt;- data.frame(x, p_2 = p_2)\nggplot(dat_2, aes(x = x, y = p_2)) + geom_line()+\ngeom_line(data = dat_f, aes(x = x, y = f), col='red') +\n   geom_point(data = datos, aes(x = x_1, y = y))\n\n\n\n\n\n\n\n\nLos coeficientes estimados, que en este caso muchas veces se llaman pesos, son:\n\nbeta |&gt; round(2)\n\n[1] -9.10  5.32 -6.97 -6.76  1.09 -3.31 -2.49\n\n\nque parecen ser muy grandes. Igualmente, de la figura vemos que el ajuste no parece ser muy estable (esto se puede confirmar corriendo con distintos conjuntos de entrenamiento). Podemos entonces regularizar ligeramente la devianza para resolver este problema. En primer lugar, definimos la devianza regularizada (ridge), donde penalizamos todos los coeficientes que multiplican a una variable, pero no los intercepts:\n\nperdida_cuad_fun_r &lt;- function(x, y, lambda){\n    # esta función es una fábrica de funciones\n   perdida_reg &lt;- function(beta){\n         f &lt;- feed_fow(beta, x)\n         # en esta regularizacion quitamos sesgos, pero puede hacerse también con sesgos.\n         mean((y - f)^2) + lambda * sum(beta[c(2,4,6:7)]^2) \n   }\n  perdida_reg\n}\n\n\nperdida_r &lt;- perdida_cuad_fun_r(x_1, y, 0.001) # crea función dev\nset.seed(5)\nsalida &lt;- optim(rnorm(7, 0, 1), perdida_r, method = 'BFGS') # inicializar al azar punto inicial\nbeta &lt;- salida$par\nperdida_cuad(beta) / nrow(datos)\n\n[1] 0.001831033\n\nf_2 &lt;- feed_fow(beta, x)\ndat_2 &lt;- data.frame(x, f_2 = f_2)\nbeta\n\n[1] -2.072798  2.136928 -4.317303 -4.221802  1.458438 -3.053558 -2.952405\n\nggplot(dat_2, aes(x = x, y = f_2)) + geom_line() +\ngeom_line(data = dat_f, aes(x = x, y = f), col='red') +\n   geom_point(data = datos, aes(x = x_1, y = y))\n\n\n\n\n\n\n\n\ny obtenemos un ajuste más estable. Podemos usar también keras. El modelo, con una capa intermedia de dos unidades, y regularización ridge para los coeficientes, y optimización por descenso en gradiente se define como:\n\nlibrary(keras3)\n# para reproducibilidad:\ntensorflow::set_random_seed(13) \n# construir modelo\nejemplo_mod &lt;- keras_model_sequential()\nejemplo_mod |&gt; \n   layer_dense(units = 2, \n    activation = \"sigmoid\", kernel_regularizer = regularizer_l2(0.001)) |&gt; \n  layer_dense(units = 1, \n    activation = \"linear\", kernel_regularizer = regularizer_l2(0.001))\n\n\nx_mat &lt;- as.matrix(datos$x_1, ncol = 1)\ny &lt;- datos$y\n# usamos devianza como medida de error y descenso en gradiente:\nejemplo_mod |&gt; compile(loss = \"mse\", \n  optimizer = optimizer_sgd(learning_rate = 0.2),\n  metrics = \"mse\")\n# nota: esta learning rate (lr) es demasiado alta para problemas típicos\nhistoria &lt;- ejemplo_mod |&gt; \n  fit(x_mat, y, \n      batch_size = nrow(x_mat), epochs = 500, verbose = 0)\n\nDespués de verificar convergencia (chécalo examinando la variable historia), graficamos para ver que obtuvimos resultados similares:\n\ndat_3 &lt;- tibble(x = x, f_2 = predict(ejemplo_mod, as.matrix(x, ncol = 1))[,1])\n\n2/2 - 0s - 19ms/step\n\nggplot(dat_3, aes(x = x, y = f_2)) + geom_line()+\ngeom_line(data = dat_f, aes(x = x, y = f), col='red') +\n   geom_point(data = datos, aes(x = x_1, y = y))\n\n\n\n\n\n\n\n\nLos coeficientes obtenidos se muestran abajo. Nótese: la primera componente de la lista son los coeficientes de la unidad 1 y 2 para \\(x\\). La segunda son los sesgos u ordenadas al origen, la tercera los coeficientes de la respuesta para las unidades 1 y 2, y el cuarto es el sesgo u ordenada al origen de la unidad de salida:\n\nget_weights(ejemplo_mod)\n\n[[1]]\n         [,1]      [,2]\n[1,] -2.45195 -2.848297\n\n[[2]]\n[1] -2.567333  2.578543\n\n[[3]]\n          [,1]\n[1,] -3.460155\n[2,]  3.080145\n\n[[4]]\n[1] -1.392124\n\n\nEjercicio: compara los coeficientes que obtuviste en este ejemplo con los anteriores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neuronales (intro)</span>"
    ]
  },
  {
    "objectID": "06-redes-neuronales-1.html#interacciones-en-redes-neuronales",
    "href": "06-redes-neuronales-1.html#interacciones-en-redes-neuronales",
    "title": "6  Redes neuronales (intro)",
    "section": "6.2 Interacciones en redes neuronales",
    "text": "6.2 Interacciones en redes neuronales\nEs posible capturar interacciones con redes neuronales. Consideremos el siguiente ejemplo simple:\n\nf &lt;- function(x1, x2){\n  2 + 0.1* x1 + 0.1 * x2 + 10 * (x1 - 0.5) * (x2 - 0.5)\n}\ndat &lt;- expand.grid(x1 = seq(0, 1, 0.05), x2 = seq(0, 1, 0.05))\ndat &lt;- dat |&gt; mutate(f = f(x1, x2))\nggplot(dat, aes(x=x1, y=x2)) + geom_tile(aes(fill=f))\n\n\n\n\n\n\n\n\nEsta función puede entenderse como un o exclusivo: la respuesta es alta sólo cuando \\(x_1\\) y \\(x_2\\) tienen valores opuestos (\\(x_1\\) grande pero \\(x_2\\) chica y viceversa).\nNo es posible modelar correctamente esta función mediante el modelo lineal (sin interacciones). Pero podemos incluir la interacción en el modelo lineal o intentar usar una red neuronal. Primero simulamos unos datos y probamos el modelo logístico con y sin interacciones:\n\nset.seed(322)\nn &lt;- 2000\ndat_ent &lt;- tibble(x1 = rbeta(n, 1, 1), x2 = rbeta(n, 1, 1)) |&gt;\n  mutate(f = f(x1, x2)) |&gt;\n  mutate(y = f + rnorm(n, 0, 0.1))\nmod_1 &lt;- lm(y ~ x1 + x2, data = dat_ent)\nmod_1\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dat_ent)\n\nCoefficients:\n(Intercept)           x1           x2  \n     1.8936       0.2046       0.2097  \n\n\nEl resultado del modelo lineal no es bueno:\n\ntibble(y_hat = fitted(mod_1), y = dat_ent$y) |&gt; \n  ggplot(aes(x = y_hat, y = y)) + geom_point(color = \"red\") +\n  geom_abline() +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\nSin embargo, agregando una interacción lo mejoramos considerablemente (examina la raíz del error cuadrático medio, por ejemplo):\n\nmod_2 &lt;- lm(y ~ x1 + x2 + x1:x2, data = dat_ent)\nmod_2\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x1:x2, data = dat_ent)\n\nCoefficients:\n(Intercept)           x1           x2        x1:x2  \n      4.499       -4.895       -4.885        9.964  \n\ntibble(y_hat = fitted(mod_2), y = dat_ent$y) |&gt; \n  ggplot(aes(x = y_hat, y = y)) + geom_point(color = \"red\") +\n  geom_abline() +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\nObservese la gran diferencia de error entre los dos modelos (en este caso, el sobreajuste no es un problema).\nAhora consideramos qué red neuronal puede ser apropiada.\n\ntensorflow::set_random_seed(421) \nmod_inter &lt;- keras_model_sequential()\nmod_inter |&gt; \n  layer_dense(units = 4, activation = \"sigmoid\",\n              name = \"capa_intermedia\") |&gt;\n  layer_dense(units = 1, name = \"capa_final\",\n              activation = \"linear\") \n\n\nmod_inter |&gt; compile(loss = \"mse\", \n  optimizer = optimizer_sgd(learning_rate = 0.3, momentum = 0.5))\nhistoria &lt;- mod_inter |&gt; \n  fit(dat_ent |&gt; select(x1, x2) |&gt; as.matrix(), dat_ent$y,\n      batch_size = 20,\n      epochs = 100, verbose = 0)\n\nVerificamos que esta red captura la interacción:\n\npreds &lt;- predict(mod_inter,\n  dat |&gt; select(x1, x2) |&gt; as.matrix())\n\n14/14 - 0s - 3ms/step\n\ndat &lt;- dat |&gt; mutate(f_red = preds)\nggplot(dat, aes(x = x1, y = x2)) + \n  geom_tile(aes(fill = f_red))\n\n\n\n\n\n\n\n\n\npreds_ent &lt;- predict(mod_inter, dat_ent |&gt; select(x1, x2) |&gt; as.matrix())\n\n63/63 - 0s - 696us/step\n\ntibble(pred = preds_ent[,1], f = dat_ent$y) |&gt; \n  ggplot(aes(x = pred, y = f)) +\n  geom_point() +\n  geom_abline(colour = \"red\") +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\nAunque podemos extraer los cálculos de la red ajustada, vamos a hacer el cálculo de la red a mano. La función feed forward es:\n\nbeta &lt;- get_weights(mod_inter)\nfeed_fow &lt;- function(beta, x){\n  a &lt;- h(t(beta[[1]]) %*% x + as.matrix(beta[[2]], 2, 1)) \n  f &lt;- t(beta[[3]]) %*% a + as.matrix(beta[[4]], 1, 1)\n  f\n}\n\nObservación: ¿cómo funciona esta red? Consideremos la capa intermedia (3 unidades) para cuatro casos: \\((0,0), (0,1), (1,0), (1,1)\\):\n\nmat_entrada &lt;- tibble(x_1 = c(0,0,1,1), x_2 = c(0,1,0,1)) |&gt; as.matrix()\ncapa_1 &lt;- keras_model(inputs = mod_inter$inputs[[1]],\n    outputs = get_layer(mod_inter, \"capa_intermedia\")$output)\npred_mat &lt;- predict(capa_1, mat_entrada) |&gt; round(2)\n\n1/1 - 0s - 21ms/step\n\nrownames(pred_mat) &lt;- c(\"apagadas\", \"segunda\", \"primera\", \"ambas\")\npred_mat\n\n         [,1] [,2] [,3] [,4]\napagadas 0.09 0.00 0.57 0.07\nsegunda  0.55 0.05 0.07 0.00\nprimera  0.00 0.05 0.05 0.56\nambas    0.05 0.56 0.00 0.05\n\n\nLos pesos de la última capa son:\n\nbeta[3:4]\n\n[[1]]\n          [,1]\n[1,] -5.659053\n[2,]  5.010898\n[3,]  5.338413\n[4,] -5.363252\n\n[[2]]\n[1] 2.272053\n\n\nEjercicio: interpreta la red en términos de qué unidades están encendidas (valor cercano a 1) o apagadas (valor cercano a 0). ¿Puedes ajustar este modelo con dos tres unidades intermedias? Haz varias pruebas: ¿qué dificultades encuentras?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neuronales (intro)</span>"
    ]
  },
  {
    "objectID": "06-redes-neuronales-1.html#cálculo-en-redes-feed-forward",
    "href": "06-redes-neuronales-1.html#cálculo-en-redes-feed-forward",
    "title": "6  Redes neuronales (intro)",
    "section": "6.3 Cálculo en redes: feed-forward",
    "text": "6.3 Cálculo en redes: feed-forward\nAhora generalizamos lo que vimos arriba para definir la arquitectura básica de redes neuronales y cómo se hacen cálculos en las redes.\n\n\n\n\n\n\nTip\n\n\n\nA las variables originales les llamamos capa de entrada de la red, y a la variable de salida capa de salida. Puede haber más de una capa intermedia. A estas les llamamos capas ocultas.\nCuando todas las conexiones posibles de cada capa a la siguiente están presente, decimos que la red es completamente conexa.\n\n\n\n\n\n\n\n\n\n\n\nComo vimos en el ejemplo de arriba, para hacer cálculos en la red empezamos con la primera capa, hacemos combinaciones lineales y aplicamos nuestra función no lineal \\(h\\). Una vez que calculamos la segunda capa, podemos calcular la siguiente de la misma forma: combinaciones lineales y aplicación de \\(h\\). Y así sucesivamente hasta que llegamos a la capa final.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neuronales (intro)</span>"
    ]
  },
  {
    "objectID": "06-redes-neuronales-1.html#notación",
    "href": "06-redes-neuronales-1.html#notación",
    "title": "6  Redes neuronales (intro)",
    "section": "6.4 Notación",
    "text": "6.4 Notación\nSea \\(L\\) el número total de capas. En primer lugar, para un cierto caso de entrada \\(x = (x_1,x_2,\\ldots, x_p)\\), denotamos por:\n\n\\(a^{(l)}_j\\) el valor que toma la unidad \\(j\\) de la capa \\(l\\), para \\(j=0,1,\\ldots, n_{l}\\), donde \\(n_l\\) es el número de unidades de la capa \\(l\\).\nPonemos \\(a^{(l)}_0=1\\) para lidiar con los sesgos.\n*En particular, ponemos \\(a^{(1)}_j = x_j\\), que son los valores de las entradas (primera capa)\nPara un problema de regresión, la última capa solo tiene un elemento, que es \\(y = a^{(L)}\\).\n\nAdicionalmente, escribimos\n\\(\\theta_{i,k}^{(l)}=\\) es el peso de entrada \\(a_{k}^{(l-1)}\\) de capa \\(l-1\\) en la entrada \\(a_{i}^{(l)}\\) de la capa \\(l\\).\nLos sesgos están dados por \\[\\theta_{i,0}^{(l)}\\]\n\nEjemplo\nEn nuestro ejemplo, tenemos que en la capa \\(l=3\\) hay dos unidades. Así que podemos calcular los valores \\(a^{(3)}_1\\) y \\(a^{(3)}_2\\). Están dados por\n\\[a_1^{(3)} = h(\\theta_{1,0}^{(2)} + \\theta_{1,1}^{(2)} a_1^{(2)}+ \\theta_{1,2}^{(2)}a_2^{(2)}+ \\theta_{1,3}^{(2)} a_3^{(2)})\\] \\[a_2^{(3)} = h(\\theta_{2,0}^{(2)} + \\theta_{2,1}^{(2)} a_1^{(2)}+ \\theta_{2,2}^{(2)}a_2^{(2)}+ \\theta_{2,3}^{(2)} a_3^{(2)})\\]\nComo se ilustra en la siguiente gráfica:\n\n\n\n\n\n\n\n\n\nPara visualizar las ordenadas (que también se llaman sesgos en este contexto), ponemos \\(a_{0}^{(2)}=1\\).\n\n\n\n\n\n\n\n\n\n\n\nEjemplo\nConsideremos propagar a la capa 3 a partir de la capa 2. Usaremos los siguientes pesos para capa 3 y valores de la capa 2 (en gris están los sesgos):\n\n\n\n\n\n\n\n\n\nQue en nuestra notación escribimos como \\[a^{(2)}_0 = 1, a^{(2)}_1 = -2, a^{(2)}_2 = 5, a^{(2)}=3\\] y los pesos son, para la primera unidad: \\[\\theta^{(2)}_{1,0} = 3,  \\,\\,\\, \\theta^{(2)}_{1,1} = 1.5,\\,\\,\\,\\theta^{(2)}_{1,2} = -1,\\,\\,\\theta^{(2)}_{1,3} = -0.5 \\] y para la segunda unidad \\[\\theta^{(2)}_{2,0} = 1,  \\,\\,\\, \\theta^{(2)}_{2,1} = 2,\\,\\,\\,\\theta^{(2)}_{2,2} = 0.5,\\,\\, \\theta^{(2)}_{2,3} = -0.2\\] Y ahora queremos calcular los valores que toman las unidades de la capa 3, que son \\(a^{(3)}_1\\) y \\(a^{(3)}_2\\)$\nPara hacer feed forward a la siguiente capa, hacemos entonces\n\\[a^{(3)}_1 = h(3 + a^{(2)}_1 - a^{(2)}_2 -0.5 a_3^{(2)}),\\] \\[a^{(3)}_2 = h(1 + 2a^{(2)}_1 + 0.5a^{(2)}_2 - 0.2 a_3^{(2)}),\\]\nPonemos los pesos y valores de la capa 2 (incluyendo sesgo):\n\na_2 &lt;- c(1, -2, 5, 3) # ponemos un 1 al principio para el sesgo\ntheta_2_1 = c(3, 1.5, -1.0, -0.5)\ntheta_2_2 = c(1, 2, 0.5, -0.2)\n\ny calculamos\n\na_3 &lt;- c(1, h(sum(theta_2_1*a_2)),h(sum(theta_2_2*a_2))) # ponemos un 1 al principio\na_3\n\n[1] 1.000000000 0.001501182 0.249739894",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neuronales (intro)</span>"
    ]
  },
  {
    "objectID": "06-redes-neuronales-1.html#algoritmo-de-feed-forward",
    "href": "06-redes-neuronales-1.html#algoritmo-de-feed-forward",
    "title": "6  Redes neuronales (intro)",
    "section": "6.5 Algoritmo de Feed forward",
    "text": "6.5 Algoritmo de Feed forward\nPara calcular los valores de salida de una red a partir de pesos y datos de entrada, usamos el algoritmo feed-forward, calculando capa por capa.\n\n\n\n\n\n\nFeed-forward\n\n\n\nPara la primera capa, escribimos las variables de entrada: \\[a^{(1)}_j = x_j, j=1\\ldots,n_1\\] Para la primera capa oculta, o la segunda capa \\[a^{(2)}_j = h\\left( \\theta_{j,0}^{(1)}+ \\sum_{k=1}^{n_1}  \\theta_{j,k}^{(1)}  a^{(1)}_k    \\right), j=1\\ldots,n_2\\] para la \\(l\\)-ésima capa: \\[a^{(l)}_j = h\\left( \\theta_{j,0}^{(l-1)}+ \\sum_{k=1}^{n_{l-1}}  \\theta_{j,k}^{(l-1)}  a^{(l-1)}_k    \\right), j=1\\ldots,n_{l}\\] y así sucesivamente. Para la capa final o de salida en un problema de regresión, suponiendo que tenemos \\(L\\) capas (\\(L-2\\) capas ocultas): \\[f(x) =     \\theta_{1,0}^{(L-1)}+ \\sum_{k=1}^{n_{L-1}}  \\theta_{1,k}^{(L-1)}  a^{(L-1)}_k     .\\]\n\n\nNótese que entonces:\n\n\n\n\n\n\nTip\n\n\n\nCada capa se caracteriza por el conjunto de parámetros \\(\\Theta^{(l)}\\), que es una matriz de \\(n_l\\times n_{l-1}\\).\nLa red completa entonces se caracteriza por:\n\nLa estructura elegida (número de capas ocultas y número de nodos en cada capa oculta).\nLas matrices de pesos en cada capa \\(\\Theta^{(1)},\\Theta^{(2)},\\ldots, \\Theta^{(L-1)}\\)\n\n\n\nAdicionalmente, escribimos en forma vectorial: \\[a^{(l)} = (a^{(l)}_0, a^{(l)}_1, a^{(l)}_2, \\ldots, a^{(l)}_{n_l})^t\\]\nPara calcular la salidas, igual que hicimos, antes, propagaremos hacia adelante los valores de las variables de entrada usando los pesos. Agregando entradas adicionales en cada capa \\(a_0^{(l)}\\), \\(l=1,2,\\ldots, L-1\\), donde \\(a_0^{l}=1\\), y agregando a \\(\\Theta^{(l)}\\) una columna con las ordenadas al origen (o sesgos) podemos escribir:\n\n\n\n\n\n\nFeed-forward matricial\n\n\n\n\nCapa 1 (vector de entradas) \\[ a^{(1)} = x\\]\nCapa 2 \\[ a^{(2)} = h(\\Theta^{(1)}a^{(1)})\\]\nCapa \\(l\\) (oculta) \\[ a^{(l)} = h(\\Theta^{(l-1)}a^{(l-1)})\\] donde \\(h\\) se aplica componente a componente sobre los vectores correspondientes.\nCapa de salida:\n\nEn un problema de regresión, la capa de salida se calcula como un regresión lineal: \\[a^{(L)}= f(x) = \\Theta^{(L-1)}a^{(L-1)}\\] Nótese que feed-foward consiste principalmente de multiplicaciones de matrices con algunas aplicaciones de \\(h\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neuronales (intro)</span>"
    ]
  },
  {
    "objectID": "06-redes-neuronales-1.html#algoritmo-de-backpropagation-cálculo-del-gradiente-regresión",
    "href": "06-redes-neuronales-1.html#algoritmo-de-backpropagation-cálculo-del-gradiente-regresión",
    "title": "6  Redes neuronales (intro)",
    "section": "6.6 Algoritmo de Backpropagation: cálculo del gradiente (regresión)",
    "text": "6.6 Algoritmo de Backpropagation: cálculo del gradiente (regresión)\nPara ajustar los pesos y sesgos de las redes (valores \\(\\theta\\)), utilizaremos descenso en gradiente y otros algoritmos derivados del gradiente (descenso estocástico). En esta parte entonces veremos cómo calcular estos gradientes con el algoritmo de back-propagation, que es una aplicación de la regla de la cadena para derivar. Back-propagation resulta en una fórmula recursiva donde propagamos errores de la red como gradientes desde el final de red (capa de salida) hasta el principio, capa por capa.\nConsideramos el problema de regresión\nRecordamos que la pérdida cuadrática (con regularización ridge, dividiendo entre 2 por conveniencia) es\n\\[D = -\\frac{1}{2n}\\sum_{i=1}^n (y^{(i)} - f(x^{(i)}))^2  + \\lambda \\sum_{l=2}^{L} \\sum_{k=1}^{n_{l-1}} \\sum_{j=1}^{n_l}(\\theta_{j,k}^{(l)})^2.\\]\nQueremos entonces calcular las derivadas de la devianza o función de pérdida con respecto a cada parámetro \\(\\theta_{j,k}^{(l)}\\). Esto nos proporciona el gradiente para nuestro algoritmo de descenso.\nConsideramos aquí el problema de regresión con pérdida cuadrática y sin regularización. La parte de la parcial que corresponde al término de regularización es fácil de agregar al final.\nRecordamos también nuestra notación para la función logística (o sigmoide):\n\\[h(z)=\\frac{1}{1+e^{-z}}.\\] Necesitaremos su derivada, que está dada por (cálculala): \\[h'(z) = h(z)(1-h(z))\\]\n\nCálculo para un caso de entrenamiento\nPrimero simplificamos el problema y consideramos calcular las parciales para un solo caso de entrenamiento \\((x,y)\\): \\[ D=  \\frac{1}{2}\\left ( y -f(x)\\right)^2 . \\]\nDespués sumaremos sobre toda la muestra de entrenamiento. Entonces queremos calcular \\[\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}}\\]\nY escribiremos, con la notación de arriba, \\[a^{(l+1)}_j = h(z^{(l+1)}_j)\\] donde \\[z^{(l+1)} = \\Theta^{(l)} a^{(l)},\\] que coordenada a coordenada se escribe como \\[z^{(l+1)}_j =  \\sum_{k=0}^{n_{l}}  \\theta_{j,k}^{(l)}  a^{(l)}_k\\]\n\nPaso 1: Derivar respecto a capa \\(l+1\\)\nComo los valores de cada capa determinan los valores de salida y la devianza, podemos escribir (recordemos que \\(a_0^{(l)}=1\\) es constante): \\[D=D(a_0^{(l+1)},a_1^{(l+1)},a_2^{(l+1)},\\ldots, a_{n_{l+1}}^{(l+1)})=D(a_1^{(l+1)},a_2^{(l+1)},\\ldots, a_{n_{l+1}}^{(l+1)})\\]\nAsí que por la regla de la cadena para varias variables: \\[\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}} =\n\\sum_{t=1}^{n_{l}} \\frac{\\partial D}{\\partial a_t^{(l+1)}}\\frac{\\partial a_t^{(l+1)}}\n{\\partial \\theta_{j,k}^{(l)} }\\]\nPero si vemos dónde aparece \\(\\theta_{j,k}^{(l)}\\) en la gráfica de la red:\n\\[ \\cdots a^{(l)}_k \\xrightarrow{\\theta_{j,k}^{(l)}} a^{(l+1)}_j  \\cdots \\rightarrow  D\\] Entonces podemos concluir que \\(\\frac{\\partial a_t^{(l+1)}}{\\partial \\theta_{j,k}^{(l)}} =0\\) cuando \\(t\\neq j\\) (pues no dependen de \\(\\theta_{j,k}^{(l)}\\)),\ny entonces, para toda \\(j=1,2,\\ldots, n_{l+1}, k=0,1,\\ldots, n_{l}\\) \\[\n\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}} =\n\\frac{\\partial D}{\\partial a_j^{(l+1)}}\\frac{\\partial a_j^{(l+1)}}{\\partial \\theta_{j,k}^{(l)} }.\n\\tag{6.1}\\]\nAdicionalmente, como \\[a_j^{(l+1)} = h(z_j^{(l+1)}) = h\\left (\\sum_{k=0}^{n_{l}}  \\theta_{j,k}^{(l)}  a^{(l)}_k \\right )\\] y las \\(a_k^{(l)}\\) no dependen de \\(\\theta_{j,k}^{(l)}\\), tenemos por la regla de la cadena que\n\\[\n\\frac{\\partial a_j^{(l+1)}}{\\partial \\theta_{j,k}^{(l)} } = h'(z_j^{(l+1)})a_k^{(l)}.\n\\]\nEsta última expresión podemos calcularla pues sólo requiere la derivada de \\(h\\) y los valores otenidos en el paso de feed-forward.\n\n\nPaso 2: Obtener fórmula recursiva\nAsí que sólo nos queda calcular las parciales (\\(j = 1,\\ldots, n_l\\)) \\[\\frac{\\partial D}{\\partial a_j^{(l)}}\\]\nPara obtener una fórmula recursiva para esta cantidad (hacia atrás), aplicamos otra vez regla de la cadena, pero con respecto a la capa \\(l\\) (ojo: queremos obtener una fórmula recursiva!):\n\\[\\frac{\\partial D}{\\partial a_j^{(l)}}= \\sum_{s=1}^{n_{l+1}}\n\\frac{\\partial D}{\\partial a_s^{(l+1)}}\\frac{\\partial  a_s^{(l+1)}}{\\partial a_j^{(l)}},\\]\nque se puede entender a partir de este diagrama:\n\n\n\n\n\n\n\n\n\nNótese que la suma empieza en \\(s=1\\), no en \\(s=0\\), pues \\(a_0^{(l+1)}\\) no depende de \\(a_k^{(l)}\\).\nEn este caso los elementos de la suma no se anulan necesariamente. Primero consideramos la derivada de:\n\\[\\frac{\\partial  a_s^{(l+1)}}{\\partial a_j^{(l)}}=h'(z_s^{(l+1)})\\theta_{s,j}^{(l)},\\]\nde modo que\n\\[\\frac{\\partial D}{\\partial a_j^{(l)}}= \\sum_{s=1}^{n_l}\n\\frac{\\partial D}{\\partial a_s^{(l+1)}} h'(z_s^{(l+1)})\\theta_{s,j}^{(l)}.\\]\nNótese que esto nos da una fórmula recursiva para las parciales que nos falta calcular (de \\(D\\) con respecto a \\(a\\)), pues las otras cantidades las conocemos por backpropagation.\n\n\nPaso 3: Simplificación de la recursión\nDefinimos para \\(l=1,\\ldots,L-2\\):\n\\[\n\\delta_s^{ (l+1)}=\\frac{\\partial D}{\\partial a_s^{(l+1)}} h'(z_s^{(l+1)})\n\\tag{6.2}\\]\nde manera que la ecuación recursiva es\n\\[\n\\frac{\\partial D}{\\partial a_j^{(l)}} = \\sum_{s=1}^{n_{l+1}}\n\\delta_s^{(l+1)}\\theta_{s,j}^{(l)}.\n\\tag{6.3}\\]\nTenemos que si \\(l=2,\\ldots,L-1\\), entonces podemos escribir (usando (Ecuación 6.3)) como fórmula recursiva:\n\\[\n\\delta_j^{(l)}\n= \\left (\\sum_{s=1}^{n_l} \\delta_s^{(l+1)} \\theta_{s,j}^{(l)}\\right ) h'(z_j^{(l)}),\n\\tag{6.4}\\] para \\(j=1,2,\\ldots, n_{l}\\).\n\n\nPaso 4: Condiciones iniciales\nPara la última capa, tenemos que (en la ecuación de arriba, en este caso la activación \\(h\\) es \\(h(z)=z\\) para la última capa):\n\\[\\delta_1^{(L)}=-(y - f(x)).\\]\nNótese que esta cantidad indica hacia dónde tenemos que mover \\(f(x)\\) para hacer el error más chico.\n\n\nPaso 5: Cálculo de parciales\nFinalmente, usando (Ecuación 6.1) y (Ecuación 6.2) , obtenemos \\[\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}} = \\delta_j^{(l+1)}a_k^{(l)},\\]\ny con esto ya podemos hacer backpropagation para calcular el gradiente sobre cada caso de entrenamiento, y solo resta acumular para obtener el gradiente sobre la muestra de entrenamiento.\nMuchas veces es útil escribir una versión vectorizada (importante para implementar):\n\n\nPaso 6: Versión matricial\nAhora podemos escribir estas ecuaciones en forma vectorial. En primer lugar, \\[\\delta^{(L)}=f(x)-y.\\] Y además se puede ver de la ecuación (Ecuación 6.4) que (\\(\\Theta_{*}^{(l+1)}\\) denota la matriz de pesos sin la columna correspondiente al sesgo):\n\\[\n\\delta^{(l)}=\\left( \\Theta_{*}^{(l)}    \\right)^t\\delta^{(l+1)} \\circ h'(z^{(l)})\n\\tag{6.5}\\]\ndonde \\(\\circ\\) denota el producto componente a componente.\nAhora todo ya está calculado. Lo interesante es que las \\(\\delta^{(l)}\\) se calculan de manera recursiva.\n\n\n\nAlgoritmo de backpropagation\n\n\n\n\n\n\nNota\n\n\n\n#Backpropagation\nPara problema de clasificación con regularización $ $. Para \\(i=1,\\ldots, N,\\) tomamos el dato de entrenamiento \\((x^{(i)}, y^{(i)})\\) y hacemos:\n\nPonemos \\(a^{(1)}=x^{(i)}\\) (vector de entradas, incluyendo 1).\nCalculamos \\(a^{(2)},a^{(3)},\\ldots, a^{(L)}\\) usando feed forward para la entrada \\(x^{(i)}.\\)\nCalculamos \\(\\delta^{(L)}=a^{(L)}-y^{(i)}\\), y luego \\(\\delta^{(L-1)},\\ldots, \\delta^{(2)}\\) según la recursión (Ecuación 6.4).\nAcumulamos \\(\\Delta_{j,k}^{(l)}=\\Delta_{j,k}^{(l)} + \\delta_j^{(l+1)}a_k^{(l)}\\).\nFinalmente, ponemos, si \\(k\\neq 0\\), \\[D_{j,k}^{(l)} = \\frac{2}{N}\\Delta_{j,k}^{(l)} + 2\\lambda\\theta_{j,k}^{(l)}\\] y si \\(k=0\\), \\[D_{j,k}^{(l)} = \\frac{2}{N}\\Delta_{j,k}^{(l)} .\\] Entonces: \\[D_{j,k}^{(l)} =\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}}.\\]\n\nNótese que back-propagation consiste principalmente de multiplicaciones de matrices con algunas aplicaciones de \\(h\\) y acumulaciones, igual que feed-forward.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neuronales (intro)</span>"
    ]
  },
  {
    "objectID": "06-redes-neuronales-1.html#ajuste-de-parámetros-introducción",
    "href": "06-redes-neuronales-1.html#ajuste-de-parámetros-introducción",
    "title": "6  Redes neuronales (intro)",
    "section": "6.7 Ajuste de parámetros (introducción)",
    "text": "6.7 Ajuste de parámetros (introducción)\nConsideramos la versión con regularización ridge (también llamada L2) de la devianza de entrenamiento como nuestro función objetivo:\n\n\n\n\n\n\nAjuste de redes neuronales\n\n\n\nPara un problema de regresión, ajustamos los pesos \\(\\Theta^{(1)},\\Theta^{(2)},\\ldots, \\Theta^{(L)}\\) de la red intentando minimizar el error cuadrático medio (penalizado) sobre la muestra de entrenamiento: \\[D = -\\frac{1}{2n}\\sum_{i=1}^n (y^{(i)} - f(x^{i}))^2= + \\lambda \\sum_{l=2}^{L} \\sum_{k=1}^{n_{l-1}} \\sum_{j=1}^{n_l}(\\theta_{j,k}^{(l)})^2.\\] Este problema en general no es convexo y puede tener múltiples mínimos.\n\n\nVeremos el proceso de ajuste, selección de arquitectura, etc. más adelante. Por el momento hacemos unas observaciones acerca de este problema de minimización:\n\nHay varios algoritmos para minimizar este error, algunos avanzados incluyendo información de segundo orden (como Newton), pero actualmente las técnicas más populares, para redes grandes, están derivadas de descenso en gradiente. Más específicamente, una variación, que es descenso estocástico.\nQue el algoritmo depende principalmente de multiplicaciones de matrices y acumulaciones implica que puede escalarse de diversas maneras. Una es paralelizando sobre la muestra de entrenamiento (y calcular acumulados al final), pero también se pueden paralelizar las multiplicaciones de matrices (para lo cual los GPUs se prestan muy bien).\nPara redes neuronales, el gradiente se calcula con un algoritmo que se llama back-propagation, que es una aplicación de la regla de la cadena para propagar errores desde la capa de salida a lo largo de todas las capas para ajustar los pesos y sesgos.\nEn estos problemas no buscamos el mínimo global, sino un mínimo local de buen desempeño. Puede haber múltiples mínimos, puntos silla, regiones relativamente planas, precipicios (curvatura alta). Nótese que la simetría implica que podemos obtener la misma red cambiando pesos entre neuronas y las conexiones correspondientes. Esto implica que necesariamente hay varios mínimos.\nTodo esto dificulta el entrenamiento de redes neuronales grandes. Para redes grandes, ni siquiera esperamos a alcanzar un mínimo local, sino que nos a veces detenemos prematuramente cuando obtenemos el mejor desempeño posible.\nPara este problema, no tiene sentido comenzar las iteraciones con todos los pesos igual a cero, pues las unidades de la red son simétricas: no hay nada que distinga una de otra si todos los pesos son iguales. Esto quiere decir que si iteramos, todas las neuronas van a aprender lo mismo.\nEs importante no comenzar valores de los pesos grandes, pues las funciones logísticas pueden quedar en regiones planas donde la minimización es lenta, o podemos tener gradientes demasiado grandes y produzcan inestabilidad en el cálculo del gradiente.\nEl ajuste de la tasa de aprendizaje es un parámetro importante, más delicado que para problemas convexos. Generalmente lo tratamos con un hiperparámetro más que hay que afinar. Tasas demasiado grandes pueden llevarnos a mínimos locales relativamente malos.\nGeneralmente los pesos se inicializan al azar con variables independientes gaussianas o uniformes centradas en cero, y con varianza chica (por ejemplo \\(U(-0.5,0.5)\\)). Una recomendación es usar \\(U(-1/\\sqrt{m}, 1/\\sqrt{m})\\) donde \\(m\\) es el número de entradas. En general, hay que experimentar con este parámetro.\n\nEl proceso para ajustar una red es entonces:\n\nDefinir número de capas ocultas, número de neuronas por cada capa, y un valor del parámetro de regularización. Estandarizar las entradas. Usualmente podemos probar comenzar con una o dos capas ocultas, de tamaño proporcional al número de entradas. Es buena idea comenzar con una red relativamente grande que tienen error bajo de entrenamiento aunque sobreajuste, y después regularizar y refinar su tamaño.\nSeleccionar parámetros al azar para \\(\\Theta^{(2)},\\Theta^{(3)},\\ldots, \\Theta^{(L)}\\). Se toman, por ejemplo, normales con media 0 y varianza chica.\nCorrer un algoritmo de minimización del error mostrada arriba. Es necesario experimentar con los parámetros del algoritmo de minimización.\nVerificar convergencia del algoritmo a un mínimo local (o el algoritmo no está mejorando).\nPredecir usando el modelo ajustado.\n\nFinalmente, podemos probar distintas arquitecturas y valores del parámetros de regularización, para afinar estos parámetros según validación cruzada o una muestra de validación.\n\nEjemplo (regresión)\n\ndat_grasa &lt;- read_csv(file = '../datos/bodyfat.csv') \nset.seed(183)\ngrasa_particion &lt;- initial_split(dat_grasa, 0.5)\ngrasa_ent &lt;- training(grasa_particion)\ngrasa_pr &lt;- testing(grasa_particion)\nnrow(grasa_ent)\n\n[1] 126\n\n\nUna exploración de este conjunto de datos revela algunos datos sospechosos. En particular un individuo con estatura de 30 pulgadas (alrededor de 75 cm), con peso normal. Probablemente no queremos incluir en entrenamiento este caso, y tampoco hacer predicciones para posibles personas que tengan tales dimensiones:\n\nlibrary(patchwork)\ng_1 &lt;- grasa_ent |&gt; ggplot(aes(x = estatura, y = peso)) + \n  geom_point()\ng_2 &lt;- grasa_ent |&gt; ggplot(aes(x = abdomen, y = peso)) + \n  geom_point()\ng_1 + g_2\n\n\n\n\n\n\n\n\n\ngrasa_receta &lt;- recipe(grasacorp ~ ., grasa_ent) |&gt; \n  step_filter(estatura &gt; 50) |&gt;\n  step_normalize(all_predictors()) |&gt; \n  prep()\n\n\nlibrary(keras3)\n# entrenamiento\nx_grasa &lt;- grasa_receta |&gt; juice() |&gt; \n  select(-grasacorp) |&gt; as.matrix()\nvars_nombres &lt;- colnames(x_grasa)\ny_grasa &lt;- grasa_receta |&gt; juice() |&gt; pull(grasacorp)\n# validación\nx_grasa_pr &lt;- grasa_receta |&gt; bake(grasa_pr) |&gt; \n  select(-grasacorp) |&gt; as.matrix()\ny_grasa_pr &lt;- grasa_receta |&gt; bake(grasa_pr) |&gt; pull(grasacorp)\n\n\nmodelo_red &lt;- keras_model_sequential() |&gt; \n  layer_dense(units = 50, activation = \"sigmoid\") |&gt; \n  layer_dense(units = 50, activation = \"sigmoid\") |&gt; \n  layer_dense(units = 1, activation = \"linear\")\nmodelo_red |&gt; compile(\n  loss = \"mse\", metrics = \"mse\",\n  optimizer = optimizer_sgd(learning_rate = 0.01, momentum = 0.9)\n)\n# esto es más eficiente hacerlo con callbacks en general:\nhistoria &lt;- modelo_red |&gt; fit(\n  x = x_grasa, y = y_grasa,\n  validation_data = list(x_grasa_pr, y_grasa_pr),\n  batch_size = 30, epochs = 250, verbose = 1)\n\nEpoch 1/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 491ms/step - loss: 371.2946 - mse: 371.2946\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 260.0376 - mse: 260.0376 - val_loss: 83.4129 - val_mse: 83.4129\nEpoch 2/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 73.2279 - mse: 73.2279\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 103.6785 - mse: 103.6785 - val_loss: 51.5754 - val_mse: 51.5754\nEpoch 3/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 41.0798 - mse: 41.0798\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 55.1550 - mse: 55.1550 - val_loss: 53.2117 - val_mse: 53.2117\nEpoch 4/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 41.8359 - mse: 41.8359\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 46.8067 - mse: 46.8067 - val_loss: 50.5936 - val_mse: 50.5936\nEpoch 5/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 56.5675 - mse: 56.5675\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 46.7040 - mse: 46.7040 - val_loss: 56.8771 - val_mse: 56.8771\nEpoch 6/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 54.3354 - mse: 54.3354\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 43.0142 - mse: 43.0142 - val_loss: 35.8100 - val_mse: 35.8100\nEpoch 7/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 33.8200 - mse: 33.8200\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 33.3196 - mse: 33.3196 - val_loss: 33.6038 - val_mse: 33.6038\nEpoch 8/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 32.1236 - mse: 32.1236\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 26.7658 - mse: 26.7658 - val_loss: 30.2307 - val_mse: 30.2307\nEpoch 9/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 30.8269 - mse: 30.8269\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.9625 - mse: 24.9625 - val_loss: 27.0051 - val_mse: 27.0051\nEpoch 10/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.7748 - mse: 24.7748\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.2964 - mse: 22.2964 - val_loss: 25.9309 - val_mse: 25.9309\nEpoch 11/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1663 - mse: 20.1663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.3947 - mse: 19.3947 - val_loss: 26.2502 - val_mse: 26.2502\nEpoch 12/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.1804 - mse: 18.1804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 18.8185 - mse: 18.8185 - val_loss: 27.0289 - val_mse: 27.0289\nEpoch 13/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 17.6545 - mse: 17.6545\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 18.1798 - mse: 18.1798 - val_loss: 26.7192 - val_mse: 26.7192\nEpoch 14/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 17.1457 - mse: 17.1457\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 17.6890 - mse: 17.6890 - val_loss: 26.1269 - val_mse: 26.1269\nEpoch 15/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 16.7010 - mse: 16.7010\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 17.2119 - mse: 17.2119 - val_loss: 26.0213 - val_mse: 26.0213\nEpoch 16/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 16.8253 - mse: 16.8253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.8784 - mse: 16.8784 - val_loss: 25.4968 - val_mse: 25.4968\nEpoch 17/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 16.3074 - mse: 16.3074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.5263 - mse: 16.5263 - val_loss: 25.4788 - val_mse: 25.4788\nEpoch 18/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 16.2110 - mse: 16.2110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.1264 - mse: 16.1264 - val_loss: 25.5155 - val_mse: 25.5155\nEpoch 19/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16.0603 - mse: 16.0603\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15.9332 - mse: 15.9332 - val_loss: 25.4657 - val_mse: 25.4657\nEpoch 20/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 15.8355 - mse: 15.8355\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15.6467 - mse: 15.6467 - val_loss: 25.4096 - val_mse: 25.4096\nEpoch 21/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 15.6199 - mse: 15.6199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15.3785 - mse: 15.3785 - val_loss: 25.3749 - val_mse: 25.3749\nEpoch 22/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 15.3480 - mse: 15.3480\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15.1411 - mse: 15.1411 - val_loss: 25.4214 - val_mse: 25.4214\nEpoch 23/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15.2261 - mse: 15.2261\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.9327 - mse: 14.9327 - val_loss: 25.3916 - val_mse: 25.3916\nEpoch 24/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 15.0004 - mse: 15.0004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.7313 - mse: 14.7313 - val_loss: 25.4258 - val_mse: 25.4258\nEpoch 25/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 14.8402 - mse: 14.8402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.5366 - mse: 14.5366 - val_loss: 25.4543 - val_mse: 25.4543\nEpoch 26/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.6551 - mse: 14.6551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.3569 - mse: 14.3569 - val_loss: 25.4696 - val_mse: 25.4696\nEpoch 27/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.4531 - mse: 14.4531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.1735 - mse: 14.1735 - val_loss: 25.4990 - val_mse: 25.4990\nEpoch 28/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.2833 - mse: 14.2833\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.0110 - mse: 14.0110 - val_loss: 25.5137 - val_mse: 25.5137\nEpoch 29/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 14.1111 - mse: 14.1111\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.8558 - mse: 13.8558 - val_loss: 25.5225 - val_mse: 25.5225\nEpoch 30/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.9562 - mse: 13.9562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.7030 - mse: 13.7030 - val_loss: 25.5185 - val_mse: 25.5185\nEpoch 31/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.7999 - mse: 13.7999\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.5542 - mse: 13.5542 - val_loss: 25.5181 - val_mse: 25.5181\nEpoch 32/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.6616 - mse: 13.6616\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.4124 - mse: 13.4124 - val_loss: 25.5079 - val_mse: 25.5079\nEpoch 33/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.5243 - mse: 13.5243\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.2750 - mse: 13.2750 - val_loss: 25.4956 - val_mse: 25.4956\nEpoch 34/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.3948 - mse: 13.3948\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.1402 - mse: 13.1402 - val_loss: 25.4800 - val_mse: 25.4800\nEpoch 35/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.2716 - mse: 13.2716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.0079 - mse: 13.0079 - val_loss: 25.4574 - val_mse: 25.4574\nEpoch 36/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.1484 - mse: 13.1484\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.8759 - mse: 12.8759 - val_loss: 25.4333 - val_mse: 25.4333\nEpoch 37/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.0293 - mse: 13.0293\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.7459 - mse: 12.7459 - val_loss: 25.4078 - val_mse: 25.4078\nEpoch 38/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.9087 - mse: 12.9087\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.6171 - mse: 12.6171 - val_loss: 25.3841 - val_mse: 25.3841\nEpoch 39/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.7864 - mse: 12.7864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.4881 - mse: 12.4881 - val_loss: 25.3629 - val_mse: 25.3629\nEpoch 40/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.6622 - mse: 12.6622\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.3589 - mse: 12.3589 - val_loss: 25.3445 - val_mse: 25.3445\nEpoch 41/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.5383 - mse: 12.5383\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.2300 - mse: 12.2300 - val_loss: 25.3274 - val_mse: 25.3274\nEpoch 42/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.4155 - mse: 12.4155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 12.1015 - mse: 12.1015 - val_loss: 25.3122 - val_mse: 25.3122\nEpoch 43/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.2951 - mse: 12.2951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.9738 - mse: 11.9738 - val_loss: 25.2983 - val_mse: 25.2983\nEpoch 44/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.1767 - mse: 12.1767\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.8465 - mse: 11.8465 - val_loss: 25.2853 - val_mse: 25.2853\nEpoch 45/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12.0599 - mse: 12.0599\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.7193 - mse: 11.7193 - val_loss: 25.2734 - val_mse: 25.2734\nEpoch 46/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.9452 - mse: 11.9452\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.5922 - mse: 11.5922 - val_loss: 25.2622 - val_mse: 25.2622\nEpoch 47/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.8326 - mse: 11.8326\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.4650 - mse: 11.4650 - val_loss: 25.2522 - val_mse: 25.2522\nEpoch 48/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.7225 - mse: 11.7225\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.3375 - mse: 11.3375 - val_loss: 25.2436 - val_mse: 25.2436\nEpoch 49/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.6147 - mse: 11.6147\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.2097 - mse: 11.2097 - val_loss: 25.2368 - val_mse: 25.2368\nEpoch 50/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.5093 - mse: 11.5093\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.0816 - mse: 11.0816 - val_loss: 25.2312 - val_mse: 25.2312\nEpoch 51/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.4053 - mse: 11.4053\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.9531 - mse: 10.9531 - val_loss: 25.2249 - val_mse: 25.2249\nEpoch 52/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.3010 - mse: 11.3010\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.8235 - mse: 10.8235 - val_loss: 25.2173 - val_mse: 25.2173\nEpoch 53/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.1964 - mse: 11.1964\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.6923 - mse: 10.6923 - val_loss: 25.2096 - val_mse: 25.2096\nEpoch 54/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.0926 - mse: 11.0926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.5591 - mse: 10.5591 - val_loss: 25.2037 - val_mse: 25.2037\nEpoch 55/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.9902 - mse: 10.9902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.4237 - mse: 10.4237 - val_loss: 25.2016 - val_mse: 25.2016\nEpoch 56/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.8890 - mse: 10.8890\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.2858 - mse: 10.2858 - val_loss: 25.2049 - val_mse: 25.2049\nEpoch 57/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.7887 - mse: 10.7887\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.1455 - mse: 10.1455 - val_loss: 25.2148 - val_mse: 25.2148\nEpoch 58/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.6891 - mse: 10.6891\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.0035 - mse: 10.0035 - val_loss: 25.2323 - val_mse: 25.2323\nEpoch 59/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.5901 - mse: 10.5901\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.8602 - mse: 9.8602 - val_loss: 25.2581 - val_mse: 25.2581\nEpoch 60/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.4915 - mse: 10.4915\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.7160 - mse: 9.7160 - val_loss: 25.2930 - val_mse: 25.2930\nEpoch 61/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.3935 - mse: 10.3935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.5714 - mse: 9.5714 - val_loss: 25.3376 - val_mse: 25.3376\nEpoch 62/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 10.2959 - mse: 10.2959\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.4265 - mse: 9.4265 - val_loss: 25.3921 - val_mse: 25.3921\nEpoch 63/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.1986 - mse: 10.1986\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.2815 - mse: 9.2815 - val_loss: 25.4568 - val_mse: 25.4568\nEpoch 64/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.1015 - mse: 10.1015\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.1366 - mse: 9.1366 - val_loss: 25.5315 - val_mse: 25.5315\nEpoch 65/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10.0041 - mse: 10.0041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.9919 - mse: 8.9919 - val_loss: 25.6160 - val_mse: 25.6160\nEpoch 66/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.9061 - mse: 9.9061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.8476 - mse: 8.8476 - val_loss: 25.7101 - val_mse: 25.7101\nEpoch 67/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.8068 - mse: 9.8068\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.7040 - mse: 8.7040 - val_loss: 25.8136 - val_mse: 25.8136\nEpoch 68/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.7055 - mse: 9.7055\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.5614 - mse: 8.5614 - val_loss: 25.9260 - val_mse: 25.9260\nEpoch 69/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.6014 - mse: 9.6014\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.4199 - mse: 8.4199 - val_loss: 26.0465 - val_mse: 26.0465\nEpoch 70/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.4937 - mse: 9.4937\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.2796 - mse: 8.2796 - val_loss: 26.1738 - val_mse: 26.1738\nEpoch 71/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.3818 - mse: 9.3818\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1404 - mse: 8.1404 - val_loss: 26.3064 - val_mse: 26.3064\nEpoch 72/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 9.2655 - mse: 9.2655\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.0023 - mse: 8.0023 - val_loss: 26.4426 - val_mse: 26.4426\nEpoch 73/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.1449 - mse: 9.1449\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8655 - mse: 7.8655 - val_loss: 26.5807 - val_mse: 26.5807\nEpoch 74/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.0205 - mse: 9.0205\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7300 - mse: 7.7300 - val_loss: 26.7193 - val_mse: 26.7193\nEpoch 75/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.8930 - mse: 8.8930\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.5960 - mse: 7.5960 - val_loss: 26.8575 - val_mse: 26.8575\nEpoch 76/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.7629 - mse: 8.7629\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.4635 - mse: 7.4635 - val_loss: 26.9940 - val_mse: 26.9940\nEpoch 77/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 8.6310 - mse: 8.6310\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.3327 - mse: 7.3327 - val_loss: 27.1281 - val_mse: 27.1281\nEpoch 78/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.4976 - mse: 8.4976\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.2037 - mse: 7.2037 - val_loss: 27.2586 - val_mse: 27.2586\nEpoch 79/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.3633 - mse: 8.3633\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.0764 - mse: 7.0764 - val_loss: 27.3850 - val_mse: 27.3850\nEpoch 80/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.2285 - mse: 8.2285\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9508 - mse: 6.9508 - val_loss: 27.5064 - val_mse: 27.5064\nEpoch 81/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.0934 - mse: 8.0934\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.8269 - mse: 6.8269 - val_loss: 27.6225 - val_mse: 27.6225\nEpoch 82/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.9587 - mse: 7.9587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7046 - mse: 6.7046 - val_loss: 27.7328 - val_mse: 27.7328\nEpoch 83/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.8245 - mse: 7.8245\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.5837 - mse: 6.5837 - val_loss: 27.8372 - val_mse: 27.8372\nEpoch 84/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.6913 - mse: 7.6913\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4642 - mse: 6.4642 - val_loss: 27.9356 - val_mse: 27.9356\nEpoch 85/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.5594 - mse: 7.5594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.3460 - mse: 6.3460 - val_loss: 28.0283 - val_mse: 28.0283\nEpoch 86/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.4291 - mse: 7.4291\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2290 - mse: 6.2290 - val_loss: 28.1155 - val_mse: 28.1155\nEpoch 87/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.3006 - mse: 7.3006\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.1130 - mse: 6.1130 - val_loss: 28.1981 - val_mse: 28.1981\nEpoch 88/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.1742 - mse: 7.1742\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9981 - mse: 5.9981 - val_loss: 28.2764 - val_mse: 28.2764\nEpoch 89/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.0499 - mse: 7.0499\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8842 - mse: 5.8842 - val_loss: 28.3519 - val_mse: 28.3519\nEpoch 90/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.9280 - mse: 6.9280\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7713 - mse: 5.7713 - val_loss: 28.4250 - val_mse: 28.4250\nEpoch 91/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.8083 - mse: 6.8083\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6596 - mse: 5.6596 - val_loss: 28.4977 - val_mse: 28.4977\nEpoch 92/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.6911 - mse: 6.6911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5493 - mse: 5.5493 - val_loss: 28.5703 - val_mse: 28.5703\nEpoch 93/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.5759 - mse: 6.5759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4406 - mse: 5.4406 - val_loss: 28.6452 - val_mse: 28.6452\nEpoch 94/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.4630 - mse: 6.4630\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3339 - mse: 5.3339 - val_loss: 28.7219 - val_mse: 28.7219\nEpoch 95/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.3516 - mse: 6.3516\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2295 - mse: 5.2295 - val_loss: 28.8028 - val_mse: 28.8028\nEpoch 96/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.2426 - mse: 6.2426\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.1278 - mse: 5.1278 - val_loss: 28.8857 - val_mse: 28.8857\nEpoch 97/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.1347 - mse: 6.1347\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0288 - mse: 5.0288 - val_loss: 28.9738 - val_mse: 28.9738\nEpoch 98/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6.0298 - mse: 6.0298\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9332 - mse: 4.9332 - val_loss: 29.0619 - val_mse: 29.0619\nEpoch 99/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.9257 - mse: 5.9257\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8404 - mse: 4.8404 - val_loss: 29.1553 - val_mse: 29.1553\nEpoch 100/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.8255 - mse: 5.8255\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7513 - mse: 4.7513 - val_loss: 29.2451 - val_mse: 29.2451\nEpoch 101/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.7257 - mse: 5.7257\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6647 - mse: 4.6647 - val_loss: 29.3410 - val_mse: 29.3410\nEpoch 102/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.6313 - mse: 5.6313\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5820 - mse: 4.5820 - val_loss: 29.4282 - val_mse: 29.4282\nEpoch 103/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.5362 - mse: 5.5362\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5013 - mse: 4.5013 - val_loss: 29.5243 - val_mse: 29.5243\nEpoch 104/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.4484 - mse: 5.4484\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4246 - mse: 4.4246 - val_loss: 29.6045 - val_mse: 29.6045\nEpoch 105/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.3578 - mse: 5.3578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3492 - mse: 4.3492 - val_loss: 29.7001 - val_mse: 29.7001\nEpoch 106/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.2774 - mse: 5.2774\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2780 - mse: 4.2780 - val_loss: 29.7689 - val_mse: 29.7689\nEpoch 107/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.1904 - mse: 5.1904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2069 - mse: 4.2069 - val_loss: 29.8650 - val_mse: 29.8650\nEpoch 108/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.1181 - mse: 5.1181\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1410 - mse: 4.1410 - val_loss: 29.9179 - val_mse: 29.9179\nEpoch 109/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.0332 - mse: 5.0332\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0735 - mse: 4.0735 - val_loss: 30.0180 - val_mse: 30.0180\nEpoch 110/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.9701 - mse: 4.9701\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0126 - mse: 4.0126 - val_loss: 30.0495 - val_mse: 30.0495\nEpoch 111/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.8851 - mse: 4.8851\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9478 - mse: 3.9478 - val_loss: 30.1603 - val_mse: 30.1603\nEpoch 112/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8327 - mse: 4.8327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8918 - mse: 3.8918 - val_loss: 30.1635 - val_mse: 30.1635\nEpoch 113/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7448 - mse: 4.7448\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8289 - mse: 3.8289 - val_loss: 30.2941 - val_mse: 30.2941\nEpoch 114/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.7054 - mse: 4.7054\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7781 - mse: 3.7781 - val_loss: 30.2600 - val_mse: 30.2600\nEpoch 115/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.6112 - mse: 4.6112\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7161 - mse: 3.7161 - val_loss: 30.4226 - val_mse: 30.4226\nEpoch 116/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.5879 - mse: 4.5879\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6711 - mse: 3.6711 - val_loss: 30.3396 - val_mse: 30.3396\nEpoch 117/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.4835 - mse: 4.4835\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6089 - mse: 3.6089 - val_loss: 30.5488 - val_mse: 30.5488\nEpoch 118/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.4802 - mse: 4.4802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.5706 - mse: 3.5706 - val_loss: 30.4035 - val_mse: 30.4035\nEpoch 119/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.3613 - mse: 4.3613\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5074 - mse: 3.5074 - val_loss: 30.6750 - val_mse: 30.6750\nEpoch 120/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.3821 - mse: 4.3821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4767 - mse: 3.4767 - val_loss: 30.4537 - val_mse: 30.4537\nEpoch 121/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2448 - mse: 4.2448\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4114 - mse: 3.4114 - val_loss: 30.8017 - val_mse: 30.8017\nEpoch 122/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2936 - mse: 4.2936\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3894 - mse: 3.3894 - val_loss: 30.4937 - val_mse: 30.4937\nEpoch 123/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.1345 - mse: 4.1345\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3210 - mse: 3.3210 - val_loss: 30.9277 - val_mse: 30.9277\nEpoch 124/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2135 - mse: 4.2135\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3084 - mse: 3.3084 - val_loss: 30.5276 - val_mse: 30.5276\nEpoch 125/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.0310 - mse: 4.0310\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2359 - mse: 3.2359 - val_loss: 31.0486 - val_mse: 31.0486\nEpoch 126/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.1395 - mse: 4.1395\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2326 - mse: 3.2326 - val_loss: 30.5602 - val_mse: 30.5602\nEpoch 127/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9342 - mse: 3.9342\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1552 - mse: 3.1552 - val_loss: 31.1575 - val_mse: 31.1575\nEpoch 128/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.0679 - mse: 4.0679\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1597 - mse: 3.1597 - val_loss: 30.5962 - val_mse: 30.5962\nEpoch 129/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8433 - mse: 3.8433\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0770 - mse: 3.0770 - val_loss: 31.2462 - val_mse: 31.2462\nEpoch 130/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9938 - mse: 3.9938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0868 - mse: 3.0868 - val_loss: 30.6394 - val_mse: 30.6394\nEpoch 131/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.7570 - mse: 3.7570\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9994 - mse: 2.9994 - val_loss: 31.3070 - val_mse: 31.3070\nEpoch 132/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.9127 - mse: 3.9127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0110 - mse: 3.0110 - val_loss: 30.6931 - val_mse: 30.6931\nEpoch 133/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.6745 - mse: 3.6745\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9209 - mse: 2.9209 - val_loss: 31.3350 - val_mse: 31.3350\nEpoch 134/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.8215 - mse: 3.8215\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9306 - mse: 2.9306 - val_loss: 30.7600 - val_mse: 30.7600\nEpoch 135/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.5957 - mse: 3.5957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8413 - mse: 2.8413 - val_loss: 31.3303 - val_mse: 31.3303\nEpoch 136/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.7201 - mse: 3.7201\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8460 - mse: 2.8460 - val_loss: 30.8421 - val_mse: 30.8421\nEpoch 137/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.5218 - mse: 3.5218\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7621 - mse: 2.7621 - val_loss: 31.3000 - val_mse: 31.3000\nEpoch 138/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.6122 - mse: 3.6122\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7597 - mse: 2.7597 - val_loss: 30.9383 - val_mse: 30.9383\nEpoch 139/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.4542 - mse: 3.4542\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6863 - mse: 2.6863 - val_loss: 31.2589 - val_mse: 31.2589\nEpoch 140/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.5045 - mse: 3.5045\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6758 - mse: 2.6758 - val_loss: 31.0420 - val_mse: 31.0420\nEpoch 141/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.3932 - mse: 3.3932\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6164 - mse: 2.6164 - val_loss: 31.2250 - val_mse: 31.2250\nEpoch 142/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.4036 - mse: 3.4036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5976 - mse: 2.5976 - val_loss: 31.1402 - val_mse: 31.1402\nEpoch 143/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.3349 - mse: 3.3349\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5522 - mse: 2.5522 - val_loss: 31.2127 - val_mse: 31.2127\nEpoch 144/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.3129 - mse: 3.3129\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5255 - mse: 2.5255 - val_loss: 31.2197 - val_mse: 31.2197\nEpoch 145/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.2729 - mse: 3.2729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4902 - mse: 2.4902 - val_loss: 31.2278 - val_mse: 31.2278\nEpoch 146/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.2319 - mse: 3.2319\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4582 - mse: 2.4582 - val_loss: 31.2748 - val_mse: 31.2748\nEpoch 147/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.2031 - mse: 3.2031\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4278 - mse: 2.4278 - val_loss: 31.2668 - val_mse: 31.2668\nEpoch 148/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.1573 - mse: 3.1573\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3938 - mse: 2.3938 - val_loss: 31.3106 - val_mse: 31.3106\nEpoch 149/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.1272 - mse: 3.1272\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3647 - mse: 2.3647 - val_loss: 31.3181 - val_mse: 31.3181\nEpoch 150/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.0849 - mse: 3.0849\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3311 - mse: 2.3311 - val_loss: 31.3422 - val_mse: 31.3422\nEpoch 151/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.0491 - mse: 3.0491\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3014 - mse: 2.3014 - val_loss: 31.3654 - val_mse: 31.3654\nEpoch 152/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.0113 - mse: 3.0113\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2699 - mse: 2.2699 - val_loss: 31.3830 - val_mse: 31.3830\nEpoch 153/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.9711 - mse: 2.9711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2380 - mse: 2.2380 - val_loss: 31.4049 - val_mse: 31.4049\nEpoch 154/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9354 - mse: 2.9354\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2091 - mse: 2.2091 - val_loss: 31.4281 - val_mse: 31.4281\nEpoch 155/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.8945 - mse: 2.8945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1766 - mse: 2.1766 - val_loss: 31.4520 - val_mse: 31.4520\nEpoch 156/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.8572 - mse: 2.8572\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1465 - mse: 2.1465 - val_loss: 31.4679 - val_mse: 31.4679\nEpoch 157/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.8199 - mse: 2.8199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1186 - mse: 2.1186 - val_loss: 31.5071 - val_mse: 31.5071\nEpoch 158/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.7797 - mse: 2.7797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0855 - mse: 2.0855 - val_loss: 31.5239 - val_mse: 31.5239\nEpoch 159/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.7455 - mse: 2.7455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0590 - mse: 2.0590 - val_loss: 31.5430 - val_mse: 31.5430\nEpoch 160/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.7074 - mse: 2.7074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0321 - mse: 2.0321 - val_loss: 31.6072 - val_mse: 31.6072\nEpoch 161/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.6706 - mse: 2.6706\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9984 - mse: 1.9984 - val_loss: 31.5955 - val_mse: 31.5955\nEpoch 162/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.6437 - mse: 2.6437\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9812 - mse: 1.9812 - val_loss: 31.6374 - val_mse: 31.6374\nEpoch 163/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5999 - mse: 2.5999\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9514 - mse: 1.9514 - val_loss: 31.7232 - val_mse: 31.7232\nEpoch 164/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5697 - mse: 2.5697\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9184 - mse: 1.9184 - val_loss: 31.6554 - val_mse: 31.6554\nEpoch 165/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5688 - mse: 2.5688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9266 - mse: 1.9266 - val_loss: 31.7786 - val_mse: 31.7786\nEpoch 166/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5107 - mse: 2.5107\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8867 - mse: 1.8867 - val_loss: 31.8240 - val_mse: 31.8240\nEpoch 167/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.4746 - mse: 2.4746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8570 - mse: 1.8570 - val_loss: 31.7064 - val_mse: 31.7064\nEpoch 168/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5557 - mse: 2.5557\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9305 - mse: 1.9305 - val_loss: 32.1288 - val_mse: 32.1288\nEpoch 169/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5742 - mse: 2.5742\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9222 - mse: 1.9222 - val_loss: 31.8624 - val_mse: 31.8624\nEpoch 170/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.4434 - mse: 2.4434\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9090 - mse: 1.9090 - val_loss: 31.7564 - val_mse: 31.7564\nEpoch 171/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5870 - mse: 2.5870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0801 - mse: 2.0801 - val_loss: 33.2385 - val_mse: 33.2385\nEpoch 172/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.3658 - mse: 3.3658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4257 - mse: 2.4257 - val_loss: 32.2732 - val_mse: 32.2732\nEpoch 173/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.2699 - mse: 3.2699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6766 - mse: 2.6766 - val_loss: 31.7297 - val_mse: 31.7297\nEpoch 174/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.4479 - mse: 2.4479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7158 - mse: 2.7158 - val_loss: 34.4486 - val_mse: 34.4486\nEpoch 175/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.7539 - mse: 4.7539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7572 - mse: 3.7572 - val_loss: 35.0701 - val_mse: 35.0701\nEpoch 176/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 7.2430 - mse: 7.2430\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7999 - mse: 5.7999 - val_loss: 33.2638 - val_mse: 33.2638\nEpoch 177/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.2644 - mse: 4.2644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3300 - mse: 5.3300 - val_loss: 31.6757 - val_mse: 31.6757\nEpoch 178/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.0253 - mse: 3.0253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1574 - mse: 4.1574 - val_loss: 33.9455 - val_mse: 33.9455\nEpoch 179/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.6193 - mse: 6.6193\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.8680 - mse: 6.8680 - val_loss: 31.3319 - val_mse: 31.3319\nEpoch 180/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.4054 - mse: 3.4054\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5976 - mse: 3.5976 - val_loss: 30.6920 - val_mse: 30.6920\nEpoch 181/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.2868 - mse: 2.2868\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2753 - mse: 2.2753 - val_loss: 32.0951 - val_mse: 32.0951\nEpoch 182/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.4324 - mse: 2.4324\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0661 - mse: 2.0661 - val_loss: 32.5811 - val_mse: 32.5811\nEpoch 183/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5116 - mse: 2.5116\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0088 - mse: 2.0088 - val_loss: 32.7618 - val_mse: 32.7618\nEpoch 184/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.4973 - mse: 2.4973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0923 - mse: 2.0923 - val_loss: 32.2091 - val_mse: 32.2091\nEpoch 185/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.1439 - mse: 2.1439\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6979 - mse: 1.6979 - val_loss: 32.4754 - val_mse: 32.4754\nEpoch 186/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.2200 - mse: 2.2200\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7013 - mse: 1.7013 - val_loss: 32.3437 - val_mse: 32.3437\nEpoch 187/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.0209 - mse: 2.0209\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6291 - mse: 1.6291 - val_loss: 32.5857 - val_mse: 32.5857\nEpoch 188/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.0217 - mse: 2.0217\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5475 - mse: 1.5475 - val_loss: 32.3496 - val_mse: 32.3496\nEpoch 189/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.9683 - mse: 1.9683\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5413 - mse: 1.5413 - val_loss: 32.5944 - val_mse: 32.5944\nEpoch 190/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.9849 - mse: 1.9849\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5324 - mse: 1.5324 - val_loss: 32.2206 - val_mse: 32.2206\nEpoch 191/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.8433 - mse: 1.8433\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4651 - mse: 1.4651 - val_loss: 32.6225 - val_mse: 32.6225\nEpoch 192/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9230 - mse: 1.9230\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4739 - mse: 1.4739 - val_loss: 32.3223 - val_mse: 32.3223\nEpoch 193/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.7867 - mse: 1.7867\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4270 - mse: 1.4270 - val_loss: 32.7145 - val_mse: 32.7145\nEpoch 194/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.8679 - mse: 1.8679\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4282 - mse: 1.4282 - val_loss: 32.3315 - val_mse: 32.3315\nEpoch 195/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.7696 - mse: 1.7696\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4046 - mse: 1.4046 - val_loss: 32.7447 - val_mse: 32.7447\nEpoch 196/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.8543 - mse: 1.8543\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4174 - mse: 1.4174 - val_loss: 32.3194 - val_mse: 32.3194\nEpoch 197/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.7210 - mse: 1.7210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3702 - mse: 1.3702 - val_loss: 32.7254 - val_mse: 32.7254\nEpoch 198/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.7958 - mse: 1.7958\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3757 - mse: 1.3757 - val_loss: 32.3492 - val_mse: 32.3492\nEpoch 199/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.6688 - mse: 1.6688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3290 - mse: 1.3290 - val_loss: 32.7091 - val_mse: 32.7091\nEpoch 200/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.7449 - mse: 1.7449\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3373 - mse: 1.3373 - val_loss: 32.3768 - val_mse: 32.3768\nEpoch 201/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.6380 - mse: 1.6380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2997 - mse: 1.2997 - val_loss: 32.7061 - val_mse: 32.7061\nEpoch 202/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.6972 - mse: 1.6972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3044 - mse: 1.3044 - val_loss: 32.4127 - val_mse: 32.4127\nEpoch 203/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.6123 - mse: 1.6123\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2747 - mse: 1.2747 - val_loss: 32.6749 - val_mse: 32.6749\nEpoch 204/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.6491 - mse: 1.6491\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2738 - mse: 1.2738 - val_loss: 32.4679 - val_mse: 32.4679\nEpoch 205/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.5732 - mse: 1.5732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2409 - mse: 1.2409 - val_loss: 32.6359 - val_mse: 32.6359\nEpoch 206/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.5996 - mse: 1.5996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2414 - mse: 1.2414 - val_loss: 32.5214 - val_mse: 32.5214\nEpoch 207/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.5459 - mse: 1.5459\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2155 - mse: 1.2155 - val_loss: 32.6245 - val_mse: 32.6245\nEpoch 208/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.5548 - mse: 1.5548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2110 - mse: 1.2110 - val_loss: 32.5499 - val_mse: 32.5499\nEpoch 209/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.5251 - mse: 1.5251\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1969 - mse: 1.1969 - val_loss: 32.6256 - val_mse: 32.6256\nEpoch 210/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.5186 - mse: 1.5186\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1868 - mse: 1.1868 - val_loss: 32.5890 - val_mse: 32.5890\nEpoch 211/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4989 - mse: 1.4989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1748 - mse: 1.1748 - val_loss: 32.6091 - val_mse: 32.6091\nEpoch 212/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.4864 - mse: 1.4864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1656 - mse: 1.1656 - val_loss: 32.6358 - val_mse: 32.6358\nEpoch 213/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4704 - mse: 1.4704\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1514 - mse: 1.1514 - val_loss: 32.6185 - val_mse: 32.6185\nEpoch 214/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4601 - mse: 1.4601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1458 - mse: 1.1458 - val_loss: 32.6404 - val_mse: 32.6404\nEpoch 215/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4443 - mse: 1.4443\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1337 - mse: 1.1337 - val_loss: 32.6596 - val_mse: 32.6596\nEpoch 216/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4267 - mse: 1.4267\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1194 - mse: 1.1194 - val_loss: 32.6481 - val_mse: 32.6481\nEpoch 217/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4240 - mse: 1.4240\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1183 - mse: 1.1183 - val_loss: 32.6735 - val_mse: 32.6735\nEpoch 218/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4037 - mse: 1.4037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1035 - mse: 1.1035 - val_loss: 32.6968 - val_mse: 32.6968\nEpoch 219/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3888 - mse: 1.3888\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0903 - mse: 1.0903 - val_loss: 32.6717 - val_mse: 32.6717\nEpoch 220/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3874 - mse: 1.3874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0920 - mse: 1.0920 - val_loss: 32.7284 - val_mse: 32.7284\nEpoch 221/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3717 - mse: 1.3717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0777 - mse: 1.0777 - val_loss: 32.7303 - val_mse: 32.7303\nEpoch 222/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3536 - mse: 1.3536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0646 - mse: 1.0646 - val_loss: 32.7022 - val_mse: 32.7022\nEpoch 223/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3513 - mse: 1.3513\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0667 - mse: 1.0667 - val_loss: 32.7999 - val_mse: 32.7999\nEpoch 224/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3485 - mse: 1.3485\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0562 - mse: 1.0562 - val_loss: 32.7715 - val_mse: 32.7715\nEpoch 225/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3284 - mse: 1.3284\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0476 - mse: 1.0476 - val_loss: 32.7366 - val_mse: 32.7366\nEpoch 226/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3131 - mse: 1.3131\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0426 - mse: 1.0426 - val_loss: 32.8933 - val_mse: 32.8933\nEpoch 227/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3358 - mse: 1.3358\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0407 - mse: 1.0407 - val_loss: 32.8301 - val_mse: 32.8301\nEpoch 228/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3268 - mse: 1.3268\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0491 - mse: 1.0491 - val_loss: 32.7765 - val_mse: 32.7765\nEpoch 229/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.2740 - mse: 1.2740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0244 - mse: 1.0244 - val_loss: 33.0106 - val_mse: 33.0106\nEpoch 230/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3327 - mse: 1.3327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0329 - mse: 1.0329 - val_loss: 32.9242 - val_mse: 32.9242\nEpoch 231/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3769 - mse: 1.3769\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0896 - mse: 1.0896 - val_loss: 32.8403 - val_mse: 32.8403\nEpoch 232/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.2533 - mse: 1.2533\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0326 - mse: 1.0326 - val_loss: 33.1354 - val_mse: 33.1354\nEpoch 233/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3261 - mse: 1.3261\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0354 - mse: 1.0354 - val_loss: 33.0877 - val_mse: 33.0877\nEpoch 234/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.5297 - mse: 1.5297\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2088 - mse: 1.2088 - val_loss: 32.9986 - val_mse: 32.9986\nEpoch 235/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3304 - mse: 1.3304\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1326 - mse: 1.1326 - val_loss: 33.2108 - val_mse: 33.2108\nEpoch 236/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.2864 - mse: 1.2864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0578 - mse: 1.0578 - val_loss: 33.3535 - val_mse: 33.3535\nEpoch 237/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.8406 - mse: 1.8406\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4624 - mse: 1.4624 - val_loss: 33.4437 - val_mse: 33.4437\nEpoch 238/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.7128 - mse: 1.7128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4678 - mse: 1.4678 - val_loss: 33.1342 - val_mse: 33.1342\nEpoch 239/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.1994 - mse: 1.1994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1229 - mse: 1.1229 - val_loss: 33.6418 - val_mse: 33.6418\nEpoch 240/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.2260 - mse: 2.2260\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8357 - mse: 1.8357 - val_loss: 34.3064 - val_mse: 34.3064\nEpoch 241/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5367 - mse: 2.5367\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0971 - mse: 2.0971 - val_loss: 32.9611 - val_mse: 32.9611\nEpoch 242/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.1802 - mse: 1.1802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2394 - mse: 1.2394 - val_loss: 33.6337 - val_mse: 33.6337\nEpoch 243/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.3075 - mse: 2.3075\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0536 - mse: 2.0536 - val_loss: 34.8405 - val_mse: 34.8405\nEpoch 244/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.0608 - mse: 3.0608\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5009 - mse: 2.5009 - val_loss: 33.0552 - val_mse: 33.0552\nEpoch 245/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3275 - mse: 1.3275\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2936 - mse: 1.2936 - val_loss: 33.2845 - val_mse: 33.2845\nEpoch 246/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.8981 - mse: 1.8981\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7770 - mse: 1.7770 - val_loss: 34.4407 - val_mse: 34.4407\nEpoch 247/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.5539 - mse: 2.5539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1583 - mse: 2.1583 - val_loss: 33.2546 - val_mse: 33.2546\nEpoch 248/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4231 - mse: 1.4231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2071 - mse: 1.2071 - val_loss: 33.0622 - val_mse: 33.0622\nEpoch 249/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.3826 - mse: 1.3826\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2386 - mse: 1.2386 - val_loss: 33.7442 - val_mse: 33.7442\nEpoch 250/250\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.6531 - mse: 1.6531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4239 - mse: 1.4239 - val_loss: 33.3320 - val_mse: 33.3320\n\n\nVemos que con esta red podemos alcanzar un error de entrenamiento cercano a cero, aún cuando vemos que sobreajusta al evaluar con la muestra de validación.\n\nplot(historia, smooth = FALSE)\n\n\n\n\n\n\n\n\nNotamos que las predicciones no son muy buenas:\n\npreds &lt;- predict(modelo_red, x_grasa_pr) \n\n4/4 - 0s - 12ms/step\n\npreds |&gt; head()\n\n          [,1]\n[1,]  9.408182\n[2,] 20.048317\n[3,]  8.154854\n[4,]  8.547846\n[5,]  8.719688\n[6,]  8.244875\n\n\nY obtenemos el siguiente resultado:\n\ng_1 &lt;- tibble(preds = preds[, 1], y = y_grasa_pr) |&gt; \n  ggplot(aes(x = preds, y = y)) + \n  geom_point() + \n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  coord_obs_pred()\ng_1\n\n\n\n\n\n\n\n\nPodemos ahora experimentar con los parámetros del optimizador, número de unidades, número de capas y regularización L2.\n\nCada cambio de número de unidades/capas o regularización requiere ajustes a la tasa de aprendizaje y otros parámetros del optimizador.\nVarias arquitecturas (número de capas y unidades) pueden dar resultados similares. En este caso, usualmente escogemos el modelo más computacionalmente simple, o dependiendo del tipo de errores de cada modelo.\nRecordamos que una consecuencia del sobreajuste es que el error de prueba es mayor que el de entrenamiento (hay un gap de generalización). Para mejorar el desempeño, podemos reducir el error de entrenamiento (reducir sesgo), y/o reducir sobreajuste (gap entre entrenamiento y prueba).\n\n\nmodelo_red_2 &lt;- keras_model_sequential() |&gt; \n  layer_dense(units = 30, activation = \"sigmoid\", \n              kernel_regularizer = regularizer_l2(0.1)) |&gt; \n  layer_dense(units = 30, activation = \"sigmoid\",\n              kernel_regularizer = regularizer_l2(0.1)) |&gt; \n  layer_dense(units = 1, activation = \"linear\", \n              kernel_regularizer = regularizer_l2(0.01))\nmodelo_red_2 |&gt; compile(loss = \"mse\",metrics = c(\"mse\"),\n  optimizer = optimizer_sgd(learning_rate = 0.0005, momentum = 0.95)\n)\nhistoria &lt;- modelo_red_2 |&gt; fit(\n  x = x_grasa, y = y_grasa,\n  validation_data = list(x_grasa_pr, y_grasa_pr),\n  batch_size = 30, epochs = 500, verbose = 1)\n\nEpoch 1/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 563ms/step - loss: 395.5947 - mse: 390.9590\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 425.5728 - mse: 420.9389 - val_loss: 317.2727 - val_mse: 312.6415\nEpoch 2/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 295.0576 - mse: 290.4265\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 296.8948 - mse: 292.2542 - val_loss: 150.3288 - val_mse: 145.6028\nEpoch 3/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 132.4550 - mse: 127.7292\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 133.1680 - mse: 128.3928 - val_loss: 66.8864 - val_mse: 61.8658\nEpoch 4/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 54.4238 - mse: 49.4032\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 72.3654 - mse: 67.2674 - val_loss: 113.0077 - val_mse: 107.6288\nEpoch 5/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 103.7894 - mse: 98.4106\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 120.0171 - mse: 114.5915 - val_loss: 143.9743 - val_mse: 138.4392\nEpoch 6/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 133.8020 - mse: 128.2670\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 129.6526 - mse: 124.1193 - val_loss: 92.6712 - val_mse: 87.1548\nEpoch 7/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 81.0299 - mse: 75.5136\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 80.3804 - mse: 74.8606 - val_loss: 60.4856 - val_mse: 54.9007\nEpoch 8/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 48.6990 - mse: 43.1142\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 63.9023 - mse: 58.2789 - val_loss: 68.2526 - val_mse: 62.4496\nEpoch 9/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 56.3188 - mse: 50.5157\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 75.4705 - mse: 69.6133 - val_loss: 73.9312 - val_mse: 67.8619\nEpoch 10/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 62.7215 - mse: 56.6522\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 77.3542 - mse: 71.2306 - val_loss: 64.9859 - val_mse: 58.6511\nEpoch 11/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 55.2960 - mse: 48.9612\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 64.0597 - mse: 57.6689 - val_loss: 53.4194 - val_mse: 46.8025\nEpoch 12/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 44.5767 - mse: 37.9599\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 49.9113 - mse: 43.2324 - val_loss: 52.2846 - val_mse: 45.3585\nEpoch 13/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 42.8115 - mse: 35.8854\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 46.0770 - mse: 39.0867 - val_loss: 58.5116 - val_mse: 51.2783\nEpoch 14/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 48.0026 - mse: 40.7693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 48.4146 - mse: 41.1236 - val_loss: 60.9351 - val_mse: 53.4332\nEpoch 15/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 50.6305 - mse: 43.1286\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 48.3101 - mse: 40.7611 - val_loss: 57.0386 - val_mse: 49.3194\nEpoch 16/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 48.8703 - mse: 41.1511\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 45.5030 - mse: 37.7468 - val_loss: 52.4616 - val_mse: 44.5738\nEpoch 17/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 47.5312 - mse: 39.6435\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 44.2066 - mse: 36.2920 - val_loss: 50.5532 - val_mse: 42.5468\nEpoch 18/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 48.4360 - mse: 40.4296\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 44.9704 - mse: 36.9480 - val_loss: 50.0402 - val_mse: 41.9682\nEpoch 19/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 49.4093 - mse: 41.3373\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 45.3690 - mse: 37.2917 - val_loss: 49.0826 - val_mse: 40.9951\nEpoch 20/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 48.5694 - mse: 40.4819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 43.9592 - mse: 35.8755 - val_loss: 47.5184 - val_mse: 39.4554\nEpoch 21/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 46.1541 - mse: 38.0911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 41.3428 - mse: 33.2899 - val_loss: 46.0497 - val_mse: 38.0382\nEpoch 22/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 43.4425 - mse: 35.4310\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 38.7941 - mse: 30.7964 - val_loss: 44.7717 - val_mse: 36.8269\nEpoch 23/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 41.0892 - mse: 33.1445\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 36.7833 - mse: 28.8541 - val_loss: 43.0553 - val_mse: 35.1834\nEpoch 24/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 38.7632 - mse: 30.8913\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 34.9356 - mse: 27.0791 - val_loss: 40.6654 - val_mse: 32.8634\nEpoch 25/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 36.1814 - mse: 28.3794\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 33.0478 - mse: 25.2593 - val_loss: 38.2719 - val_mse: 30.5283\nEpoch 26/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 33.7826 - mse: 26.0390\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 31.4864 - mse: 23.7529 - val_loss: 36.6139 - val_mse: 28.9109\nEpoch 27/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 32.1040 - mse: 24.4010\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 30.5286 - mse: 22.8315 - val_loss: 35.6965 - val_mse: 28.0142\nEpoch 28/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 31.0706 - mse: 23.3883\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 29.9109 - mse: 22.2307 - val_loss: 35.0837 - val_mse: 27.4050\nEpoch 29/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 30.2609 - mse: 22.5821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 29.2493 - mse: 21.5698 - val_loss: 34.5801 - val_mse: 26.8933\nEpoch 30/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 29.5121 - mse: 21.8253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 28.5276 - mse: 20.8386 - val_loss: 34.3037 - val_mse: 26.6045\nEpoch 31/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 28.9463 - mse: 21.2471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 27.9645 - mse: 20.2634 - val_loss: 34.3003 - val_mse: 26.5923\nEpoch 32/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 28.5952 - mse: 20.8872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 27.6308 - mse: 19.9225 - val_loss: 34.4093 - val_mse: 26.7026\nEpoch 33/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 28.3019 - mse: 20.5951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27.4072 - mse: 19.7030 - val_loss: 34.4700 - val_mse: 26.7787\nEpoch 34/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 27.9411 - mse: 20.2498\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 27.1931 - mse: 19.5074 - val_loss: 34.4388 - val_mse: 26.7782\nEpoch 35/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 27.5140 - mse: 19.8534\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 26.9674 - mse: 19.3154 - val_loss: 34.3051 - val_mse: 26.6889\nEpoch 36/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 27.0381 - mse: 19.4219\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 26.7056 - mse: 19.1004 - val_loss: 34.0461 - val_mse: 26.4851\nEpoch 37/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 26.4998 - mse: 18.9389\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 26.3738 - mse: 18.8255 - val_loss: 33.6849 - val_mse: 26.1861\nEpoch 38/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 25.9259 - mse: 18.4271\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 25.9938 - mse: 18.5086 - val_loss: 33.3060 - val_mse: 25.8726\nEpoch 39/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 25.3985 - mse: 17.9651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 25.6356 - mse: 18.2160 - val_loss: 32.9866 - val_mse: 25.6188\nEpoch 40/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.9800 - mse: 17.6122\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 25.3446 - mse: 17.9903 - val_loss: 32.7520 - val_mse: 25.4478\nEpoch 41/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.6731 - mse: 17.3689\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 25.1194 - mse: 17.8280 - val_loss: 32.5968 - val_mse: 25.3530\nEpoch 42/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.4597 - mse: 17.2159\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.9485 - mse: 17.7169 - val_loss: 32.5072 - val_mse: 25.3202\nEpoch 43/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.3289 - mse: 17.1419\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.8266 - mse: 17.6511 - val_loss: 32.4546 - val_mse: 25.3213\nEpoch 44/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.2603 - mse: 17.1270\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.7372 - mse: 17.6149 - val_loss: 32.4008 - val_mse: 25.3190\nEpoch 45/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.2167 - mse: 17.1349\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.6544 - mse: 17.5833 - val_loss: 32.3192 - val_mse: 25.2879\nEpoch 46/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.1665 - mse: 17.1352\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.5618 - mse: 17.5411 - val_loss: 32.2027 - val_mse: 25.2218\nEpoch 47/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.0960 - mse: 17.1150\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.4581 - mse: 17.4878 - val_loss: 32.0552 - val_mse: 25.1248\nEpoch 48/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.0029 - mse: 17.0725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.3480 - mse: 17.4283 - val_loss: 31.8866 - val_mse: 25.0067\nEpoch 49/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.8913 - mse: 17.0114\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.2369 - mse: 17.3677 - val_loss: 31.7126 - val_mse: 24.8826\nEpoch 50/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.7709 - mse: 16.9409\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.1307 - mse: 17.3111 - val_loss: 31.5494 - val_mse: 24.7679\nEpoch 51/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.6536 - mse: 16.8721\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.0329 - mse: 17.2614 - val_loss: 31.4063 - val_mse: 24.6713\nEpoch 52/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 23.5463 - mse: 16.8113\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.9425 - mse: 17.2171 - val_loss: 31.2852 - val_mse: 24.5946\nEpoch 53/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.4504 - mse: 16.7598\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.8571 - mse: 17.1756 - val_loss: 31.1850 - val_mse: 24.5367\nEpoch 54/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.3653 - mse: 16.7169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.7767 - mse: 17.1371 - val_loss: 31.1021 - val_mse: 24.4943\nEpoch 55/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.2887 - mse: 16.6809\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.7023 - mse: 17.1031 - val_loss: 31.0298 - val_mse: 24.4615\nEpoch 56/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.2161 - mse: 16.6479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.6337 - mse: 17.0737 - val_loss: 30.9604 - val_mse: 24.4309\nEpoch 57/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.1423 - mse: 16.6128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.5685 - mse: 17.0473 - val_loss: 30.8876 - val_mse: 24.3963\nEpoch 58/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.0635 - mse: 16.5723\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.5040 - mse: 17.0209 - val_loss: 30.8085 - val_mse: 24.3551\nEpoch 59/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.9787 - mse: 16.5253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.4376 - mse: 16.9922 - val_loss: 30.7237 - val_mse: 24.3075\nEpoch 60/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.8893 - mse: 16.4731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.3686 - mse: 16.9603 - val_loss: 30.6361 - val_mse: 24.2563\nEpoch 61/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.7987 - mse: 16.4189\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.2981 - mse: 16.9260 - val_loss: 30.5500 - val_mse: 24.2056\nEpoch 62/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.7112 - mse: 16.3669\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23.2287 - mse: 16.8919 - val_loss: 30.4690 - val_mse: 24.1591\nEpoch 63/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.6302 - mse: 16.3203\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.1630 - mse: 16.8604 - val_loss: 30.3949 - val_mse: 24.1185\nEpoch 64/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.5571 - mse: 16.2806\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.1022 - mse: 16.8328 - val_loss: 30.3276 - val_mse: 24.0835\nEpoch 65/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.4916 - mse: 16.2475\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 23.0464 - mse: 16.8091 - val_loss: 30.2652 - val_mse: 24.0524\nEpoch 66/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.4319 - mse: 16.2192\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.9941 - mse: 16.7880 - val_loss: 30.2052 - val_mse: 24.0230\nEpoch 67/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.3757 - mse: 16.1935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.9436 - mse: 16.7679 - val_loss: 30.1455 - val_mse: 23.9930\nEpoch 68/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.3207 - mse: 16.1683\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.8936 - mse: 16.7475 - val_loss: 30.0849 - val_mse: 23.9616\nEpoch 69/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.2657 - mse: 16.1424\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.8435 - mse: 16.7264 - val_loss: 30.0236 - val_mse: 23.9288\nEpoch 70/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.2102 - mse: 16.1155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.7937 - mse: 16.7050 - val_loss: 29.9622 - val_mse: 23.8954\nEpoch 71/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 22.1548 - mse: 16.0879\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.7447 - mse: 16.6838 - val_loss: 29.9020 - val_mse: 23.8624\nEpoch 72/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 22.1002 - mse: 16.0605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.6970 - mse: 16.6632 - val_loss: 29.8440 - val_mse: 23.8310\nEpoch 73/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 22.0470 - mse: 16.0340\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.6510 - mse: 16.6437 - val_loss: 29.7887 - val_mse: 23.8016\nEpoch 74/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.9959 - mse: 16.0088\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.6066 - mse: 16.6250 - val_loss: 29.7364 - val_mse: 23.7744\nEpoch 75/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.9470 - mse: 15.9850\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.5637 - mse: 16.6071 - val_loss: 29.6866 - val_mse: 23.7491\nEpoch 76/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 21.9001 - mse: 15.9626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.5221 - mse: 16.5898 - val_loss: 29.6390 - val_mse: 23.7253\nEpoch 77/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.8548 - mse: 15.9412\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.4816 - mse: 16.5731 - val_loss: 29.5929 - val_mse: 23.7026\nEpoch 78/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.8108 - mse: 15.9204\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.4424 - mse: 16.5570 - val_loss: 29.5480 - val_mse: 23.6803\nEpoch 79/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.7678 - mse: 15.9001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.4042 - mse: 16.5414 - val_loss: 29.5037 - val_mse: 23.6582\nEpoch 80/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.7255 - mse: 15.8800\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.3670 - mse: 16.5262 - val_loss: 29.4600 - val_mse: 23.6362\nEpoch 81/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.6839 - mse: 15.8601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.3305 - mse: 16.5114 - val_loss: 29.4169 - val_mse: 23.6143\nEpoch 82/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 21.6432 - mse: 15.8405\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.2948 - mse: 16.4967 - val_loss: 29.3746 - val_mse: 23.5926\nEpoch 83/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 21.6035 - mse: 15.8215\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.2598 - mse: 16.4823 - val_loss: 29.3333 - val_mse: 23.5715\nEpoch 84/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.5651 - mse: 15.8033\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.2257 - mse: 16.4683 - val_loss: 29.2931 - val_mse: 23.5511\nEpoch 85/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 21.5281 - mse: 15.7860\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.1926 - mse: 16.4549 - val_loss: 29.2541 - val_mse: 23.5314\nEpoch 86/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.4925 - mse: 15.7697\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.1606 - mse: 16.4421 - val_loss: 29.2163 - val_mse: 23.5124\nEpoch 87/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.4582 - mse: 15.7543\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.1296 - mse: 16.4298 - val_loss: 29.1794 - val_mse: 23.4939\nEpoch 88/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 21.4251 - mse: 15.7396\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.0994 - mse: 16.4180 - val_loss: 29.1432 - val_mse: 23.4758\nEpoch 89/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 21.3929 - mse: 15.7255\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 22.0700 - mse: 16.4065 - val_loss: 29.1077 - val_mse: 23.4580\nEpoch 90/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.3615 - mse: 15.7117\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.0412 - mse: 16.3954 - val_loss: 29.0727 - val_mse: 23.4403\nEpoch 91/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 21.3307 - mse: 15.6983\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.0130 - mse: 16.3844 - val_loss: 29.0382 - val_mse: 23.4228\nEpoch 92/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.3007 - mse: 15.6852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.9854 - mse: 16.3737 - val_loss: 29.0044 - val_mse: 23.4056\nEpoch 93/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 21.2713 - mse: 15.6725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.9584 - mse: 16.3633 - val_loss: 28.9713 - val_mse: 23.3888\nEpoch 94/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.2427 - mse: 15.6601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.9321 - mse: 16.3531 - val_loss: 28.9389 - val_mse: 23.3724\nEpoch 95/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.2147 - mse: 15.6482\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.9064 - mse: 16.3433 - val_loss: 28.9073 - val_mse: 23.3564\nEpoch 96/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.1875 - mse: 15.6366\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.8812 - mse: 16.3338 - val_loss: 28.8764 - val_mse: 23.3408\nEpoch 97/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.1610 - mse: 15.6254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.8567 - mse: 16.3245 - val_loss: 28.8462 - val_mse: 23.3256\nEpoch 98/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.1352 - mse: 15.6146\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.8327 - mse: 16.3154 - val_loss: 28.8166 - val_mse: 23.3107\nEpoch 99/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.1099 - mse: 15.6040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.8092 - mse: 16.3066 - val_loss: 28.7876 - val_mse: 23.2962\nEpoch 100/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.0851 - mse: 15.5937\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.7862 - mse: 16.2980 - val_loss: 28.7592 - val_mse: 23.2818\nEpoch 101/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.0609 - mse: 15.5835\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.7637 - mse: 16.2895 - val_loss: 28.7312 - val_mse: 23.2677\nEpoch 102/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 21.0371 - mse: 15.5736\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.7417 - mse: 16.2812 - val_loss: 28.7038 - val_mse: 23.2539\nEpoch 103/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.0138 - mse: 15.5639\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.7201 - mse: 16.2732 - val_loss: 28.6770 - val_mse: 23.2403\nEpoch 104/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.9910 - mse: 15.5544\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.6989 - mse: 16.2652 - val_loss: 28.6506 - val_mse: 23.2270\nEpoch 105/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.9687 - mse: 15.5451\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.6781 - mse: 16.2575 - val_loss: 28.6248 - val_mse: 23.2140\nEpoch 106/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.9468 - mse: 15.5360\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.6578 - mse: 16.2499 - val_loss: 28.5995 - val_mse: 23.2012\nEpoch 107/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.9254 - mse: 15.5271\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.6379 - mse: 16.2424 - val_loss: 28.5747 - val_mse: 23.1887\nEpoch 108/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.9044 - mse: 15.5184\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.6184 - mse: 16.2351 - val_loss: 28.5504 - val_mse: 23.1764\nEpoch 109/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.8838 - mse: 15.5099\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.5992 - mse: 16.2280 - val_loss: 28.5265 - val_mse: 23.1644\nEpoch 110/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.8636 - mse: 15.5015\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.5805 - mse: 16.2210 - val_loss: 28.5031 - val_mse: 23.1525\nEpoch 111/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.8438 - mse: 15.4933\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.5621 - mse: 16.2142 - val_loss: 28.4801 - val_mse: 23.1409\nEpoch 112/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.8244 - mse: 15.4852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.5440 - mse: 16.2074 - val_loss: 28.4575 - val_mse: 23.1295\nEpoch 113/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.8053 - mse: 15.4773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.5263 - mse: 16.2008 - val_loss: 28.4353 - val_mse: 23.1183\nEpoch 114/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.7866 - mse: 15.4695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.5089 - mse: 16.1943 - val_loss: 28.4136 - val_mse: 23.1073\nEpoch 115/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.7682 - mse: 15.4618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.4918 - mse: 16.1879 - val_loss: 28.3922 - val_mse: 23.0964\nEpoch 116/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.7501 - mse: 15.4543\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.4750 - mse: 16.1816 - val_loss: 28.3713 - val_mse: 23.0858\nEpoch 117/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.7324 - mse: 15.4469\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.4585 - mse: 16.1755 - val_loss: 28.3507 - val_mse: 23.0754\nEpoch 118/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.7149 - mse: 15.4396\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.4424 - mse: 16.1694 - val_loss: 28.3305 - val_mse: 23.0652\nEpoch 119/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.6978 - mse: 15.4325\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.4265 - mse: 16.1634 - val_loss: 28.3107 - val_mse: 23.0551\nEpoch 120/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.6810 - mse: 15.4254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.4109 - mse: 16.1576 - val_loss: 28.2912 - val_mse: 23.0452\nEpoch 121/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.6644 - mse: 15.4184\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.3955 - mse: 16.1518 - val_loss: 28.2720 - val_mse: 23.0355\nEpoch 122/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.6481 - mse: 15.4116\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.3804 - mse: 16.1461 - val_loss: 28.2532 - val_mse: 23.0259\nEpoch 123/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.6321 - mse: 15.4048\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.3656 - mse: 16.1405 - val_loss: 28.2347 - val_mse: 23.0165\nEpoch 124/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 20.6164 - mse: 15.3982\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.3511 - mse: 16.1349 - val_loss: 28.2165 - val_mse: 23.0072\nEpoch 125/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.6009 - mse: 15.3916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.3367 - mse: 16.1295 - val_loss: 28.1987 - val_mse: 22.9981\nEpoch 126/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.5857 - mse: 15.3851\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.3226 - mse: 16.1241 - val_loss: 28.1811 - val_mse: 22.9891\nEpoch 127/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.5707 - mse: 15.3787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.3088 - mse: 16.1188 - val_loss: 28.1639 - val_mse: 22.9803\nEpoch 128/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.5559 - mse: 15.3724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.2952 - mse: 16.1136 - val_loss: 28.1469 - val_mse: 22.9716\nEpoch 129/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.5414 - mse: 15.3661\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.2817 - mse: 16.1084 - val_loss: 28.1302 - val_mse: 22.9631\nEpoch 130/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.5271 - mse: 15.3600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.2686 - mse: 16.1033 - val_loss: 28.1139 - val_mse: 22.9547\nEpoch 131/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.5130 - mse: 15.3539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.2556 - mse: 16.0983 - val_loss: 28.0977 - val_mse: 22.9464\nEpoch 132/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.4992 - mse: 15.3478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.2428 - mse: 16.0933 - val_loss: 28.0819 - val_mse: 22.9382\nEpoch 133/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.4855 - mse: 15.3419\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.2302 - mse: 16.0884 - val_loss: 28.0663 - val_mse: 22.9302\nEpoch 134/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.4721 - mse: 15.3360\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.2178 - mse: 16.0835 - val_loss: 28.0510 - val_mse: 22.9222\nEpoch 135/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.4588 - mse: 15.3301\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.2056 - mse: 16.0787 - val_loss: 28.0359 - val_mse: 22.9144\nEpoch 136/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.4458 - mse: 15.3243\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1936 - mse: 16.0739 - val_loss: 28.0211 - val_mse: 22.9067\nEpoch 137/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.4329 - mse: 15.3186\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1818 - mse: 16.0692 - val_loss: 28.0065 - val_mse: 22.8991\nEpoch 138/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.4203 - mse: 15.3129\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1701 - mse: 16.0645 - val_loss: 27.9921 - val_mse: 22.8917\nEpoch 139/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.4078 - mse: 15.3073\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1587 - mse: 16.0599 - val_loss: 27.9780 - val_mse: 22.8843\nEpoch 140/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.3955 - mse: 15.3018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1473 - mse: 16.0553 - val_loss: 27.9640 - val_mse: 22.8770\nEpoch 141/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.3833 - mse: 15.2963\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1362 - mse: 16.0508 - val_loss: 27.9504 - val_mse: 22.8698\nEpoch 142/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.3713 - mse: 15.2908\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1252 - mse: 16.0463 - val_loss: 27.9369 - val_mse: 22.8627\nEpoch 143/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.3595 - mse: 15.2854\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1144 - mse: 16.0418 - val_loss: 27.9236 - val_mse: 22.8557\nEpoch 144/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.3479 - mse: 15.2800\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1037 - mse: 16.0374 - val_loss: 27.9105 - val_mse: 22.8488\nEpoch 145/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.3364 - mse: 15.2747\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.0932 - mse: 16.0330 - val_loss: 27.8977 - val_mse: 22.8420\nEpoch 146/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.3251 - mse: 15.2694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.0828 - mse: 16.0286 - val_loss: 27.8850 - val_mse: 22.8353\nEpoch 147/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.3139 - mse: 15.2642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.0726 - mse: 16.0243 - val_loss: 27.8725 - val_mse: 22.8286\nEpoch 148/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 20.3029 - mse: 15.2590\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 21.0625 - mse: 16.0200 - val_loss: 27.8602 - val_mse: 22.8220\nEpoch 149/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.2920 - mse: 15.2538\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.0525 - mse: 16.0158 - val_loss: 27.8481 - val_mse: 22.8155\nEpoch 150/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.2813 - mse: 15.2487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.0427 - mse: 16.0115 - val_loss: 27.8362 - val_mse: 22.8091\nEpoch 151/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.2707 - mse: 15.2436\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.0330 - mse: 16.0073 - val_loss: 27.8244 - val_mse: 22.8027\nEpoch 152/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.2602 - mse: 15.2385\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.0234 - mse: 16.0031 - val_loss: 27.8128 - val_mse: 22.7965\nEpoch 153/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.2499 - mse: 15.2335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.0140 - mse: 15.9990 - val_loss: 27.8014 - val_mse: 22.7902\nEpoch 154/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.2397 - mse: 15.2285\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.0047 - mse: 15.9948 - val_loss: 27.7902 - val_mse: 22.7841\nEpoch 155/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.2296 - mse: 15.2236\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9955 - mse: 15.9907 - val_loss: 27.7791 - val_mse: 22.7780\nEpoch 156/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 20.2197 - mse: 15.2186\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9864 - mse: 15.9866 - val_loss: 27.7681 - val_mse: 22.7720\nEpoch 157/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.2099 - mse: 15.2137\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9774 - mse: 15.9826 - val_loss: 27.7574 - val_mse: 22.7660\nEpoch 158/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.2002 - mse: 15.2089\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9686 - mse: 15.9785 - val_loss: 27.7467 - val_mse: 22.7601\nEpoch 159/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1906 - mse: 15.2040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9598 - mse: 15.9745 - val_loss: 27.7362 - val_mse: 22.7543\nEpoch 160/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1811 - mse: 15.1992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9512 - mse: 15.9705 - val_loss: 27.7259 - val_mse: 22.7485\nEpoch 161/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1718 - mse: 15.1944\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9426 - mse: 15.9665 - val_loss: 27.7157 - val_mse: 22.7428\nEpoch 162/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1625 - mse: 15.1897\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9342 - mse: 15.9625 - val_loss: 27.7056 - val_mse: 22.7371\nEpoch 163/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1534 - mse: 15.1849\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9259 - mse: 15.9585 - val_loss: 27.6957 - val_mse: 22.7315\nEpoch 164/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1444 - mse: 15.1802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9176 - mse: 15.9546 - val_loss: 27.6859 - val_mse: 22.7259\nEpoch 165/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1355 - mse: 15.1755\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9095 - mse: 15.9507 - val_loss: 27.6762 - val_mse: 22.7204\nEpoch 166/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1267 - mse: 15.1708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.9014 - mse: 15.9467 - val_loss: 27.6666 - val_mse: 22.7149\nEpoch 167/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1179 - mse: 15.1662\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8935 - mse: 15.9428 - val_loss: 27.6572 - val_mse: 22.7094\nEpoch 168/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1093 - mse: 15.1615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8856 - mse: 15.9389 - val_loss: 27.6479 - val_mse: 22.7040\nEpoch 169/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.1008 - mse: 15.1569\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8778 - mse: 15.9350 - val_loss: 27.6387 - val_mse: 22.6987\nEpoch 170/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0924 - mse: 15.1523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8701 - mse: 15.9312 - val_loss: 27.6297 - val_mse: 22.6934\nEpoch 171/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0841 - mse: 15.1478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8625 - mse: 15.9273 - val_loss: 27.6207 - val_mse: 22.6881\nEpoch 172/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0758 - mse: 15.1432\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8550 - mse: 15.9234 - val_loss: 27.6119 - val_mse: 22.6829\nEpoch 173/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0677 - mse: 15.1387\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8476 - mse: 15.9196 - val_loss: 27.6031 - val_mse: 22.6777\nEpoch 174/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0596 - mse: 15.1342\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8402 - mse: 15.9157 - val_loss: 27.5945 - val_mse: 22.6725\nEpoch 175/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0516 - mse: 15.1296\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8329 - mse: 15.9119 - val_loss: 27.5859 - val_mse: 22.6674\nEpoch 176/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0437 - mse: 15.1252\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8257 - mse: 15.9081 - val_loss: 27.5775 - val_mse: 22.6623\nEpoch 177/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 20.0359 - mse: 15.1207\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8185 - mse: 15.9042 - val_loss: 27.5692 - val_mse: 22.6572\nEpoch 178/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0282 - mse: 15.1162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8114 - mse: 15.9004 - val_loss: 27.5609 - val_mse: 22.6521\nEpoch 179/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0205 - mse: 15.1118\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.8044 - mse: 15.8966 - val_loss: 27.5528 - val_mse: 22.6471\nEpoch 180/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0130 - mse: 15.1073\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7975 - mse: 15.8928 - val_loss: 27.5447 - val_mse: 22.6422\nEpoch 181/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.0055 - mse: 15.1029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7906 - mse: 15.8890 - val_loss: 27.5368 - val_mse: 22.6372\nEpoch 182/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9981 - mse: 15.0985\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7838 - mse: 15.8851 - val_loss: 27.5289 - val_mse: 22.6323\nEpoch 183/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9907 - mse: 15.0941\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7771 - mse: 15.8813 - val_loss: 27.5211 - val_mse: 22.6274\nEpoch 184/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9834 - mse: 15.0897\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7704 - mse: 15.8775 - val_loss: 27.5134 - val_mse: 22.6225\nEpoch 185/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9762 - mse: 15.0853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7638 - mse: 15.8737 - val_loss: 27.5058 - val_mse: 22.6177\nEpoch 186/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.9691 - mse: 15.0809\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7572 - mse: 15.8699 - val_loss: 27.4983 - val_mse: 22.6128\nEpoch 187/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9620 - mse: 15.0766\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7507 - mse: 15.8661 - val_loss: 27.4908 - val_mse: 22.6080\nEpoch 188/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9550 - mse: 15.0722\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7443 - mse: 15.8623 - val_loss: 27.4834 - val_mse: 22.6032\nEpoch 189/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9481 - mse: 15.0679\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7379 - mse: 15.8585 - val_loss: 27.4761 - val_mse: 22.5985\nEpoch 190/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9412 - mse: 15.0635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7315 - mse: 15.8547 - val_loss: 27.4689 - val_mse: 22.5937\nEpoch 191/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9344 - mse: 15.0592\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7252 - mse: 15.8509 - val_loss: 27.4618 - val_mse: 22.5890\nEpoch 192/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9276 - mse: 15.0549\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7190 - mse: 15.8470 - val_loss: 27.4547 - val_mse: 22.5843\nEpoch 193/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9209 - mse: 15.0505\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7128 - mse: 15.8432 - val_loss: 27.4477 - val_mse: 22.5796\nEpoch 194/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9143 - mse: 15.0462\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7067 - mse: 15.8394 - val_loss: 27.4407 - val_mse: 22.5749\nEpoch 195/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9077 - mse: 15.0419\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7006 - mse: 15.8356 - val_loss: 27.4338 - val_mse: 22.5703\nEpoch 196/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.9012 - mse: 15.0376\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6946 - mse: 15.8317 - val_loss: 27.4270 - val_mse: 22.5656\nEpoch 197/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8947 - mse: 15.0333\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6886 - mse: 15.8279 - val_loss: 27.4203 - val_mse: 22.5610\nEpoch 198/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8883 - mse: 15.0290\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6826 - mse: 15.8241 - val_loss: 27.4136 - val_mse: 22.5564\nEpoch 199/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8819 - mse: 15.0247\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6767 - mse: 15.8202 - val_loss: 27.4069 - val_mse: 22.5518\nEpoch 200/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8756 - mse: 15.0204\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6708 - mse: 15.8164 - val_loss: 27.4004 - val_mse: 22.5472\nEpoch 201/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8693 - mse: 15.0161\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6650 - mse: 15.8125 - val_loss: 27.3938 - val_mse: 22.5426\nEpoch 202/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8631 - mse: 15.0119\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6592 - mse: 15.8086 - val_loss: 27.3874 - val_mse: 22.5380\nEpoch 203/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8570 - mse: 15.0076\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6535 - mse: 15.8048 - val_loss: 27.3810 - val_mse: 22.5334\nEpoch 204/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8508 - mse: 15.0033\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6478 - mse: 15.8009 - val_loss: 27.3746 - val_mse: 22.5289\nEpoch 205/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8447 - mse: 14.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6421 - mse: 15.7970 - val_loss: 27.3684 - val_mse: 22.5243\nEpoch 206/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8387 - mse: 14.9947\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20.6365 - mse: 15.7931 - val_loss: 27.3621 - val_mse: 22.5198\nEpoch 207/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8327 - mse: 14.9904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6309 - mse: 15.7892 - val_loss: 27.3559 - val_mse: 22.5153\nEpoch 208/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8268 - mse: 14.9861\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6253 - mse: 15.7853 - val_loss: 27.3498 - val_mse: 22.5107\nEpoch 209/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8208 - mse: 14.9818\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6198 - mse: 15.7814 - val_loss: 27.3437 - val_mse: 22.5062\nEpoch 210/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.8150 - mse: 14.9775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6142 - mse: 15.7774 - val_loss: 27.3376 - val_mse: 22.5017\nEpoch 211/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8091 - mse: 14.9732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6088 - mse: 15.7735 - val_loss: 27.3316 - val_mse: 22.4972\nEpoch 212/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.8033 - mse: 14.9689\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.6033 - mse: 15.7695 - val_loss: 27.3257 - val_mse: 22.4927\nEpoch 213/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7976 - mse: 14.9646\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5979 - mse: 15.7655 - val_loss: 27.3197 - val_mse: 22.4882\nEpoch 214/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7918 - mse: 14.9603\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5925 - mse: 15.7616 - val_loss: 27.3139 - val_mse: 22.4837\nEpoch 215/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7861 - mse: 14.9560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5872 - mse: 15.7576 - val_loss: 27.3080 - val_mse: 22.4792\nEpoch 216/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7805 - mse: 14.9517\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5818 - mse: 15.7536 - val_loss: 27.3023 - val_mse: 22.4747\nEpoch 217/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7749 - mse: 14.9473\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5765 - mse: 15.7496 - val_loss: 27.2965 - val_mse: 22.4702\nEpoch 218/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7693 - mse: 14.9430\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5712 - mse: 15.7455 - val_loss: 27.2908 - val_mse: 22.4658\nEpoch 219/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7637 - mse: 14.9387\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5660 - mse: 15.7415 - val_loss: 27.2851 - val_mse: 22.4613\nEpoch 220/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7581 - mse: 14.9343\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5607 - mse: 15.7374 - val_loss: 27.2795 - val_mse: 22.4568\nEpoch 221/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7526 - mse: 14.9299\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5555 - mse: 15.7334 - val_loss: 27.2739 - val_mse: 22.4523\nEpoch 222/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7471 - mse: 14.9256\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5503 - mse: 15.7293 - val_loss: 27.2683 - val_mse: 22.4478\nEpoch 223/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7417 - mse: 14.9212\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5451 - mse: 15.7252 - val_loss: 27.2627 - val_mse: 22.4433\nEpoch 224/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7362 - mse: 14.9168\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5400 - mse: 15.7211 - val_loss: 27.2572 - val_mse: 22.4388\nEpoch 225/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7308 - mse: 14.9124\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5348 - mse: 15.7169 - val_loss: 27.2518 - val_mse: 22.4344\nEpoch 226/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7254 - mse: 14.9080\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20.5297 - mse: 15.7128 - val_loss: 27.2463 - val_mse: 22.4299\nEpoch 227/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7200 - mse: 14.9036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5246 - mse: 15.7087 - val_loss: 27.2409 - val_mse: 22.4254\nEpoch 228/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7147 - mse: 14.8991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5195 - mse: 15.7045 - val_loss: 27.2355 - val_mse: 22.4209\nEpoch 229/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7093 - mse: 14.8947\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20.5145 - mse: 15.7003 - val_loss: 27.2302 - val_mse: 22.4164\nEpoch 230/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.7040 - mse: 14.8902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5094 - mse: 15.6961 - val_loss: 27.2248 - val_mse: 22.4119\nEpoch 231/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6987 - mse: 14.8858\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.5044 - mse: 15.6919 - val_loss: 27.2195 - val_mse: 22.4073\nEpoch 232/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6934 - mse: 14.8813\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4994 - mse: 15.6876 - val_loss: 27.2142 - val_mse: 22.4028\nEpoch 233/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6882 - mse: 14.8768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4944 - mse: 15.6834 - val_loss: 27.2090 - val_mse: 22.3983\nEpoch 234/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6829 - mse: 14.8722\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4894 - mse: 15.6791 - val_loss: 27.2037 - val_mse: 22.3938\nEpoch 235/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 19.6777 - mse: 14.8677\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4844 - mse: 15.6748 - val_loss: 27.1985 - val_mse: 22.3892\nEpoch 236/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6724 - mse: 14.8632\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4794 - mse: 15.6705 - val_loss: 27.1933 - val_mse: 22.3847\nEpoch 237/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6672 - mse: 14.8586\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4744 - mse: 15.6662 - val_loss: 27.1882 - val_mse: 22.3801\nEpoch 238/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6620 - mse: 14.8540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4695 - mse: 15.6619 - val_loss: 27.1830 - val_mse: 22.3756\nEpoch 239/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6568 - mse: 14.8494\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4645 - mse: 15.6575 - val_loss: 27.1779 - val_mse: 22.3710\nEpoch 240/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6516 - mse: 14.8448\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4596 - mse: 15.6532 - val_loss: 27.1728 - val_mse: 22.3665\nEpoch 241/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6464 - mse: 14.8401\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4547 - mse: 15.6488 - val_loss: 27.1677 - val_mse: 22.3619\nEpoch 242/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6413 - mse: 14.8354\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4498 - mse: 15.6444 - val_loss: 27.1626 - val_mse: 22.3573\nEpoch 243/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6361 - mse: 14.8307\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4449 - mse: 15.6399 - val_loss: 27.1576 - val_mse: 22.3527\nEpoch 244/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6309 - mse: 14.8260\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4400 - mse: 15.6355 - val_loss: 27.1525 - val_mse: 22.3481\nEpoch 245/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6257 - mse: 14.8213\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4351 - mse: 15.6310 - val_loss: 27.1475 - val_mse: 22.3435\nEpoch 246/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6206 - mse: 14.8166\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4302 - mse: 15.6266 - val_loss: 27.1425 - val_mse: 22.3388\nEpoch 247/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6154 - mse: 14.8118\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4253 - mse: 15.6221 - val_loss: 27.1375 - val_mse: 22.3342\nEpoch 248/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6103 - mse: 14.8070\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4205 - mse: 15.6175 - val_loss: 27.1325 - val_mse: 22.3295\nEpoch 249/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6051 - mse: 14.8022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4156 - mse: 15.6130 - val_loss: 27.1275 - val_mse: 22.3249\nEpoch 250/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.6000 - mse: 14.7973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4108 - mse: 15.6085 - val_loss: 27.1226 - val_mse: 22.3202\nEpoch 251/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5948 - mse: 14.7924\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4059 - mse: 15.6039 - val_loss: 27.1176 - val_mse: 22.3155\nEpoch 252/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5897 - mse: 14.7876\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4011 - mse: 15.5993 - val_loss: 27.1127 - val_mse: 22.3108\nEpoch 253/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5845 - mse: 14.7826\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3962 - mse: 15.5947 - val_loss: 27.1078 - val_mse: 22.3061\nEpoch 254/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5793 - mse: 14.7777\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3914 - mse: 15.5901 - val_loss: 27.1028 - val_mse: 22.3014\nEpoch 255/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5742 - mse: 14.7727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3865 - mse: 15.5854 - val_loss: 27.0979 - val_mse: 22.2967\nEpoch 256/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5690 - mse: 14.7677\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3817 - mse: 15.5808 - val_loss: 27.0930 - val_mse: 22.2919\nEpoch 257/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5638 - mse: 14.7627\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3769 - mse: 15.5761 - val_loss: 27.0882 - val_mse: 22.2872\nEpoch 258/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5587 - mse: 14.7577\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3721 - mse: 15.5714 - val_loss: 27.0833 - val_mse: 22.2824\nEpoch 259/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5535 - mse: 14.7526\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3672 - mse: 15.5667 - val_loss: 27.0784 - val_mse: 22.2776\nEpoch 260/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5483 - mse: 14.7475\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3624 - mse: 15.5619 - val_loss: 27.0736 - val_mse: 22.2728\nEpoch 261/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5431 - mse: 14.7424\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3576 - mse: 15.5572 - val_loss: 27.0687 - val_mse: 22.2680\nEpoch 262/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5379 - mse: 14.7372\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3528 - mse: 15.5524 - val_loss: 27.0639 - val_mse: 22.2632\nEpoch 263/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5327 - mse: 14.7320\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3480 - mse: 15.5476 - val_loss: 27.0590 - val_mse: 22.2584\nEpoch 264/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5275 - mse: 14.7268\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20.3432 - mse: 15.5428 - val_loss: 27.0542 - val_mse: 22.2535\nEpoch 265/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5223 - mse: 14.7216\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3384 - mse: 15.5380 - val_loss: 27.0494 - val_mse: 22.2487\nEpoch 266/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5170 - mse: 14.7163\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3335 - mse: 15.5331 - val_loss: 27.0446 - val_mse: 22.2438\nEpoch 267/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5118 - mse: 14.7110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3287 - mse: 15.5282 - val_loss: 27.0398 - val_mse: 22.2389\nEpoch 268/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5066 - mse: 14.7057\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3239 - mse: 15.5234 - val_loss: 27.0350 - val_mse: 22.2340\nEpoch 269/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.5013 - mse: 14.7003\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3191 - mse: 15.5185 - val_loss: 27.0302 - val_mse: 22.2291\nEpoch 270/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4960 - mse: 14.6950\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3143 - mse: 15.5135 - val_loss: 27.0254 - val_mse: 22.2242\nEpoch 271/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4908 - mse: 14.6896\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3095 - mse: 15.5086 - val_loss: 27.0206 - val_mse: 22.2193\nEpoch 272/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4855 - mse: 14.6841\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.3047 - mse: 15.5036 - val_loss: 27.0159 - val_mse: 22.2144\nEpoch 273/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4802 - mse: 14.6787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2999 - mse: 15.4987 - val_loss: 27.0111 - val_mse: 22.2094\nEpoch 274/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4749 - mse: 14.6732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2951 - mse: 15.4937 - val_loss: 27.0064 - val_mse: 22.2045\nEpoch 275/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4696 - mse: 14.6677\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2903 - mse: 15.4887 - val_loss: 27.0017 - val_mse: 22.1995\nEpoch 276/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19.4643 - mse: 14.6621\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2855 - mse: 15.4836 - val_loss: 26.9969 - val_mse: 22.1945\nEpoch 277/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4589 - mse: 14.6565\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2807 - mse: 15.4786 - val_loss: 26.9922 - val_mse: 22.1896\nEpoch 278/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4536 - mse: 14.6509\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2759 - mse: 15.4735 - val_loss: 26.9875 - val_mse: 22.1846\nEpoch 279/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4482 - mse: 14.6453\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2712 - mse: 15.4685 - val_loss: 26.9828 - val_mse: 22.1796\nEpoch 280/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4429 - mse: 14.6397\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2664 - mse: 15.4634 - val_loss: 26.9781 - val_mse: 22.1746\nEpoch 281/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4375 - mse: 14.6340\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2616 - mse: 15.4583 - val_loss: 26.9734 - val_mse: 22.1696\nEpoch 282/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4321 - mse: 14.6283\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2568 - mse: 15.4531 - val_loss: 26.9688 - val_mse: 22.1646\nEpoch 283/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4267 - mse: 14.6225\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2520 - mse: 15.4480 - val_loss: 26.9641 - val_mse: 22.1596\nEpoch 284/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4213 - mse: 14.6168\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2472 - mse: 15.4429 - val_loss: 26.9595 - val_mse: 22.1545\nEpoch 285/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4159 - mse: 14.6110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2424 - mse: 15.4377 - val_loss: 26.9549 - val_mse: 22.1495\nEpoch 286/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4105 - mse: 14.6051\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2376 - mse: 15.4325 - val_loss: 26.9502 - val_mse: 22.1445\nEpoch 287/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.4051 - mse: 14.5993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2328 - mse: 15.4273 - val_loss: 26.9456 - val_mse: 22.1395\nEpoch 288/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3996 - mse: 14.5934\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2281 - mse: 15.4221 - val_loss: 26.9410 - val_mse: 22.1344\nEpoch 289/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3942 - mse: 14.5875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2233 - mse: 15.4168 - val_loss: 26.9365 - val_mse: 22.1294\nEpoch 290/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3887 - mse: 14.5816\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2185 - mse: 15.4116 - val_loss: 26.9319 - val_mse: 22.1244\nEpoch 291/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3832 - mse: 14.5757\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2137 - mse: 15.4063 - val_loss: 26.9274 - val_mse: 22.1193\nEpoch 292/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3777 - mse: 14.5697\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.2089 - mse: 15.4011 - val_loss: 26.9228 - val_mse: 22.1143\nEpoch 293/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3722 - mse: 14.5637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.2042 - mse: 15.3958 - val_loss: 26.9183 - val_mse: 22.1093\nEpoch 294/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3667 - mse: 14.5577\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1994 - mse: 15.3905 - val_loss: 26.9138 - val_mse: 22.1042\nEpoch 295/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3612 - mse: 14.5516\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.1946 - mse: 15.3852 - val_loss: 26.9094 - val_mse: 22.0992\nEpoch 296/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19.3557 - mse: 14.5455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.1898 - mse: 15.3798 - val_loss: 26.9049 - val_mse: 22.0942\nEpoch 297/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 19.3502 - mse: 14.5394\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.1851 - mse: 15.3745 - val_loss: 26.9005 - val_mse: 22.0892\nEpoch 298/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 19.3446 - mse: 14.5333\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1803 - mse: 15.3691 - val_loss: 26.8961 - val_mse: 22.0842\nEpoch 299/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3391 - mse: 14.5272\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1755 - mse: 15.3638 - val_loss: 26.8917 - val_mse: 22.0792\nEpoch 300/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3335 - mse: 14.5210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1708 - mse: 15.3584 - val_loss: 26.8873 - val_mse: 22.0742\nEpoch 301/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3280 - mse: 14.5148\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1660 - mse: 15.3530 - val_loss: 26.8829 - val_mse: 22.0692\nEpoch 302/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3224 - mse: 14.5086\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1612 - mse: 15.3476 - val_loss: 26.8786 - val_mse: 22.0642\nEpoch 303/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3168 - mse: 14.5024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1565 - mse: 15.3422 - val_loss: 26.8743 - val_mse: 22.0592\nEpoch 304/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3112 - mse: 14.4961\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1517 - mse: 15.3368 - val_loss: 26.8700 - val_mse: 22.0543\nEpoch 305/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3056 - mse: 14.4899\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1470 - mse: 15.3314 - val_loss: 26.8657 - val_mse: 22.0493\nEpoch 306/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.3000 - mse: 14.4836\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1422 - mse: 15.3259 - val_loss: 26.8615 - val_mse: 22.0444\nEpoch 307/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2944 - mse: 14.4772\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1375 - mse: 15.3205 - val_loss: 26.8573 - val_mse: 22.0394\nEpoch 308/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2887 - mse: 14.4709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1328 - mse: 15.3150 - val_loss: 26.8531 - val_mse: 22.0345\nEpoch 309/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2831 - mse: 14.4646\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1280 - mse: 15.3096 - val_loss: 26.8489 - val_mse: 22.0296\nEpoch 310/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2775 - mse: 14.4582\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1233 - mse: 15.3041 - val_loss: 26.8448 - val_mse: 22.0247\nEpoch 311/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2718 - mse: 14.4518\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 20.1186 - mse: 15.2986 - val_loss: 26.8407 - val_mse: 22.0198\nEpoch 312/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2662 - mse: 14.4454\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1138 - mse: 15.2931 - val_loss: 26.8366 - val_mse: 22.0150\nEpoch 313/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2605 - mse: 14.4389\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1091 - mse: 15.2877 - val_loss: 26.8325 - val_mse: 22.0101\nEpoch 314/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2549 - mse: 14.4325\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.1044 - mse: 15.2822 - val_loss: 26.8285 - val_mse: 22.0053\nEpoch 315/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2492 - mse: 14.4260\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0997 - mse: 15.2767 - val_loss: 26.8245 - val_mse: 22.0005\nEpoch 316/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2435 - mse: 14.4195\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0950 - mse: 15.2711 - val_loss: 26.8205 - val_mse: 21.9957\nEpoch 317/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2378 - mse: 14.4130\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0903 - mse: 15.2656 - val_loss: 26.8166 - val_mse: 21.9910\nEpoch 318/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2321 - mse: 14.4065\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0856 - mse: 15.2601 - val_loss: 26.8127 - val_mse: 21.9862\nEpoch 319/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2264 - mse: 14.4000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0809 - mse: 15.2546 - val_loss: 26.8088 - val_mse: 21.9815\nEpoch 320/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2207 - mse: 14.3934\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0763 - mse: 15.2490 - val_loss: 26.8049 - val_mse: 21.9768\nEpoch 321/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 19.2150 - mse: 14.3869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0716 - mse: 15.2435 - val_loss: 26.8011 - val_mse: 21.9721\nEpoch 322/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2093 - mse: 14.3803\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0669 - mse: 15.2380 - val_loss: 26.7973 - val_mse: 21.9674\nEpoch 323/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.2036 - mse: 14.3737\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0623 - mse: 15.2324 - val_loss: 26.7936 - val_mse: 21.9628\nEpoch 324/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1979 - mse: 14.3671\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0576 - mse: 15.2269 - val_loss: 26.7898 - val_mse: 21.9581\nEpoch 325/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1922 - mse: 14.3605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0530 - mse: 15.2213 - val_loss: 26.7861 - val_mse: 21.9535\nEpoch 326/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19.1864 - mse: 14.3538\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0483 - mse: 15.2158 - val_loss: 26.7825 - val_mse: 21.9490\nEpoch 327/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19.1807 - mse: 14.3472\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0437 - mse: 15.2103 - val_loss: 26.7789 - val_mse: 21.9444\nEpoch 328/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1750 - mse: 14.3406\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0391 - mse: 15.2047 - val_loss: 26.7753 - val_mse: 21.9399\nEpoch 329/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1693 - mse: 14.3339\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0344 - mse: 15.1992 - val_loss: 26.7717 - val_mse: 21.9354\nEpoch 330/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1635 - mse: 14.3272\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0298 - mse: 15.1936 - val_loss: 26.7682 - val_mse: 21.9309\nEpoch 331/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1578 - mse: 14.3205\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0252 - mse: 15.1881 - val_loss: 26.7647 - val_mse: 21.9265\nEpoch 332/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1520 - mse: 14.3138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0206 - mse: 15.1825 - val_loss: 26.7612 - val_mse: 21.9220\nEpoch 333/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1463 - mse: 14.3071\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0161 - mse: 15.1770 - val_loss: 26.7578 - val_mse: 21.9177\nEpoch 334/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1405 - mse: 14.3004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0115 - mse: 15.1714 - val_loss: 26.7544 - val_mse: 21.9133\nEpoch 335/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1348 - mse: 14.2937\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0069 - mse: 15.1659 - val_loss: 26.7511 - val_mse: 21.9090\nEpoch 336/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1291 - mse: 14.2870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.0024 - mse: 15.1603 - val_loss: 26.7477 - val_mse: 21.9046\nEpoch 337/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1233 - mse: 14.2802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9978 - mse: 15.1548 - val_loss: 26.7445 - val_mse: 21.9004\nEpoch 338/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1176 - mse: 14.2735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9933 - mse: 15.1493 - val_loss: 26.7412 - val_mse: 21.8961\nEpoch 339/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.1118 - mse: 14.2667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9888 - mse: 15.1437 - val_loss: 26.7380 - val_mse: 21.8919\nEpoch 340/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19.1060 - mse: 14.2600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9842 - mse: 15.1382 - val_loss: 26.7348 - val_mse: 21.8877\nEpoch 341/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19.1003 - mse: 14.2532\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9797 - mse: 15.1327 - val_loss: 26.7317 - val_mse: 21.8835\nEpoch 342/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0946 - mse: 14.2464\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9752 - mse: 15.1272 - val_loss: 26.7286 - val_mse: 21.8794\nEpoch 343/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0888 - mse: 14.2397\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9707 - mse: 15.1216 - val_loss: 26.7255 - val_mse: 21.8753\nEpoch 344/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0831 - mse: 14.2329\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9663 - mse: 15.1161 - val_loss: 26.7225 - val_mse: 21.8713\nEpoch 345/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0773 - mse: 14.2261\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9618 - mse: 15.1106 - val_loss: 26.7195 - val_mse: 21.8672\nEpoch 346/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0716 - mse: 14.2193\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9573 - mse: 15.1051 - val_loss: 26.7165 - val_mse: 21.8632\nEpoch 347/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0658 - mse: 14.2125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9529 - mse: 15.0996 - val_loss: 26.7136 - val_mse: 21.8592\nEpoch 348/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0601 - mse: 14.2057\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9485 - mse: 15.0942 - val_loss: 26.7107 - val_mse: 21.8553\nEpoch 349/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0544 - mse: 14.1989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9440 - mse: 15.0887 - val_loss: 26.7079 - val_mse: 21.8514\nEpoch 350/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 19.0486 - mse: 14.1922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9396 - mse: 15.0832 - val_loss: 26.7051 - val_mse: 21.8475\nEpoch 351/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0429 - mse: 14.1854\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9352 - mse: 15.0778 - val_loss: 26.7023 - val_mse: 21.8437\nEpoch 352/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0372 - mse: 14.1786\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9308 - mse: 15.0723 - val_loss: 26.6996 - val_mse: 21.8399\nEpoch 353/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0315 - mse: 14.1718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9265 - mse: 15.0669 - val_loss: 26.6969 - val_mse: 21.8361\nEpoch 354/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0257 - mse: 14.1650\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9221 - mse: 15.0614 - val_loss: 26.6942 - val_mse: 21.8324\nEpoch 355/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0200 - mse: 14.1582\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9178 - mse: 15.0560 - val_loss: 26.6916 - val_mse: 21.8287\nEpoch 356/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0143 - mse: 14.1514\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9134 - mse: 15.0506 - val_loss: 26.6890 - val_mse: 21.8250\nEpoch 357/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0086 - mse: 14.1446\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9091 - mse: 15.0452 - val_loss: 26.6864 - val_mse: 21.8214\nEpoch 358/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 19.0029 - mse: 14.1379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9048 - mse: 15.0398 - val_loss: 26.6839 - val_mse: 21.8178\nEpoch 359/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9972 - mse: 14.1311\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.9005 - mse: 15.0344 - val_loss: 26.6814 - val_mse: 21.8142\nEpoch 360/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9915 - mse: 14.1243\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8962 - mse: 15.0290 - val_loss: 26.6790 - val_mse: 21.8107\nEpoch 361/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9859 - mse: 14.1175\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8919 - mse: 15.0236 - val_loss: 26.6766 - val_mse: 21.8072\nEpoch 362/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9802 - mse: 14.1108\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8877 - mse: 15.0183 - val_loss: 26.6742 - val_mse: 21.8037\nEpoch 363/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9745 - mse: 14.1040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8834 - mse: 15.0130 - val_loss: 26.6719 - val_mse: 21.8003\nEpoch 364/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9689 - mse: 14.0973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8792 - mse: 15.0076 - val_loss: 26.6696 - val_mse: 21.7969\nEpoch 365/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9632 - mse: 14.0905\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8750 - mse: 15.0023 - val_loss: 26.6673 - val_mse: 21.7935\nEpoch 366/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.9576 - mse: 14.0838\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8707 - mse: 14.9970 - val_loss: 26.6651 - val_mse: 21.7902\nEpoch 367/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9519 - mse: 14.0771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8666 - mse: 14.9917 - val_loss: 26.6629 - val_mse: 21.7869\nEpoch 368/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9463 - mse: 14.0703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8624 - mse: 14.9864 - val_loss: 26.6608 - val_mse: 21.7837\nEpoch 369/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9407 - mse: 14.0636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8582 - mse: 14.9812 - val_loss: 26.6586 - val_mse: 21.7805\nEpoch 370/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9351 - mse: 14.0569\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8541 - mse: 14.9759 - val_loss: 26.6566 - val_mse: 21.7773\nEpoch 371/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9295 - mse: 14.0502\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8499 - mse: 14.9707 - val_loss: 26.6545 - val_mse: 21.7741\nEpoch 372/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9239 - mse: 14.0435\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8458 - mse: 14.9654 - val_loss: 26.6525 - val_mse: 21.7710\nEpoch 373/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9184 - mse: 14.0369\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8417 - mse: 14.9602 - val_loss: 26.6505 - val_mse: 21.7679\nEpoch 374/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9128 - mse: 14.0302\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8376 - mse: 14.9550 - val_loss: 26.6486 - val_mse: 21.7649\nEpoch 375/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9072 - mse: 14.0235\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8335 - mse: 14.9498 - val_loss: 26.6467 - val_mse: 21.7619\nEpoch 376/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.9017 - mse: 14.0169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8295 - mse: 14.9447 - val_loss: 26.6448 - val_mse: 21.7589\nEpoch 377/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8962 - mse: 14.0102\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8254 - mse: 14.9395 - val_loss: 26.6430 - val_mse: 21.7560\nEpoch 378/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8906 - mse: 14.0036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8214 - mse: 14.9344 - val_loss: 26.6412 - val_mse: 21.7531\nEpoch 379/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 18.8851 - mse: 13.9970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8174 - mse: 14.9293 - val_loss: 26.6394 - val_mse: 21.7502\nEpoch 380/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8796 - mse: 13.9904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8134 - mse: 14.9242 - val_loss: 26.6377 - val_mse: 21.7474\nEpoch 381/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8742 - mse: 13.9838\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8094 - mse: 14.9191 - val_loss: 26.6360 - val_mse: 21.7446\nEpoch 382/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8687 - mse: 13.9773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8054 - mse: 14.9140 - val_loss: 26.6343 - val_mse: 21.7418\nEpoch 383/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8632 - mse: 13.9707\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.8014 - mse: 14.9089 - val_loss: 26.6327 - val_mse: 21.7391\nEpoch 384/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.8578 - mse: 13.9642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7975 - mse: 14.9039 - val_loss: 26.6311 - val_mse: 21.7364\nEpoch 385/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.8524 - mse: 13.9576\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7936 - mse: 14.8989 - val_loss: 26.6296 - val_mse: 21.7337\nEpoch 386/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8470 - mse: 13.9511\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7896 - mse: 14.8938 - val_loss: 26.6280 - val_mse: 21.7311\nEpoch 387/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8415 - mse: 13.9446\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7857 - mse: 14.8888 - val_loss: 26.6265 - val_mse: 21.7285\nEpoch 388/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8362 - mse: 13.9381\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7819 - mse: 14.8839 - val_loss: 26.6251 - val_mse: 21.7260\nEpoch 389/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8308 - mse: 13.9317\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7780 - mse: 14.8789 - val_loss: 26.6236 - val_mse: 21.7234\nEpoch 390/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8254 - mse: 13.9252\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7742 - mse: 14.8740 - val_loss: 26.6222 - val_mse: 21.7209\nEpoch 391/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8201 - mse: 13.9188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7703 - mse: 14.8690 - val_loss: 26.6209 - val_mse: 21.7185\nEpoch 392/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8148 - mse: 13.9124\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7665 - mse: 14.8641 - val_loss: 26.6195 - val_mse: 21.7161\nEpoch 393/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8094 - mse: 13.9059\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7627 - mse: 14.8592 - val_loss: 26.6182 - val_mse: 21.7137\nEpoch 394/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.8041 - mse: 13.8996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7589 - mse: 14.8544 - val_loss: 26.6170 - val_mse: 21.7113\nEpoch 395/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7989 - mse: 13.8932\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7551 - mse: 14.8495 - val_loss: 26.6157 - val_mse: 21.7090\nEpoch 396/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7936 - mse: 13.8868\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7514 - mse: 14.8447 - val_loss: 26.6145 - val_mse: 21.7067\nEpoch 397/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7883 - mse: 13.8805\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7476 - mse: 14.8399 - val_loss: 26.6133 - val_mse: 21.7044\nEpoch 398/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7831 - mse: 13.8742\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7439 - mse: 14.8351 - val_loss: 26.6122 - val_mse: 21.7022\nEpoch 399/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7779 - mse: 13.8679\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7402 - mse: 14.8303 - val_loss: 26.6110 - val_mse: 21.7000\nEpoch 400/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7727 - mse: 13.8616\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7365 - mse: 14.8255 - val_loss: 26.6099 - val_mse: 21.6978\nEpoch 401/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7675 - mse: 13.8554\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7328 - mse: 14.8208 - val_loss: 26.6089 - val_mse: 21.6957\nEpoch 402/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7623 - mse: 13.8491\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7292 - mse: 14.8160 - val_loss: 26.6078 - val_mse: 21.6936\nEpoch 403/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7572 - mse: 13.8429\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7255 - mse: 14.8113 - val_loss: 26.6068 - val_mse: 21.6915\nEpoch 404/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7520 - mse: 13.8367\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7219 - mse: 14.8066 - val_loss: 26.6059 - val_mse: 21.6895\nEpoch 405/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7469 - mse: 13.8305\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7183 - mse: 14.8020 - val_loss: 26.6049 - val_mse: 21.6875\nEpoch 406/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7418 - mse: 13.8244\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7147 - mse: 14.7973 - val_loss: 26.6040 - val_mse: 21.6855\nEpoch 407/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7367 - mse: 13.8182\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7111 - mse: 14.7927 - val_loss: 26.6031 - val_mse: 21.6835\nEpoch 408/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7317 - mse: 13.8121\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.7076 - mse: 14.7881 - val_loss: 26.6022 - val_mse: 21.6816\nEpoch 409/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7266 - mse: 13.8060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7040 - mse: 14.7835 - val_loss: 26.6014 - val_mse: 21.6797\nEpoch 410/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7216 - mse: 13.8000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7005 - mse: 14.7789 - val_loss: 26.6005 - val_mse: 21.6779\nEpoch 411/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7166 - mse: 13.7939\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6970 - mse: 14.7743 - val_loss: 26.5997 - val_mse: 21.6760\nEpoch 412/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7116 - mse: 13.7879\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6935 - mse: 14.7698 - val_loss: 26.5990 - val_mse: 21.6742\nEpoch 413/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7066 - mse: 13.7819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6900 - mse: 14.7653 - val_loss: 26.5982 - val_mse: 21.6725\nEpoch 414/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.7016 - mse: 13.7759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6865 - mse: 14.7608 - val_loss: 26.5975 - val_mse: 21.6707\nEpoch 415/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6967 - mse: 13.7699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6831 - mse: 14.7563 - val_loss: 26.5968 - val_mse: 21.6690\nEpoch 416/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6918 - mse: 13.7640\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6796 - mse: 14.7519 - val_loss: 26.5961 - val_mse: 21.6673\nEpoch 417/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6869 - mse: 13.7580\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6762 - mse: 14.7474 - val_loss: 26.5955 - val_mse: 21.6656\nEpoch 418/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6820 - mse: 13.7521\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6728 - mse: 14.7430 - val_loss: 26.5949 - val_mse: 21.6640\nEpoch 419/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6771 - mse: 13.7462\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6694 - mse: 14.7386 - val_loss: 26.5943 - val_mse: 21.6624\nEpoch 420/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6723 - mse: 13.7404\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6661 - mse: 14.7342 - val_loss: 26.5937 - val_mse: 21.6608\nEpoch 421/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6674 - mse: 13.7345\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6627 - mse: 14.7299 - val_loss: 26.5931 - val_mse: 21.6592\nEpoch 422/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6626 - mse: 13.7287\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6594 - mse: 14.7255 - val_loss: 26.5926 - val_mse: 21.6577\nEpoch 423/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6578 - mse: 13.7229\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6560 - mse: 14.7212 - val_loss: 26.5921 - val_mse: 21.6562\nEpoch 424/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6530 - mse: 13.7172\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6527 - mse: 14.7169 - val_loss: 26.5916 - val_mse: 21.6547\nEpoch 425/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6483 - mse: 13.7114\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6494 - mse: 14.7126 - val_loss: 26.5911 - val_mse: 21.6533\nEpoch 426/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.6436 - mse: 13.7057\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6462 - mse: 14.7084 - val_loss: 26.5907 - val_mse: 21.6518\nEpoch 427/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6388 - mse: 13.7000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6429 - mse: 14.7041 - val_loss: 26.5902 - val_mse: 21.6504\nEpoch 428/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6341 - mse: 13.6943\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6397 - mse: 14.6999 - val_loss: 26.5898 - val_mse: 21.6491\nEpoch 429/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6295 - mse: 13.6887\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6364 - mse: 14.6957 - val_loss: 26.5894 - val_mse: 21.6477\nEpoch 430/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6248 - mse: 13.6831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6332 - mse: 14.6915 - val_loss: 26.5891 - val_mse: 21.6464\nEpoch 431/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6202 - mse: 13.6775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6300 - mse: 14.6874 - val_loss: 26.5887 - val_mse: 21.6451\nEpoch 432/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6155 - mse: 13.6719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6269 - mse: 14.6833 - val_loss: 26.5884 - val_mse: 21.6438\nEpoch 433/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6109 - mse: 13.6663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6237 - mse: 14.6791 - val_loss: 26.5881 - val_mse: 21.6425\nEpoch 434/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6063 - mse: 13.6608\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6205 - mse: 14.6750 - val_loss: 26.5878 - val_mse: 21.6413\nEpoch 435/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.6018 - mse: 13.6553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6174 - mse: 14.6710 - val_loss: 26.5875 - val_mse: 21.6401\nEpoch 436/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5972 - mse: 13.6498\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6143 - mse: 14.6669 - val_loss: 26.5872 - val_mse: 21.6389\nEpoch 437/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5927 - mse: 13.6443\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6112 - mse: 14.6629 - val_loss: 26.5870 - val_mse: 21.6377\nEpoch 438/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5882 - mse: 13.6389\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6081 - mse: 14.6588 - val_loss: 26.5868 - val_mse: 21.6365\nEpoch 439/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5837 - mse: 13.6335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6050 - mse: 14.6548 - val_loss: 26.5866 - val_mse: 21.6354\nEpoch 440/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5792 - mse: 13.6281\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.6020 - mse: 14.6509 - val_loss: 26.5864 - val_mse: 21.6343\nEpoch 441/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5748 - mse: 13.6227\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5989 - mse: 14.6469 - val_loss: 26.5862 - val_mse: 21.6332\nEpoch 442/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5703 - mse: 13.6174\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5959 - mse: 14.6430 - val_loss: 26.5860 - val_mse: 21.6322\nEpoch 443/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5659 - mse: 13.6120\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5929 - mse: 14.6391 - val_loss: 26.5859 - val_mse: 21.6311\nEpoch 444/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5615 - mse: 13.6067\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5899 - mse: 14.6352 - val_loss: 26.5858 - val_mse: 21.6301\nEpoch 445/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.5572 - mse: 13.6015\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5869 - mse: 14.6313 - val_loss: 26.5856 - val_mse: 21.6291\nEpoch 446/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5528 - mse: 13.5962\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5839 - mse: 14.6274 - val_loss: 26.5855 - val_mse: 21.6281\nEpoch 447/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5485 - mse: 13.5910\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5810 - mse: 14.6236 - val_loss: 26.5855 - val_mse: 21.6271\nEpoch 448/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5441 - mse: 13.5858\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5781 - mse: 14.6198 - val_loss: 26.5854 - val_mse: 21.6262\nEpoch 449/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5398 - mse: 13.5806\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5751 - mse: 14.6160 - val_loss: 26.5853 - val_mse: 21.6252\nEpoch 450/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5356 - mse: 13.5755\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5722 - mse: 14.6122 - val_loss: 26.5853 - val_mse: 21.6243\nEpoch 451/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5313 - mse: 13.5703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5693 - mse: 14.6084 - val_loss: 26.5852 - val_mse: 21.6234\nEpoch 452/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5271 - mse: 13.5652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5665 - mse: 14.6047 - val_loss: 26.5852 - val_mse: 21.6226\nEpoch 453/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5228 - mse: 13.5602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5636 - mse: 14.6010 - val_loss: 26.5852 - val_mse: 21.6217\nEpoch 454/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5186 - mse: 13.5551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5608 - mse: 14.5973 - val_loss: 26.5852 - val_mse: 21.6209\nEpoch 455/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5144 - mse: 13.5501\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5579 - mse: 14.5936 - val_loss: 26.5852 - val_mse: 21.6200\nEpoch 456/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5103 - mse: 13.5451\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5551 - mse: 14.5900 - val_loss: 26.5853 - val_mse: 21.6192\nEpoch 457/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5061 - mse: 13.5401\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5523 - mse: 14.5863 - val_loss: 26.5853 - val_mse: 21.6184\nEpoch 458/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.5020 - mse: 13.5351\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5495 - mse: 14.5827 - val_loss: 26.5854 - val_mse: 21.6177\nEpoch 459/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4979 - mse: 13.5302\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5467 - mse: 14.5791 - val_loss: 26.5854 - val_mse: 21.6169\nEpoch 460/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4938 - mse: 13.5253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5440 - mse: 14.5755 - val_loss: 26.5855 - val_mse: 21.6162\nEpoch 461/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4897 - mse: 13.5204\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5412 - mse: 14.5720 - val_loss: 26.5856 - val_mse: 21.6154\nEpoch 462/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4857 - mse: 13.5155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5385 - mse: 14.5684 - val_loss: 26.5857 - val_mse: 21.6147\nEpoch 463/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4816 - mse: 13.5107\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5358 - mse: 14.5649 - val_loss: 26.5858 - val_mse: 21.6140\nEpoch 464/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4776 - mse: 13.5059\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5330 - mse: 14.5614 - val_loss: 26.5859 - val_mse: 21.6134\nEpoch 465/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4736 - mse: 13.5011\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5304 - mse: 14.5579 - val_loss: 26.5860 - val_mse: 21.6127\nEpoch 466/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4696 - mse: 13.4963\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.5277 - mse: 14.5544 - val_loss: 26.5861 - val_mse: 21.6120\nEpoch 467/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.4657 - mse: 13.4916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5250 - mse: 14.5510 - val_loss: 26.5863 - val_mse: 21.6114\nEpoch 468/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4617 - mse: 13.4869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5224 - mse: 14.5476 - val_loss: 26.5864 - val_mse: 21.6108\nEpoch 469/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4578 - mse: 13.4822\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5197 - mse: 14.5442 - val_loss: 26.5866 - val_mse: 21.6102\nEpoch 470/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4539 - mse: 13.4775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5171 - mse: 14.5408 - val_loss: 26.5867 - val_mse: 21.6096\nEpoch 471/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4500 - mse: 13.4728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5145 - mse: 14.5374 - val_loss: 26.5869 - val_mse: 21.6090\nEpoch 472/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4461 - mse: 13.4682\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5119 - mse: 14.5340 - val_loss: 26.5871 - val_mse: 21.6084\nEpoch 473/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4423 - mse: 13.4636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5093 - mse: 14.5307 - val_loss: 26.5873 - val_mse: 21.6079\nEpoch 474/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4384 - mse: 13.4590\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5067 - mse: 14.5274 - val_loss: 26.5875 - val_mse: 21.6073\nEpoch 475/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4346 - mse: 13.4545\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5042 - mse: 14.5241 - val_loss: 26.5877 - val_mse: 21.6068\nEpoch 476/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.4308 - mse: 13.4500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.5016 - mse: 14.5208 - val_loss: 26.5879 - val_mse: 21.6063\nEpoch 477/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4270 - mse: 13.4454\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4991 - mse: 14.5176 - val_loss: 26.5881 - val_mse: 21.6058\nEpoch 478/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4233 - mse: 13.4410\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4966 - mse: 14.5143 - val_loss: 26.5883 - val_mse: 21.6053\nEpoch 479/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4195 - mse: 13.4365\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4941 - mse: 14.5111 - val_loss: 26.5886 - val_mse: 21.6048\nEpoch 480/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4158 - mse: 13.4321\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4916 - mse: 14.5079 - val_loss: 26.5888 - val_mse: 21.6043\nEpoch 481/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4121 - mse: 13.4276\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4891 - mse: 14.5047 - val_loss: 26.5890 - val_mse: 21.6039\nEpoch 482/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4084 - mse: 13.4233\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4866 - mse: 14.5016 - val_loss: 26.5893 - val_mse: 21.6034\nEpoch 483/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4047 - mse: 13.4189\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4842 - mse: 14.4984 - val_loss: 26.5895 - val_mse: 21.6030\nEpoch 484/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.4011 - mse: 13.4145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4817 - mse: 14.4953 - val_loss: 26.5898 - val_mse: 21.6026\nEpoch 485/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3974 - mse: 13.4102\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4793 - mse: 14.4922 - val_loss: 26.5900 - val_mse: 21.6022\nEpoch 486/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3938 - mse: 13.4059\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4769 - mse: 14.4891 - val_loss: 26.5903 - val_mse: 21.6018\nEpoch 487/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3902 - mse: 13.4016\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4744 - mse: 14.4860 - val_loss: 26.5906 - val_mse: 21.6014\nEpoch 488/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3866 - mse: 13.3974\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4721 - mse: 14.4829 - val_loss: 26.5909 - val_mse: 21.6010\nEpoch 489/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3830 - mse: 13.3931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4697 - mse: 14.4799 - val_loss: 26.5911 - val_mse: 21.6006\nEpoch 490/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3795 - mse: 13.3889\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4673 - mse: 14.4769 - val_loss: 26.5914 - val_mse: 21.6002\nEpoch 491/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.3759 - mse: 13.3847\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4649 - mse: 14.4739 - val_loss: 26.5917 - val_mse: 21.5999\nEpoch 492/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.3724 - mse: 13.3806\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4626 - mse: 14.4709 - val_loss: 26.5920 - val_mse: 21.5995\nEpoch 493/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3689 - mse: 13.3764\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4603 - mse: 14.4679 - val_loss: 26.5923 - val_mse: 21.5992\nEpoch 494/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3654 - mse: 13.3723\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4579 - mse: 14.4649 - val_loss: 26.5926 - val_mse: 21.5989\nEpoch 495/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3619 - mse: 13.3682\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.4556 - mse: 14.4620 - val_loss: 26.5929 - val_mse: 21.5986\nEpoch 496/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3585 - mse: 13.3641\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4533 - mse: 14.4591 - val_loss: 26.5932 - val_mse: 21.5982\nEpoch 497/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 18.3550 - mse: 13.3601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4510 - mse: 14.4562 - val_loss: 26.5935 - val_mse: 21.5979\nEpoch 498/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.3516 - mse: 13.3560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4487 - mse: 14.4533 - val_loss: 26.5938 - val_mse: 21.5976\nEpoch 499/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.3482 - mse: 13.3520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4465 - mse: 14.4504 - val_loss: 26.5941 - val_mse: 21.5974\nEpoch 500/500\n\n\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 18.3448 - mse: 13.3481\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.4442 - mse: 14.4475 - val_loss: 26.5945 - val_mse: 21.5971\n\n\n\nplot(historia, smooth = FALSE)\n\n\n\n\n\n\n\n\nLa afinación nos da mejores resultados:\n\npreds_2 &lt;- predict(modelo_red_2, x_grasa_pr) \n\n4/4 - 0s - 12ms/step\n\ng_2 &lt;- tibble(preds_2 = preds_2[, 1], y = y_grasa_pr) |&gt; \n  ggplot(aes(x = preds_2, y = y)) + \n  geom_point() + \n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  coord_obs_pred()\ng_1 + g_2\n\n\n\n\n\n\n\n\n\n\nResumen\nEjercicio: En nuestra referencia The Deep Learning Book, puedes revisar la sección 11.4.1 (afinación manual) y 11.4.2 (afinación por búsqueda automática). Explica qué efecto tienen sobre sesgo y sobreajuste los siguientes parámetros:\n\nNúmero de capas\nNúmero de unidades por capa oculta\nRegularización L2 de los pesos\nTasa de aprendizaje (ojo, este parámetro es especial)\n\nEn la parte 2 de redes neuronales veremos más estrategias de regularización, y cómo agregar estructura a redes adaptada a problemas particulares. Notamos para por el momento que las redes neuronales con solamente capas conexas no han sido particularmente exitosas para ningún problema particular. Típicamente requieren una gran cantidad de datos y entrenarlas es más difícil que otros métodos. Sin embargo, la historia es diferente cuando escogemos adecuadamente la arquitectura de la red para un problema dado, es decir, qué unidades están conectadas y bajo qué patrones. En estos casos, las conexiones pueden ser relativamente pocas comparadas con redes totalmente conexas con el mismo tamaño de unidades, y si la estructura es correcta, no incurrimos en mucho sesgo. Ejemplos son procesamiento de imágenes, audio, o lenguaje natural.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes neuronales (intro)</span>"
    ]
  },
  {
    "objectID": "81-apendice-descenso.html",
    "href": "81-apendice-descenso.html",
    "title": "Apéndice A — Apéndice 1: descenso en gradiente",
    "section": "",
    "text": "Ejemplo\nSi tenemos\nlibrary(tidyverse)\nh &lt;- function(x) x^2 + (x - 2)^2 - log(x^2 + 1)\nCalculamos (a mano):\nh_deriv &lt;- function(x) 2 * x + 2 * (x - 2) - 2*x/(x^2 + 1)\nAhora iteramos con \\(\\eta = 0.4\\) y valor inicial \\(z_0=5\\)\nz_0 &lt;- 5\neta &lt;- 0.4\ndescenso &lt;- function(n, z_0, eta, h_deriv){\n  z &lt;- matrix(0,n, length(z_0))\n  z[1, ] &lt;- z_0\n  for(i in 1:(n-1)){\n    # paso de descenso\n    z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ])\n  }\n  z\n}\nz &lt;- descenso(20, z_0, eta, h_deriv)\nz\n\n            [,1]\n [1,]  5.0000000\n [2,] -1.2461538\n [3,]  1.9571861\n [4,]  0.7498212\n [5,]  1.5340816\n [6,]  1.0455267\n [7,]  1.3722879\n [8,]  1.1573987\n [9,]  1.3013251\n[10,]  1.2057209\n[11,]  1.2696685\n[12,]  1.2270627\n[13,]  1.2555319\n[14,]  1.2365431\n[15,]  1.2492245\n[16,]  1.2407623\n[17,]  1.2464122\n[18,]  1.2426413\n[19,]  1.2451587\n[20,]  1.2434784\nY vemos que estamos cerca de la convergencia. Podemos graficar:\ndat_iteraciones &lt;- tibble(iteracion = 1:nrow(z), \n                              x = z[, 1], y = h(z[, 1]))\nlibrary(gganimate)\ncurva &lt;- ggplot(tibble(x = seq(-4, 5, 0.1)), aes(x = x)) + stat_function(fun = h) +\n     xlim(c(-4, 5))\ndescenso_g &lt;- curva +\n    geom_point(data = dat_iteraciones, aes(x = x, y = y), col = \"red\", size = 3) +\n    transition_time(iteracion) + \n    theme_minimal(base_size = 20)\nif(FALSE){\n  animate(descenso_g)\n}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Apéndice 1: descenso en gradiente</span>"
    ]
  },
  {
    "objectID": "81-apendice-descenso.html#cálculo-del-gradiente",
    "href": "81-apendice-descenso.html#cálculo-del-gradiente",
    "title": "Apéndice A — Apéndice 1: descenso en gradiente",
    "section": "A.1 Cálculo del gradiente",
    "text": "A.1 Cálculo del gradiente\nVamos a escribir ahora el algoritmo de descenso en gradiente para regresión lineal. Igual que en los ejemplos anteriores, tenemos que precalcular el gradiente. Una vez que esto esté terminado, escribir la iteración es fácil.\nRecordamos que queremos minimizar (dividiendo entre dos para simplificar más adelante) \\[L(\\beta) = \\frac{1}{2N}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\]\nLa derivada de la suma es la suma de las derivadas, así nos concentramos en derivar uno de los términos\n\\[  u^{(i)}=\\frac{1}{2}(y^{(i)} - f_\\beta(x^{(i)}))^2 \\] Usamos la regla de la cadena para obtener \\[ \\frac{1}{2}\\frac{\\partial}{\\partial \\beta_j} (y^{(i)} - f_\\beta(x^{(i)}))^2 =\n-(y^{(i)} - f_\\beta(x^{(i)})) \\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j}\\]\nAhora recordamos que \\[f_{\\beta} (x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\\]\nY vemos que tenemos dos casos. Si \\(j=0\\),\n\\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_0} = 1\\] y si \\(j=1,2,\\ldots, p\\) entonces\n\\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j} = x_j^{(i)}\\]\nEntonces, si ponemos \\(u^{(i)}=\\frac{1}{2}(y^{(i)} - f_\\beta(x^{(i)}))^2\\):\n\\[\\frac{\\partial u^{(i)}}{\\partial \\beta_0} = -(y^{(i)} - f_\\beta(x^{(i)}))\\] y\n\\[\\frac{\\partial u^{(i)}}{\\partial \\beta_j} = - x_j^{(i)}(y^{(i)} - f_\\beta(x^{(i)}))\\]\nY sumando todos los términos (uno para cada caso de entrenamiento):\n\n\n\n\n\n\nGradiente para regresión lineal\n\n\n\nSea \\(e^{(i)} = y_{(i)} - f_{\\beta} (x^{(i)})\\). Entonces \\[\\begin{equation}\n  \\frac{\\partial L(\\beta)}{\\partial \\beta_0} = - \\frac{1}{N}\\sum_{i=1}^N e^{(i)}\n  (\\#eq:grad1)\n\\end{equation}\\] \\[\\begin{equation}\n  \\frac{\\partial L(\\beta)}{\\partial \\beta_j} = - \\frac{1}{N}\\sum_{i=1}^N x_j^{(i)}e^{(i)}\n  (\\#eq:grad2)\n\\end{equation}\\] para \\(j=1,2,\\ldots, p\\).\n\n\nNótese que cada punto de entrenamiento contribuye al cálculo del gradiente - la contribución es la dirección de descenso de error para ese punto particular de entrenamiento. Nos movemos entonces en una dirección promedio, para intentar hacer el error total lo más chico posible.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Apéndice 1: descenso en gradiente</span>"
    ]
  },
  {
    "objectID": "81-apendice-descenso.html#implementación",
    "href": "81-apendice-descenso.html#implementación",
    "title": "Apéndice A — Apéndice 1: descenso en gradiente",
    "section": "A.2 Implementación",
    "text": "A.2 Implementación\nEn este punto, podemos intentar una implementación simple basada en el código anterior para hacer descenso en gradiente para nuestro problema de regresión (es un buen ejercicio). En lugar de eso, mostraremos cómo usar librerías ahora estándar para hacer esto. En particular usamos keras (con tensorflow), que tienen la ventaja:\n\nEn tensorflow y keras no es necesario calcular las derivadas a mano. Utiliza diferenciación automática, que no es diferenciación numérica ni simbólica: se basa en la regla de la cadena y la codificación explícita de las derivadas de funciones elementales.\n\n\nlibrary(tidymodels)\nlibrary(keras3)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(68821)\n# dividir muestra\ncasas_split &lt;- initial_split(casas |&gt;\n                      select(precio_m2_miles, area_hab_m2, calidad_gral, num_coches), \n                             prop = 0.75)\n# obtener muestra de entrenamiento\ncasas_entrena &lt;- training(casas_split)\ncasas_receta &lt;- recipe(precio_m2_miles ~ ., casas_entrena) \n\n\n# definición de estructura del modelo (regresión lineal)\nx_ent &lt;- casas_receta |&gt; prep() |&gt; juice()  |&gt; select(-precio_m2_miles) |&gt; as.matrix()\ny_ent &lt;- casas_receta |&gt; prep() |&gt; juice() |&gt; pull(precio_m2_miles)\nn_entrena &lt;- nrow(x_ent)\ncrear_modelo &lt;- function(lr = 0.01){\n    modelo_casas &lt;- \n        keras_model_sequential() |&gt;\n        layer_dense(units = 1,        #una sola respuesta,\n            activation = \"linear\",    # combinar variables linealmente\n            kernel_initializer = initializer_constant(0), #inicializamos coeficientes en 0\n            bias_initializer = initializer_constant(0))   #inicializamos ordenada en 0\n    # compilar seleccionando cantidad a minimizar, optimizador y métricas\n    modelo_casas |&gt; compile(\n        loss = \"mean_squared_error\",  # pérdida cuadrática\n        optimizer = optimizer_sgd(learning_rate = lr), # descenso en gradiente\n        metrics = list(\"mean_squared_error\"))\n    modelo_casas\n}\n# tasa de aprendizaje es lr, tenemos que poner una tasa chica (prueba)\nmodelo_casas &lt;- crear_modelo(lr = 0.00001)\n# Ahora iteramos\n# Primero probamos con un número bajo de iteraciones\nhistoria &lt;- modelo_casas |&gt; fit(\n  x_ent,    # x entradas\n  y_ent,    # y salida o target\n  batch_size = nrow(x_ent), # para descenso en gradiente\n  epochs = 20, # número de iteraciones\n  verbose = 0\n)\n\n\nplot(historia, metrics = \"mean_squared_error\", smooth = FALSE) +\n  geom_line()\n\n\n\n\n\n\n\nhistoria$metrics$mean_squared_error |&gt; round(4)\n\n [1] 1.7903 0.7636 0.4549 0.3621 0.3342 0.3258 0.3232 0.3224 0.3222 0.3221\n[11] 0.3220 0.3220 0.3220 0.3219 0.3219 0.3219 0.3218 0.3218 0.3218 0.3217\n\n\nProbamos con más corridas para checar convergencia:\n\n# Agregamos iteraciones: esta historia comienza en los últimos valores de\n# la corrida anterior\nhistoria &lt;- modelo_casas |&gt; fit(\n  as.matrix(x_ent), # x entradas\n  y_ent,            # y salida o target\n  batch_size = nrow(x_ent), # para descenso en gradiente\n  epochs = 1000, # número de iteraciones\n  verbose = 0\n)\n\n\nplot(historia, metrics = \"mean_squared_error\", smooth = FALSE) \n\n\n\n\n\n\n\n\nEl modelo parece todavía ir mejorando. Veamos de todas formas los coeficientes estimados hasta ahora:\n\nkeras3::get_weights(modelo_casas)\n\n[[1]]\n            [,1]\n[1,] 0.007331367\n[2,] 0.016437242\n[3,] 0.004633876\n\n[[2]]\n[1] 0.003028649\n\n\nLa implementación oficial de R es lm, que en general tiene buen desempeño para datos que caben en memoria:\n\nlm(precio_m2_miles ~ area_hab_m2 + calidad_gral + num_coches, \n   data = casas_entrena) |&gt; \n  coef()\n\n (Intercept)  area_hab_m2 calidad_gral   num_coches \n  0.66869194  -0.00449751   0.16807663   0.13115749 \n\n\nDe modo que todavía requerimos más iteraciones para alcanzar convergencia. ¿Por qué la convergencia es tan lenta? En parte, la razón es que las escalas de las variables de entrada son muy diferentes, de modo que es difícil ajustar una tasa de aprendizaje constante que funcione bien. Podemos remediar esto poniendo todas las entradas en la misma escala (normalizando)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Apéndice 1: descenso en gradiente</span>"
    ]
  },
  {
    "objectID": "81-apendice-descenso.html#normalización-de-entradas",
    "href": "81-apendice-descenso.html#normalización-de-entradas",
    "title": "Apéndice A — Apéndice 1: descenso en gradiente",
    "section": "A.3 Normalización de entradas",
    "text": "A.3 Normalización de entradas\nLa convergencia de descenso en gradiente (y también el desempeño numérico para otros algoritmos) puede dificultarse cuando las variables tienen escalas muy diferentes. Esto produce curvaturas altas en la función que queremos minimizar.\nEn este ejemplo simple, una variable tiene desviación estándar 10 y otra 1:\n\nx1 &lt;- rnorm(100, 0, 5) \nx2 &lt;- rnorm(100, 0, 1) +  0.1*x1\ny &lt;- 0*x1 + 0*x2 + rnorm(100, 0, 0.1) \ndat &lt;- tibble(x1, x2,  y)\nrss &lt;- function(beta)  mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) \ngrid_beta &lt;- expand.grid(beta1 = seq(-1, 1, length.out = 50), \n                         beta2 = seq(-1, 1, length.out = 50))\nrss_1 &lt;- apply(grid_beta, 1, rss) \ndat_x &lt;- data.frame(grid_beta, rss_1)\nggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + \n    geom_contour(binwidth = 0.5) +\n    coord_equal() \n\n\n\n\n\n\n\n\nEn algunas direcciones el gradiente es muy grande, y en otras chico. Esto implica que la convergencia puede ser muy lenta en algunas direcciones, puede diverger en otras, y que hay que ajustar el paso \\(\\eta &gt; 0\\) con cuidado, dependiendo de dónde comiencen las iteraciones.\nPor ejemplo, con un tamaño de paso relativamente chico, damos unos saltos grandes al principio y luego avanzamos muy lentamente:\n\ngrad_calc &lt;- function(x_ent, y_ent){\n  # calculamos directamente el gradiente\n  salida_grad &lt;- function(beta){\n    n &lt;- length(y_ent)\n    f_beta &lt;- as.matrix(cbind(1, x_ent)) %*% beta\n    e &lt;- y_ent - f_beta\n    grad_out &lt;- - as.numeric(t(cbind(1, x_ent)) %*% e) / n\n    names(grad_out) &lt;- c('Intercept', colnames(x_ent))\n    grad_out\n  }\n  salida_grad\n}\ngrad_sin_norm &lt;- grad_calc(dat[, 1:2, drop = FALSE], dat$y)\niteraciones &lt;- descenso(10, c(0, -0.25, -0.75), 0.02, grad_sin_norm)\nggplot(dat_x) + \n    geom_contour(aes(x = beta1, y = beta2, z = rss_1), binwidth = 0.5) +\n    coord_equal() +\n  geom_path(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red') +\n  geom_point(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red')\n\n\n\n\n\n\n\n\nSi incrementamos el tamaño de paso observamos también convergencia lenta. En este caso particular, subir más el tamaño de paso puede producir divergencia:\n\niteraciones &lt;- descenso(10, c(0, -0.25, -0.75), 0.07, grad_sin_norm)\nggplot(dat_x) + \n    geom_contour(aes(x = beta1, y = beta2, z = rss_1), binwidth = 0.5) +\n    coord_equal() +\n  geom_path(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red') +\n  geom_point(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red')\n\n\n\n\n\n\n\n\nUna normalización usual es con la media y desviación estándar, donde hacemos, para cada variable de entrada \\(j=1,2,\\ldots, p\\) \\[ x_j^{(i)} = \\frac{ x_j^{(i)} - \\bar{x}_j}{s_j}\\] donde \\[\\bar{x}_j = \\frac{1}{N} \\sum_{i=1}^N x_j^{(i)}\\] \\[s_j = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N (x_j^{(i)}- \\bar{x}_j )^2}\\] es decir, centramos y normalizamos por columna. Otra opción común es restar el mínimo y dividir entre la diferencia del máximo y el mínimo, de modo que las variables resultantes toman valores en \\([0,1]\\).\nEntonces escalamos antes de ajustar:\n\nx1_s = (x1 - mean(x1))/sd(x1)\nx2_s = (x2 - mean(x2))/sd(x2)\ndat &lt;- tibble(x1_s = x1_s, x2_s = x2_s,  y = y)\nrss &lt;- function(beta)  mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) \ngrid_beta &lt;- expand.grid(beta1 = seq(-1, 1, length.out = 50), \n                         beta2 = seq(-1, 1, length.out = 50))\nrss_1 &lt;- apply(grid_beta, 1, rss) \ndat_x &lt;- data.frame(grid_beta, rss_1)\nggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + \n    geom_contour(binwidth = 0.5) +\n    coord_equal() \n\n\n\n\n\n\n\n\nNótese que los coeficientes ajustados serán diferentes a los del caso no normalizado.\nSi normalizamos, obtenemos convergencia más rápida\n\ngrad_sin_norm &lt;- grad_calc(dat[, 1:2, drop = FALSE], dat$y)\niteraciones &lt;- descenso(10, c(0, -0.25, -0.75), 0.5, grad_sin_norm)\nggplot(dat_x) + \n    geom_contour(aes(x = beta1, y = beta2, z = rss_1), binwidth = 0.5) +\n    coord_equal() +\n  geom_path(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red') +\n  geom_point(data = data.frame(iteraciones[, 2:3]), aes(x=X1, y=X2), colour = 'red')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCuando normalizamos antes de ajustar el modelo, las predicciones deben hacerse con entradas normalizadas. La normalización se hace con los mismos valores que se usaron en el entrenamiento (y no recalculando medias y desviaciones estándar con el conjunto de prueba). En cuanto a la forma funcional del predictor \\(f\\), el problema con entradas normalizadas es equivalente al de las entradas no normalizadas. Asegúrate de esto escribiendo cómo correponden los coeficientes de cada modelo normalizado con los coeficientes del modelo no normalizado.\n\n\nSupongamos que el modelo en las variables originales es \\[{f}_\\beta (X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p,\\] Consideramos el modelo con variables estandarizadas \\[{g}_{\\beta^s} (X) = \\beta_0^s + \\beta_1^s Z_1 + \\beta_2^s Z_2 + \\cdots + \\beta_p^s Z_p,\\]\nSustituyendo \\(Z_j = (X_j - \\mu_j)/s_j,\\)\n\\[{g}_{\\beta^s} (X) = (\\beta_0^s - \\sum_{j=1}^p \\beta_j^s \\mu_j/s_j) + \\frac{\\beta_1^s}{s_j} X_1 + \\frac{\\beta_2^s}{s_2} X_2 + \\cdots + \\frac{\\beta_p^s}{s_p} X_p,\\] Y vemos que tiene la misma forma funcional de \\(f_\\beta(X)\\). Si la solución de mínimos cuadrados es única, entonces una vez que ajustemos tenemos que tener \\(\\hat{f}_\\beta(X) = \\hat{g}_{\\beta^s} (X)\\), lo que implica que \\[\\hat{\\beta}_0 = \\hat{\\beta}_0^s -  \\sum_{j=1}^p \\hat{\\beta}_j^s\\mu_j/s_j\\] y \\[\\hat{\\beta}_j = \\hat{\\beta}_j^s/s_j.\\]\nNótese que para pasar del problema estandarizado al no estandarizado simplemente se requiere escalar los coeficientes por la \\(s_j\\) correspondiente.\n\nEjemplo\nRepetimos nuestro modelo, pero normalizando las entradas:\n\n# usamos recipes para este ejemplo, no necesitas usarlo\ncasas_receta &lt;- recipe(precio_m2_miles ~ ., casas_entrena) |&gt;\n  step_normalize(all_predictors()) \ncasas_receta |&gt; summary()\n\n# A tibble: 4 × 4\n  variable        type      role      source  \n  &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 area_hab_m2     &lt;chr [2]&gt; predictor original\n2 calidad_gral    &lt;chr [2]&gt; predictor original\n3 num_coches      &lt;chr [2]&gt; predictor original\n4 precio_m2_miles &lt;chr [2]&gt; outcome   original\n\n\n\nmodelo_lineal &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\ncasas_flujo &lt;- workflow() |&gt;\n  add_recipe(casas_receta) |&gt; \n  add_model(modelo_lineal)\n\n\nlibrary(keras3)\n# definición de estructura del modelo (regresión lineal)\nx_ent_s &lt;-  prep(casas_receta) |&gt; juice() |&gt; select(-precio_m2_miles) |&gt; \n  as.matrix()\najustar_casas &lt;- function(modelo, x, y, n_epochs = 100){\n  ajuste &lt;- modelo |&gt; fit(\n    as.matrix(x), y,\n    batch_size = nrow(x_ent), # para descenso en gradiente\n    epochs = n_epochs, # número de iteraciones\n    verbose = 0) |&gt; as_tibble()\n  ajuste\n}\nmodelo_casas_ns &lt;- crear_modelo(0.00001)\nmodelo_casas_s &lt;- crear_modelo(0.2)\nhistoria_s &lt;- ajustar_casas(modelo_casas_s, x_ent_s, y_ent) |&gt;\n  mutate(tipo = \"Estandarizar\")\nhistoria_ns &lt;- ajustar_casas(modelo_casas_ns, x_ent, y_ent) |&gt; \n  mutate(tipo = \"Sin estandarizar\")\nhistoria &lt;- bind_rows(historia_ns, historia_s) |&gt;  filter(metric == \"mean_squared_error\")\nggplot(historia, aes(x = epoch, y = value, colour = tipo)) +\n     geom_line() + geom_point() +scale_x_log10() + scale_y_log10()\n\n\n\n\n\n\n\n\nObservamos que el modelo con datos estandarizados convergió:\n\nkeras3::get_weights(modelo_casas_s)\n\n[[1]]\n            [,1]\n[1,] -0.22045916\n[2,]  0.23149671\n[3,]  0.09673478\n\n[[2]]\n[1] 1.295027\n\ncoef(lm.fit(cbind(1,x_ent_s), y_ent))\n\n              area_hab_m2 calidad_gral   num_coches \n  1.29502679  -0.22045919   0.23149675   0.09673476 \n\n\nMientras que el modelo no estandarizado todavía requiere iteraciones:\n\nkeras3::get_weights(modelo_casas_ns)\n\n[[1]]\n             [,1]\n[1,] 0.0079817399\n[2,] 0.0019486143\n[3,] 0.0005532746\n\n[[2]]\n[1] 0.0003491883\n\ncoef(lm.fit(cbind(1, x_ent), y_ent))\n\n              area_hab_m2 calidad_gral   num_coches \n  0.66869194  -0.00449751   0.16807663   0.13115749",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Apéndice 1: descenso en gradiente</span>"
    ]
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html",
    "href": "82-apendice-descenso-estocastico.html",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "",
    "text": "B.1 Algoritmo de descenso estocástico",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Apéndice 2: Descenso estocástico</span>"
    ]
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#algoritmo-de-descenso-estocástico",
    "href": "82-apendice-descenso-estocastico.html#algoritmo-de-descenso-estocástico",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "",
    "text": "Descenso estocástico\n\n\n\nSeparamos al azar los datos de entrenamiento en \\(n\\) minilotes de tamaño \\(m\\).\n\nPara épocas \\(e =1,2,\\ldots, n_e\\)\n\nCalcular el gradiente sobre el minilote y hacer actualización, sucesivamente para cada uno de los minilotes \\(k=1,2,\\ldots, n/m\\): \\[\\beta_{i+1} = \\beta_{i} - \\eta\\frac{1}{m}\\sum_{j=1}^m \\nabla D^{(k)}_j (\\beta_i)\\] donde \\(D^{(k)}_j (\\beta_i)\\) es la devianza para el \\(j\\)-ésimo caso del minilote \\(k\\).\n\nRepetir para la siguiente época (opcional: reordenar antes al azar los minilotes, para evitar ciclos).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Apéndice 2: Descenso estocástico</span>"
    ]
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#por-qué-usar-descenso-estocástico-por-minilotes",
    "href": "82-apendice-descenso-estocastico.html#por-qué-usar-descenso-estocástico-por-minilotes",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "B.2 ¿Por qué usar descenso estocástico por minilotes?",
    "text": "B.2 ¿Por qué usar descenso estocástico por minilotes?\nLas propiedades importantes de descenso estocástico son:\n\nMuchas veces no es necesario usar todos los datos para encontrar una buena dirección de descenso. Podemos ver la dirección de descenso en gradiente como un valor esperado sobre la muestra de entrenamiento (pues la pérdida es un promedio sobre el conjunto de entrenamiento). Una submuestra (minilote) puede ser suficiente para estimar ese valor esperado, con costo menor de cómputo. Adicionalmente, quizá no es tan buena idea intentar estimar el gradiente con la mejor precisión pues es solamente una dirección de descenso local (así que quizá no da la mejor decisión de a dónde moverse en cada punto). Es mejor hacer iteraciones más rápidas con direcciones estimadas.\nDesde este punto de vista, calcular el gradiente completo para descenso en gradiente es computacionalmente ineficiente. Si el conjunto de entrenamiento es masivo, descenso en gradiente puede no ser factible.\nDesde el punto de vista de sobreajuste, el uso de distintos datos para cada paso evita mínimos sobreajustados o de bajo desempeño que están presentes en la pérdida calculada con todos los datos.\n¿Cuál es el mejor tamaño de minilote? Por un lado, minilotes más grandes nos dan mejores eficiencias en paralelización (multiplicación de matrices), especialmente en GPUs. Por otro lado, con minilotes más grandes puede ser que hagamos trabajo de más, por las razones expuestas en los incisos anteriores, y tengamos menos iteraciones en el mismo tiempo. El mejor punto está entre minilotes demasiado chicos (donde no aprovechamos paralelismo) o demasiado grande (donde hacemos demasiado trabajo por iteración).\n\n4.Una propiedad importante de descenso estocástico en minilotes es que su convergencia no depende del tamaño del conjunto de entrenamiento, es decir, el tiempo de iteración para descenso estocástico no crece con el número de casos totales. Podemos tener obtener buenos ajustes incluso con tamaños muy grandes de conjuntos de entrenamiento (por ejemplo, antes de procesar todos los datos de entrenamiento). Descenso estocástico escala bien en este sentido: el factor limitante es el tamaño de minilote y el número de iteraciones.\n\nEs importante permutar al azar los datos antes de hacer los minibatches, pues órdenes “naturales” en los datos pueden afectar la convergencia. Se ha observado también que permutar los minibatches en cada iteración típicamente acelera la convergencia (si se pueden tener los datos en memoria).\n\n\nEjemplo\nEn el ejemplo anterior nota que las direcciones de descenso de descenso estocástico son muy razonables (punto 1). Nota también que obtenemos una buena aproximación a la solución con menos cómputo (punto 2 - mismo número de iteraciones, pero cada iteración con un minilote).\n\nggplot(filter(dat_dev, iteracion &gt;= 1), \n       aes(x=iteracion, y=dev_ent, colour=algoritmo)) + geom_line() +\n  facet_wrap(~tipo, ncol=1)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Apéndice 2: Descenso estocástico</span>"
    ]
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#escogiendo-la-tasa-de-aprendizaje",
    "href": "82-apendice-descenso-estocastico.html#escogiendo-la-tasa-de-aprendizaje",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "B.3 Escogiendo la tasa de aprendizaje",
    "text": "B.3 Escogiendo la tasa de aprendizaje\nPara escoger la tasa, monitoreamos las curvas de error de entrenamiento y de validación. Si la tasa es muy grande, habrá oscilaciones grandes y muchas veces incrementos grandes en la función objectivo (error de entrenamiento). Algunas oscilaciones suaves no tienen problema -es la naturaleza estocástica del algoritmo. Si la tasa es muy baja, el aprendizaje es lento y podemos quedarnos en un valor demasiado alto.\nConviene monitorear las primeras iteraciones y escoger una tasa más alta que la mejor que tengamos acutalmente, pero no tan alta que cause inestabilidad. Una gráfica como la siguiente es útil. En este ejemplo, incluso podríamos detenernos antes para evitar el sobreajuste de la última parte de las iteraciones:\n\nggplot(filter(dat_dev, algoritmo=='descenso_estocastico'), \n       aes(x=iteracion, y=dev_ent, colour=tipo)) + geom_line() \n\n\n\n\n\n\n\n\nPor ejemplo: tasa demasiado alta:\n\niter_estocastico &lt;- descenso_estocastico(20, z_0, 0.5, minilotes) |&gt;\n  as_tibble() \ndev_ent &lt;- perdida_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_ent$y)\ndev_valid &lt;- perdida_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_valid$y)\ndat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) |&gt;\n   mutate(entrena = apply(iter_estocastico, 1, dev_ent), \n  validacion = apply(iter_estocastico, 1, dev_valid)) |&gt;\n  gather(tipo, devianza, entrena:validacion)\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\nggplot(dat_dev, \n       aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() \n\n\n\n\n\n\n\n\nTasa demasiado chica ( o hacer más iteraciones):\n\niter_estocastico &lt;- descenso_estocastico(20, z_0, 0.001, minilotes) |&gt;\n  as_tibble() \ndev_ent &lt;- perdida_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_ent$y)\ndev_valid &lt;- perdida_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_valid$y)\ndat_dev &lt;- tibble(iteracion = 1:nrow(iter_estocastico)) |&gt;\n   mutate(entrena = apply(iter_estocastico, 1, dev_ent), \n  validacion = apply(iter_estocastico, 1, dev_valid)) |&gt;\n  gather(tipo, devianza, entrena:validacion)\nggplot(dat_dev, \n       aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() \n\n\n\n\n\n\n\n\n\nPara redes neuronales, es importante explorar distintas tasas de aprendizaje, aún cuando no parezca haber oscilaciones grandes o convergencia muy lenta. En algunos casos, si la tasa es demasiado grande, puede ser que el algoritmo llegue a lugares con gradientes cercanos a cero (por ejemplo, por activaciones demasiado grandes) y tenga dificultad para moverse.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Apéndice 2: Descenso estocástico</span>"
    ]
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#mejoras-al-algoritmo-de-descenso-estocástico.",
    "href": "82-apendice-descenso-estocastico.html#mejoras-al-algoritmo-de-descenso-estocástico.",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "B.4 Mejoras al algoritmo de descenso estocástico.",
    "text": "B.4 Mejoras al algoritmo de descenso estocástico.\n\nB.4.1 Decaimiento de tasa de aprendizaje\nHay muchos algoritmos derivados de descenso estocástico. La primera mejora consiste en reducir gradualmente la tasa de aprendizaje para aprender rápido al principio, pero filtrar el ruido de la estimación de minilotes más adelante en las iteraciones y permitir que el algoritmo se asiente en un mínimo.\n\ndescenso_estocastico &lt;- function(n_epocas, z_0, eta, minilotes, decaimiento = 0.0){\n  #minilotes es una lista\n  m &lt;- length(minilotes)\n  z &lt;- matrix(0, m*n_epocas, length(z_0))\n  z[1, ] &lt;- z_0\n  for(i in 1:(m*n_epocas-1)){\n    k &lt;- i %% m + 1\n    if(i %% m == 0){\n      #comenzar nueva época y reordenar minilotes al azar\n      minilotes &lt;- minilotes[sample(1:m, m)]\n    }\n    h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y)\n    z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ])\n    eta &lt;- eta*(1/(1+decaimiento*i))\n  }\n  colnames(z) &lt;- names(z_0)\n  z\n}\n\nY ahora vemos qué pasa con decaimiento:\n\niter_estocastico &lt;- descenso_estocastico(10, z_0, 0.1, \n                                         minilotes, decaimiento = 1e-3) |&gt;\n  as_tibble() |&gt; rename(beta_0 = Intercept, beta_1 = x_1, beta_2 = x_2, beta_3 = x_3)\ndev_ent &lt;- perdida_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_ent$y)\ndev_valid &lt;- perdida_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_valid$y)\ndat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) |&gt;\n   mutate(entrena = apply(iter_estocastico, 1, dev_ent), \n  validacion = apply(iter_estocastico, 1, dev_valid)) |&gt;\n  gather(tipo, devianza, entrena:validacion)\nggplot(filter(dat_dev, iteracion&gt;1), \n       aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() \n\n\n\n\n\n\n\n\nPara los primeros dos parámetros, las iteraciones se ven:\n\nggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() +\n  geom_point() +\n  geom_path(data = iter_estocastico, colour ='red', alpha=0.2) +\n  geom_point(data = iter_estocastico, colour ='red', alpha=0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTasa de aprendizaje\n\n\n\nLa tasa de aprendizaje es uno de los parámetros en redes neuronales más importantes de afinar. Generalmente se empieza con una tasa de aprendizaje con un valor bajo (0.01, o 0.1), pero es necesario experimentar.\n\nUn valor muy alto puede provocar oscilaciones muy fuertes en la pérdida\nUn valor alto también puede provocar que el algoritmo se detenga en lugar con función pérdida alta (sobreajusta rápidamente).\nUn valor demasiado bajo produce convergencia lenta.\n\n\n\n\n\nB.4.2 Momento\nTambién es posible utilizar una idea adicional que acelera la convergencia. La idea es que muchas veces la aleatoriedad del algoritmo puede producir iteraciones en direcciones que no son tan buenas (pues la estimación del gradiente es mala). Esto es parte del algoritmo. Sin embargo, si en varias iteraciones hemos observado movimientos en direcciones consistentes, quizá deberíamos movernos en esas direcciones consistentes, y reducir el peso de la dirección del minilote (que nos puede llevar en una dirección mala). El resultado es un suavizamiento de las curvas de aprendizaje.\nEsto es similar al movimiento de una canica en una superficie: la dirección de su movimiento está dada en parte por la dirección de descenso (el gradiente) y en parte la velocidad actual de la canica. La canica se mueve en un promedio de estas dos direcciones\n\n\n\n\n\n\nDescenso estocástico con momento\n\n\n\nSeparamos al azar los datos de entrenamiento en \\(n\\) minilotes de tamaño \\(m\\).\n\nPara épocas \\(e =1,2,\\ldots, n_e\\)\n\nCalcular el gradiente sobre el minilote y hacer actualización, sucesivamente para cada uno de los minilotes \\(k=1,2,\\ldots, n/m\\): \\[\\beta_{i+1} = \\beta_{i} + v,\\] \\[v= \\alpha v - \\eta\\frac{1}{m}\\sum_{j=1}^m \\nabla D^{(k)}_j\\] donde \\(D^{(k)}_j (\\beta_i)\\) es la devianza para el \\(j\\)-ésimo caso del minilote \\(k\\). A \\(v\\) se llama la velocidad\n\nRepetir para la siguiente época\n\n\n\n\ndescenso_estocastico &lt;- function(n_epocas, z_0, eta, minilotes, \n                                 momento = 0.0, decaimiento = 0.0){\n  #minilotes es una lista\n  m &lt;- length(minilotes)\n  z &lt;- matrix(0, m*n_epocas, length(z_0))\n  z[1, ] &lt;- z_0\n  v &lt;- 0\n  for(i in 1:(m*n_epocas-1)){\n    k &lt;- i %% m + 1\n    if(i %% m == 0){\n      #comenzar nueva época y reordenar minilotes al azar\n      minilotes &lt;- minilotes[sample(1:m, m)]\n      v &lt;- 0\n    }\n    h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y)\n    z[i+1, ] &lt;- z[i, ] + v\n    v &lt;- momento*v - eta * h_deriv(z[i, ])\n    eta &lt;- eta*(1/(1+decaimiento*i))\n  }\n  colnames(z) &lt;- names(z_0)\n  z\n}\n\nY ahora vemos que usando momento el algoritmo es más parecido a descenso en gradiente usual (pues tenemos cierta memoria de direcciones anteriores de descenso):\n\nset.seed(232)\niter_estocastico &lt;- descenso_estocastico(10, z_0, 0.005, minilotes, momento = 0.9, decaimiento = 0.00001) |&gt;\n  as_tibble() |&gt; rename(beta_0 = Intercept, beta_1 = x_1, beta_2 = x_2, beta_3 = x_3)\ndev_ent &lt;- perdida_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_ent$y)\ndev_valid &lt;- perdida_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]), \n                             y=dat_valid$y)\ndat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) |&gt;\n   mutate(entrena = apply(iter_estocastico, 1, dev_ent), \n  validacion = apply(iter_estocastico, 1, dev_valid)) |&gt;\n  gather(tipo, devianza, entrena:validacion)\nggplot(filter(dat_dev, iteracion &gt; 1), \n       aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() + geom_point()\n\n\n\n\n\n\n\n\n\nggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() +\n  geom_point() +\n  geom_path(data = iter_estocastico, colour ='red', alpha=0.5) +\n  geom_point(data = iter_estocastico, colour ='red', alpha=0.5)\n\n\n\n\n\n\n\n\nNótese cómo llegamos más rápido a una buena solución (comparado con el ejemplo sin momento). Adicionalmente, error de entrenamiento y validación lucen más suaves, producto de promediar velocidades a lo largo de iteraciones.\nValores típicos para momento son 0,0.5,0.9 o 0.99.\n\n\nB.4.3 Otras variaciones\nOtras variaciones incluyen usar una tasa adaptativa de aprendizaje por cada parámetro (algoritmos adagrad, rmsprop, adam y adamax), o actualizaciones de momento un poco diferentes (Nesterov).\nLos más comunes son descenso estocástico, descenso estocástico con momento (a veces con la modificación de Nesterov), rmsprop y adam (Capítulo 8 del Deep Learning Book, (Goodfellow, Bengio, y Courville 2016)).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Apéndice 2: Descenso estocástico</span>"
    ]
  },
  {
    "objectID": "82-apendice-descenso-estocastico.html#ajuste-de-redes-con-descenso-estocástico",
    "href": "82-apendice-descenso-estocastico.html#ajuste-de-redes-con-descenso-estocástico",
    "title": "Apéndice B — Apéndice 2: Descenso estocástico",
    "section": "B.5 Ajuste de redes con descenso estocástico",
    "text": "B.5 Ajuste de redes con descenso estocástico\n\nlibrary(keras3)\n\n\nset.seed(21321)\nx_ent &lt;- as.matrix(dat_ent[,c('x_1','x_2','x_3')])\nx_valid &lt;-  as.matrix(dat_valid[,c('x_1','x_2','x_3')])\ny_ent &lt;- dat_ent$y\ny_valid &lt;- dat_valid$y\n\nEmpezamos con regresión (sin capas ocultas), que se escribe y ajusta como sigue:\n\nmodelo &lt;- keras_model_sequential() \nmodelo |&gt;\n  layer_dense(units = 1, \n              activation = \"linear\",\n              input_shape = c(3))\n\nmodelo |&gt; compile(loss = 'mse',\n                   optimizer = optimizer_sgd(learning_rate = 0.1, momentum = 0,\n                                             decay = 0))\n\nhistory &lt;- modelo |&gt; \n  fit(x_ent, y_ent, \n      epochs = 50, batch_size = 10, \n      verbose = 0,\n      validation_data = list(x_valid, y_valid))\n\nPodemos ver el progreso del algoritmo por época\n\naprendizaje &lt;- as_tibble(history)\nggplot(aprendizaje, \n       aes(x=epoch, y=value, colour=data, group=data)) +\n  facet_wrap(~metric, ncol = 1) + geom_line() + geom_point(size = 0.5)\n\n\n\n\n\n\n\n\nVer los pesos:\n\nget_weights(modelo)\n\n[[1]]\n            [,1]\n[1,] -2.57271719\n[2,] -0.02831176\n[3,]  0.23202585\n\n[[2]]\n[1] -0.4566561\n\n\nY verificamos que concuerda con la salida de lm:\n\nmod_lineal &lt;- lm(y ~ x_1 + x_2+ x_3, data = dat_ent) \ncoef(mod_lineal)\n\n(Intercept)         x_1         x_2         x_3 \n-0.36904266 -2.46877687 -0.07368414  0.06632769 \n\n\n\n\n\n\nGoodfellow, Ian, Yoshua Bengio, y Aaron Courville. 2016. Deep Learning. MIT Press.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Apéndice 2: Descenso estocástico</span>"
    ]
  },
  {
    "objectID": "99-referencias.html",
    "href": "99-referencias.html",
    "title": "Referencias",
    "section": "",
    "text": "Bishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning (Information Science and Statistics). Secaucus, NJ, USA:\nSpringer-Verlag New York, Inc.\n\n\nChambers, J. M., W. S. Cleveland, B. Kleiner, and P. A. Tukey. 1983.\nGraphical Methods for Data Analysis. Chapman & Hall\nStatistics Series. Wadsworth International Group. https://books.google.com.mx/books?id=I-tQAAAAMAAJ.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. Springer Series in Statistics.\nSpringer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2014. An Introduction to Statistical Learning: With Applications in\nr. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.\n\n\nKuhn, M., and J. Silge. 2022. Tidy Modeling with r. O’Reilly\nMedia. https://books.google.com.mx/books?id=9cJ6EAAAQBAJ.",
    "crumbs": [
      "Appendices",
      "Referencias"
    ]
  }
]